{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb94fc6",
   "metadata": {},
   "source": [
    "# 정리 후 백그라운드로 실행할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d49ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_1537297/1092903822.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  txts['answer_processed'] = txts[\"answer\"].map(lambda x: re.sub('[,\\.!?]', '', x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deciding', 'whether', 'to', 'get', 'master', 'degree', 'can', 'be', 'quite', 'personal', 'journey', 'and', 've', 'seen', 'different', 'sides', 'of', 'this', 'coin', 'through', 'my', 'own', 'experience', 'and', 'that', 'of', 'friends', 'and', 'colleagues', 'personally']\n"
     ]
    }
   ],
   "source": [
    "## data load\n",
    "txts = pd.read_excel(\"combined_data_NLP.xlsx\")\n",
    "model_idxs = txts.model\n",
    "txts = txts.drop([\"model\", \"question\", \"attempt\"], axis = 1)\n",
    "\n",
    "## Remove punctuation\n",
    "txts['answer_processed'] = txts[\"answer\"].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "## Convert the titles to lowercase\n",
    "txts['answer_processed'] = txts['answer_processed'].map(lambda x: x.lower())\n",
    "\n",
    "## Print out the first rows of txts\n",
    "txts['answer_processed'].head()\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = txts.answer_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69390b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1b595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d9d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "['get', 'quite', 'personal', 'journey', 'see', 'different', 'side', 'coin', 'experience', 'friend', 'colleague', 'personally', 'choose', 'pursue_master', 'degree', 'right', 'complete', 'undergraduate', 'study', 'time', 'feel', 'much', 'real', 'world', 'experience', 'gain', 'want', 'dive', 'straight', 'workforce']\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0ad6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 2), (27, 1), (28, 1), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c27501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"people\" + 0.012*\"think\" + 0.011*\"thing\" + 0.011*\"even\" + '\n",
      "  '0.009*\"make\" + 0.009*\"get\" + 0.008*\"way\" + 0.007*\"time\" + 0.006*\"know\" + '\n",
      "  '0.006*\"find\"'),\n",
      " (1,\n",
      "  '0.014*\"people\" + 0.011*\"think\" + 0.009*\"tooth\" + 0.008*\"even\" + 0.007*\"get\" '\n",
      "  '+ 0.006*\"thing\" + 0.006*\"feel\" + 0.006*\"make\" + 0.005*\"day\" + 0.005*\"time\"'),\n",
      " (2,\n",
      "  '0.017*\"time\" + 0.008*\"get\" + 0.008*\"make\" + 0.006*\"travel\" + 0.006*\"go\" + '\n",
      "  '0.006*\"think\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"work\" + '\n",
      "  '0.005*\"year\"'),\n",
      " (3,\n",
      "  '0.014*\"get\" + 0.011*\"make\" + 0.008*\"name\" + 0.008*\"work\" + 0.007*\"go\" + '\n",
      "  '0.007*\"even\" + 0.007*\"day\" + 0.006*\"time\" + 0.006*\"take\" + 0.005*\"sound\"'),\n",
      " (4,\n",
      "  '0.012*\"get\" + 0.011*\"go\" + 0.010*\"say\" + 0.008*\"time\" + 0.007*\"know\" + '\n",
      "  '0.006*\"make\" + 0.006*\"day\" + 0.006*\"take\" + 0.006*\"see\" + 0.006*\"tell\"'),\n",
      " (5,\n",
      "  '0.013*\"feel\" + 0.010*\"people\" + 0.010*\"go\" + 0.009*\"say\" + 0.008*\"think\" + '\n",
      "  '0.008*\"know\" + 0.008*\"really\" + 0.008*\"get\" + 0.008*\"time\" + 0.008*\"make\"'),\n",
      " (6,\n",
      "  '0.011*\"get\" + 0.009*\"think\" + 0.009*\"work\" + 0.009*\"people\" + 0.008*\"time\" '\n",
      "  '+ 0.007*\"learn\" + 0.007*\"make\" + 0.006*\"good\" + 0.006*\"also\" + 0.006*\"see\"'),\n",
      " (7,\n",
      "  '0.011*\"get\" + 0.010*\"go\" + 0.009*\"work\" + 0.008*\"thing\" + 0.008*\"know\" + '\n",
      "  '0.007*\"life\" + 0.007*\"try\" + 0.007*\"make\" + 0.006*\"think\" + 0.006*\"time\"'),\n",
      " (8,\n",
      "  '0.011*\"make\" + 0.010*\"go\" + 0.009*\"know\" + 0.009*\"say\" + 0.009*\"people\" + '\n",
      "  '0.009*\"get\" + 0.008*\"day\" + 0.008*\"life\" + 0.008*\"think\" + 0.008*\"time\"'),\n",
      " (9,\n",
      "  '0.014*\"feel\" + 0.014*\"make\" + 0.013*\"life\" + 0.012*\"people\" + 0.009*\"think\" '\n",
      "  '+ 0.008*\"thing\" + 0.008*\"find\" + 0.008*\"even\" + 0.007*\"way\" + 0.006*\"time\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f42a3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.3518118387571053\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211525bf",
   "metadata": {},
   "source": [
    "`-` 이거는 다른 퍼플렉시티 같은데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a87cdcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/NLP/lib/python3.12/site-packages/gensim/models/ldamodel.py:850: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1492.821032721594"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.log_perplexity([corpus[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29e6d7",
   "metadata": {},
   "source": [
    "# 그냥 코드나 리뷰하고 있자\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:47:52,421 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-09-10 11:47:53,336 : INFO : adding document #10000 to Dictionary<23014 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...>\n",
      "2025-09-10 11:47:53,790 : INFO : built Dictionary<34546 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...> from 14833 documents (total 1640281 corpus positions)\n",
      "2025-09-10 11:47:53,790 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<34546 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...> from 14833 documents (total 1640281 corpus positions)\", 'datetime': '2025-09-10T11:47:53.790908', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-09-10 11:47:54,808 : INFO : using autotuned alpha, starting with [0.5, 0.5]\n",
      "2025-09-10 11:47:54,809 : INFO : using symmetric eta at 0.5\n",
      "2025-09-10 11:47:54,814 : INFO : using serial LDA version on this node\n",
      "2025-09-10 11:47:54,819 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 14833 documents, updating model once every 2000 documents, evaluating perplexity every 14833 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-09-10 11:47:54,820 : INFO : PROGRESS: pass 0, at document #2000/14833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----토픽 개수 산정을 위한 coherence score 계산 중-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:47:55,961 : INFO : optimized alpha [0.9009148, 0.80155915]\n",
      "2025-09-10 11:47:55,963 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:47:55,971 : INFO : topic #0 (0.901): 0.013*\"like\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"people\" + 0.006*\"also\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"really\"\n",
      "2025-09-10 11:47:55,972 : INFO : topic #1 (0.802): 0.011*\"like\" + 0.009*\"one\" + 0.008*\"people\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"life\" + 0.006*\"might\" + 0.005*\"often\" + 0.005*\"way\"\n",
      "2025-09-10 11:47:55,972 : INFO : topic diff=3.478361, rho=1.000000\n",
      "2025-09-10 11:47:55,973 : INFO : PROGRESS: pass 0, at document #4000/14833\n",
      "2025-09-10 11:47:57,117 : INFO : optimized alpha [1.1482253, 0.9960377]\n",
      "2025-09-10 11:47:57,120 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:47:57,122 : INFO : topic #0 (1.148): 0.014*\"like\" + 0.010*\"think\" + 0.009*\"one\" + 0.008*\"something\" + 0.008*\"people\" + 0.007*\"really\" + 0.007*\"time\" + 0.007*\"might\" + 0.006*\"also\" + 0.006*\"even\"\n",
      "2025-09-10 11:47:57,123 : INFO : topic #1 (0.996): 0.012*\"like\" + 0.010*\"one\" + 0.008*\"think\" + 0.007*\"people\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"life\" + 0.005*\"way\" + 0.005*\"really\" + 0.004*\"know\"\n",
      "2025-09-10 11:47:57,123 : INFO : topic diff=0.613604, rho=0.707107\n",
      "2025-09-10 11:47:57,124 : INFO : PROGRESS: pass 0, at document #6000/14833\n",
      "2025-09-10 11:47:58,226 : INFO : optimized alpha [1.0122145, 0.9298487]\n",
      "2025-09-10 11:47:58,229 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:47:58,231 : INFO : topic #0 (1.012): 0.017*\"like\" + 0.012*\"think\" + 0.009*\"people\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"really\" + 0.008*\"even\" + 0.007*\"also\" + 0.006*\"someone\" + 0.006*\"things\"\n",
      "2025-09-10 11:47:58,232 : INFO : topic #1 (0.930): 0.015*\"like\" + 0.010*\"one\" + 0.008*\"think\" + 0.008*\"even\" + 0.007*\"people\" + 0.006*\"time\" + 0.006*\"really\" + 0.005*\"know\" + 0.005*\"okay\" + 0.005*\"way\"\n",
      "2025-09-10 11:47:58,232 : INFO : topic diff=0.483787, rho=0.577350\n",
      "2025-09-10 11:47:58,233 : INFO : PROGRESS: pass 0, at document #8000/14833\n",
      "2025-09-10 11:47:59,298 : INFO : optimized alpha [0.76271635, 0.7036354]\n",
      "2025-09-10 11:47:59,300 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:47:59,303 : INFO : topic #0 (0.763): 0.014*\"like\" + 0.011*\"think\" + 0.009*\"people\" + 0.009*\"something\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"would\" + 0.006*\"also\"\n",
      "2025-09-10 11:47:59,304 : INFO : topic #1 (0.704): 0.012*\"like\" + 0.008*\"one\" + 0.007*\"think\" + 0.007*\"people\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"really\" + 0.005*\"actually\" + 0.004*\"get\" + 0.004*\"way\"\n",
      "2025-09-10 11:47:59,304 : INFO : topic diff=0.434418, rho=0.500000\n",
      "2025-09-10 11:47:59,305 : INFO : PROGRESS: pass 0, at document #10000/14833\n",
      "2025-09-10 11:48:00,387 : INFO : optimized alpha [0.5434821, 0.5793663]\n",
      "2025-09-10 11:48:00,390 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:00,393 : INFO : topic #0 (0.543): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"would\" + 0.006*\"time\" + 0.005*\"might\"\n",
      "2025-09-10 11:48:00,394 : INFO : topic #1 (0.579): 0.011*\"like\" + 0.008*\"one\" + 0.006*\"people\" + 0.006*\"even\" + 0.006*\"think\" + 0.005*\"time\" + 0.005*\"actually\" + 0.005*\"way\" + 0.004*\"really\" + 0.004*\"back\"\n",
      "2025-09-10 11:48:00,394 : INFO : topic diff=0.379949, rho=0.447214\n",
      "2025-09-10 11:48:00,395 : INFO : PROGRESS: pass 0, at document #12000/14833\n",
      "2025-09-10 11:48:01,390 : INFO : optimized alpha [0.43428257, 0.4751329]\n",
      "2025-09-10 11:48:01,393 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:01,395 : INFO : topic #0 (0.434): 0.012*\"like\" + 0.012*\"people\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"someone\" + 0.005*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:01,396 : INFO : topic #1 (0.475): 0.011*\"like\" + 0.009*\"one\" + 0.007*\"even\" + 0.006*\"people\" + 0.005*\"time\" + 0.005*\"way\" + 0.004*\"think\" + 0.004*\"actually\" + 0.004*\"really\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:01,396 : INFO : topic diff=0.350534, rho=0.408248\n",
      "2025-09-10 11:48:01,397 : INFO : PROGRESS: pass 0, at document #14000/14833\n",
      "2025-09-10 11:48:02,370 : INFO : optimized alpha [0.44726068, 0.51749694]\n",
      "2025-09-10 11:48:02,372 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:02,375 : INFO : topic #0 (0.447): 0.010*\"people\" + 0.010*\"like\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"would\" + 0.006*\"even\" + 0.006*\"something\" + 0.006*\"think\" + 0.005*\"someone\" + 0.004*\"things\"\n",
      "2025-09-10 11:48:02,376 : INFO : topic #1 (0.517): 0.009*\"one\" + 0.009*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"people\" + 0.004*\"day\" + 0.004*\"years\" + 0.004*\"would\" + 0.004*\"get\" + 0.004*\"life\"\n",
      "2025-09-10 11:48:02,376 : INFO : topic diff=0.791139, rho=0.377964\n",
      "2025-09-10 11:48:03,001 : INFO : -8.732 per-word bound, 425.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:03,001 : INFO : PROGRESS: pass 0, at document #14833/14833\n",
      "2025-09-10 11:48:03,393 : INFO : optimized alpha [0.37713033, 0.5334218]\n",
      "2025-09-10 11:48:03,395 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:03,398 : INFO : topic #0 (0.377): 0.011*\"people\" + 0.009*\"like\" + 0.007*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"think\" + 0.005*\"something\" + 0.005*\"someone\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:03,399 : INFO : topic #1 (0.533): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"people\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"get\" + 0.004*\"back\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:03,400 : INFO : topic diff=0.574171, rho=0.353553\n",
      "2025-09-10 11:48:03,400 : INFO : PROGRESS: pass 1, at document #2000/14833\n",
      "2025-09-10 11:48:04,396 : INFO : optimized alpha [0.3781054, 0.28272733]\n",
      "2025-09-10 11:48:04,399 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:04,402 : INFO : topic #0 (0.378): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"think\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"even\" + 0.006*\"would\" + 0.005*\"someone\"\n",
      "2025-09-10 11:48:04,402 : INFO : topic #1 (0.283): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"people\" + 0.004*\"would\" + 0.004*\"back\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:04,403 : INFO : topic diff=0.445207, rho=0.325878\n",
      "2025-09-10 11:48:04,404 : INFO : PROGRESS: pass 1, at document #4000/14833\n",
      "2025-09-10 11:48:05,294 : INFO : optimized alpha [0.39770502, 0.26758674]\n",
      "2025-09-10 11:48:05,297 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:05,300 : INFO : topic #0 (0.398): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"would\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:05,301 : INFO : topic #1 (0.268): 0.010*\"one\" + 0.009*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"back\" + 0.004*\"people\" + 0.004*\"get\" + 0.004*\"would\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:05,301 : INFO : topic diff=0.307121, rho=0.325878\n",
      "2025-09-10 11:48:05,302 : INFO : PROGRESS: pass 1, at document #6000/14833\n",
      "2025-09-10 11:48:06,257 : INFO : optimized alpha [0.3682305, 0.26132876]\n",
      "2025-09-10 11:48:06,260 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:06,263 : INFO : topic #0 (0.368): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:06,264 : INFO : topic #1 (0.261): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"get\" + 0.004*\"people\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:06,264 : INFO : topic diff=0.276624, rho=0.325878\n",
      "2025-09-10 11:48:06,265 : INFO : PROGRESS: pass 1, at document #8000/14833\n",
      "2025-09-10 11:48:07,146 : INFO : optimized alpha [0.35463178, 0.25606012]\n",
      "2025-09-10 11:48:07,149 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:07,152 : INFO : topic #0 (0.355): 0.013*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"someone\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"time\"\n",
      "2025-09-10 11:48:07,153 : INFO : topic #1 (0.256): 0.011*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"really\" + 0.004*\"day\" + 0.004*\"people\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:07,153 : INFO : topic diff=0.258963, rho=0.325878\n",
      "2025-09-10 11:48:07,153 : INFO : PROGRESS: pass 1, at document #10000/14833\n",
      "2025-09-10 11:48:07,988 : INFO : optimized alpha [0.3129653, 0.24575159]\n",
      "2025-09-10 11:48:07,990 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:07,993 : INFO : topic #0 (0.313): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"would\" + 0.006*\"someone\" + 0.006*\"might\" + 0.006*\"time\"\n",
      "2025-09-10 11:48:07,994 : INFO : topic #1 (0.246): 0.011*\"like\" + 0.009*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"back\" + 0.004*\"day\" + 0.004*\"people\" + 0.004*\"really\" + 0.004*\"years\" + 0.004*\"actually\"\n",
      "2025-09-10 11:48:07,994 : INFO : topic diff=0.270262, rho=0.325878\n",
      "2025-09-10 11:48:07,995 : INFO : PROGRESS: pass 1, at document #12000/14833\n",
      "2025-09-10 11:48:08,771 : INFO : optimized alpha [0.2793663, 0.2341739]\n",
      "2025-09-10 11:48:08,774 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:08,777 : INFO : topic #0 (0.279): 0.012*\"people\" + 0.012*\"like\" + 0.008*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"time\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:08,778 : INFO : topic #1 (0.234): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.004*\"people\" + 0.004*\"back\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"way\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:08,778 : INFO : topic diff=0.271873, rho=0.325878\n",
      "2025-09-10 11:48:08,779 : INFO : PROGRESS: pass 1, at document #14000/14833\n",
      "2025-09-10 11:48:09,648 : INFO : optimized alpha [0.27757585, 0.25883183]\n",
      "2025-09-10 11:48:09,651 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:09,654 : INFO : topic #0 (0.278): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"one\" + 0.007*\"think\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:09,655 : INFO : topic #1 (0.259): 0.009*\"one\" + 0.009*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"back\" + 0.004*\"would\" + 0.004*\"people\" + 0.004*\"years\" + 0.004*\"got\"\n",
      "2025-09-10 11:48:09,655 : INFO : topic diff=0.588123, rho=0.325878\n",
      "2025-09-10 11:48:10,217 : INFO : -8.611 per-word bound, 390.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:10,217 : INFO : PROGRESS: pass 1, at document #14833/14833\n",
      "2025-09-10 11:48:10,525 : INFO : optimized alpha [0.23327078, 0.27525052]\n",
      "2025-09-10 11:48:10,528 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:10,530 : INFO : topic #0 (0.233): 0.012*\"people\" + 0.009*\"like\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:10,531 : INFO : topic #1 (0.275): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"back\" + 0.004*\"get\" + 0.004*\"said\" + 0.004*\"people\"\n",
      "2025-09-10 11:48:10,531 : INFO : topic diff=0.467183, rho=0.325878\n",
      "2025-09-10 11:48:10,532 : INFO : PROGRESS: pass 2, at document #2000/14833\n",
      "2025-09-10 11:48:11,304 : INFO : optimized alpha [0.25632805, 0.20739067]\n",
      "2025-09-10 11:48:11,307 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:11,309 : INFO : topic #0 (0.256): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:11,311 : INFO : topic #1 (0.207): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"get\" + 0.004*\"said\" + 0.004*\"people\"\n",
      "2025-09-10 11:48:11,311 : INFO : topic diff=0.389437, rho=0.309841\n",
      "2025-09-10 11:48:11,312 : INFO : PROGRESS: pass 2, at document #4000/14833\n",
      "2025-09-10 11:48:12,089 : INFO : optimized alpha [0.28552613, 0.20438188]\n",
      "2025-09-10 11:48:12,091 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:12,094 : INFO : topic #0 (0.286): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"even\" + 0.006*\"something\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:12,095 : INFO : topic #1 (0.204): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.004*\"would\" + 0.004*\"get\" + 0.004*\"got\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:12,095 : INFO : topic diff=0.273356, rho=0.309841\n",
      "2025-09-10 11:48:12,096 : INFO : PROGRESS: pass 2, at document #6000/14833\n",
      "2025-09-10 11:48:12,878 : INFO : optimized alpha [0.29092735, 0.20944051]\n",
      "2025-09-10 11:48:12,880 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:12,883 : INFO : topic #0 (0.291): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:12,884 : INFO : topic #1 (0.209): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"really\" + 0.004*\"get\" + 0.004*\"got\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:12,884 : INFO : topic diff=0.252351, rho=0.309841\n",
      "2025-09-10 11:48:12,885 : INFO : PROGRESS: pass 2, at document #8000/14833\n",
      "2025-09-10 11:48:13,607 : INFO : optimized alpha [0.3016899, 0.21386515]\n",
      "2025-09-10 11:48:13,609 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:13,612 : INFO : topic #0 (0.302): 0.013*\"like\" + 0.011*\"think\" + 0.011*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"would\"\n",
      "2025-09-10 11:48:13,613 : INFO : topic #1 (0.214): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"really\" + 0.004*\"get\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:13,613 : INFO : topic diff=0.239611, rho=0.309841\n",
      "2025-09-10 11:48:13,614 : INFO : PROGRESS: pass 2, at document #10000/14833\n",
      "2025-09-10 11:48:14,387 : INFO : optimized alpha [0.28842816, 0.2131863]\n",
      "2025-09-10 11:48:14,390 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:14,392 : INFO : topic #0 (0.288): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"someone\" + 0.006*\"would\" + 0.006*\"might\" + 0.006*\"time\"\n",
      "2025-09-10 11:48:14,393 : INFO : topic #1 (0.213): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"really\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:14,394 : INFO : topic diff=0.251011, rho=0.309841\n",
      "2025-09-10 11:48:14,394 : INFO : PROGRESS: pass 2, at document #12000/14833\n",
      "2025-09-10 11:48:15,092 : INFO : optimized alpha [0.27333274, 0.20970811]\n",
      "2025-09-10 11:48:15,094 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:15,097 : INFO : topic #0 (0.273): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"time\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:15,098 : INFO : topic #1 (0.210): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.004*\"back\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"day\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:15,098 : INFO : topic diff=0.255095, rho=0.309841\n",
      "2025-09-10 11:48:15,098 : INFO : PROGRESS: pass 2, at document #14000/14833\n",
      "2025-09-10 11:48:15,828 : INFO : optimized alpha [0.27417552, 0.23379394]\n",
      "2025-09-10 11:48:15,830 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:15,833 : INFO : topic #0 (0.274): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:15,834 : INFO : topic #1 (0.234): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"got\" + 0.004*\"would\" + 0.004*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:15,834 : INFO : topic diff=0.547444, rho=0.309841\n",
      "2025-09-10 11:48:16,362 : INFO : -8.581 per-word bound, 383.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:16,363 : INFO : PROGRESS: pass 2, at document #14833/14833\n",
      "2025-09-10 11:48:16,650 : INFO : optimized alpha [0.23071982, 0.24970962]\n",
      "2025-09-10 11:48:16,652 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:16,655 : INFO : topic #0 (0.231): 0.012*\"people\" + 0.010*\"like\" + 0.007*\"one\" + 0.007*\"think\" + 0.007*\"would\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:16,656 : INFO : topic #1 (0.250): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.004*\"back\" + 0.004*\"even\" + 0.004*\"said\" + 0.004*\"get\" + 0.004*\"got\"\n",
      "2025-09-10 11:48:16,656 : INFO : topic diff=0.432984, rho=0.309841\n",
      "2025-09-10 11:48:16,657 : INFO : PROGRESS: pass 3, at document #2000/14833\n",
      "2025-09-10 11:48:17,373 : INFO : optimized alpha [0.25491765, 0.19925967]\n",
      "2025-09-10 11:48:17,375 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:17,378 : INFO : topic #0 (0.255): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:17,379 : INFO : topic #1 (0.199): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.004*\"back\" + 0.004*\"even\" + 0.004*\"get\" + 0.004*\"said\" + 0.004*\"got\"\n",
      "2025-09-10 11:48:17,380 : INFO : topic diff=0.360754, rho=0.295960\n",
      "2025-09-10 11:48:17,380 : INFO : PROGRESS: pass 3, at document #4000/14833\n",
      "2025-09-10 11:48:18,113 : INFO : optimized alpha [0.2851976, 0.19686924]\n",
      "2025-09-10 11:48:18,115 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:18,118 : INFO : topic #0 (0.285): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"something\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:18,119 : INFO : topic #1 (0.197): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:18,119 : INFO : topic diff=0.255087, rho=0.295960\n",
      "2025-09-10 11:48:18,120 : INFO : PROGRESS: pass 3, at document #6000/14833\n",
      "2025-09-10 11:48:18,864 : INFO : optimized alpha [0.29549894, 0.20321119]\n",
      "2025-09-10 11:48:18,866 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:18,869 : INFO : topic #0 (0.295): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:18,870 : INFO : topic #1 (0.203): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"really\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:18,870 : INFO : topic diff=0.236605, rho=0.295960\n",
      "2025-09-10 11:48:18,871 : INFO : PROGRESS: pass 3, at document #8000/14833\n",
      "2025-09-10 11:48:19,564 : INFO : optimized alpha [0.31007728, 0.20881706]\n",
      "2025-09-10 11:48:19,567 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:19,570 : INFO : topic #0 (0.310): 0.013*\"like\" + 0.011*\"think\" + 0.011*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:19,571 : INFO : topic #1 (0.209): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"really\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:19,571 : INFO : topic diff=0.225497, rho=0.295960\n",
      "2025-09-10 11:48:19,572 : INFO : PROGRESS: pass 3, at document #10000/14833\n",
      "2025-09-10 11:48:20,276 : INFO : optimized alpha [0.30224413, 0.20961033]\n",
      "2025-09-10 11:48:20,279 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:20,282 : INFO : topic #0 (0.302): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"would\"\n",
      "2025-09-10 11:48:20,283 : INFO : topic #1 (0.210): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"really\" + 0.004*\"still\"\n",
      "2025-09-10 11:48:20,283 : INFO : topic diff=0.236059, rho=0.295960\n",
      "2025-09-10 11:48:20,284 : INFO : PROGRESS: pass 3, at document #12000/14833\n",
      "2025-09-10 11:48:21,008 : INFO : optimized alpha [0.28962824, 0.20740578]\n",
      "2025-09-10 11:48:21,011 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:21,014 : INFO : topic #0 (0.290): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"time\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:21,015 : INFO : topic #1 (0.207): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"day\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:21,015 : INFO : topic diff=0.242055, rho=0.295960\n",
      "2025-09-10 11:48:21,016 : INFO : PROGRESS: pass 3, at document #14000/14833\n",
      "2025-09-10 11:48:21,793 : INFO : optimized alpha [0.2913071, 0.23152958]\n",
      "2025-09-10 11:48:21,796 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:21,799 : INFO : topic #0 (0.291): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:21,800 : INFO : topic #1 (0.232): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"got\" + 0.004*\"would\" + 0.004*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:21,801 : INFO : topic diff=0.519058, rho=0.295960\n",
      "2025-09-10 11:48:22,345 : INFO : -8.569 per-word bound, 379.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:22,346 : INFO : PROGRESS: pass 3, at document #14833/14833\n",
      "2025-09-10 11:48:22,642 : INFO : optimized alpha [0.24510229, 0.24678823]\n",
      "2025-09-10 11:48:22,644 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:22,647 : INFO : topic #0 (0.245): 0.012*\"people\" + 0.010*\"like\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"would\" + 0.006*\"time\" + 0.005*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:22,648 : INFO : topic #1 (0.247): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.004*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"get\" + 0.004*\"got\"\n",
      "2025-09-10 11:48:22,649 : INFO : topic diff=0.411039, rho=0.295960\n",
      "2025-09-10 11:48:22,650 : INFO : PROGRESS: pass 4, at document #2000/14833\n",
      "2025-09-10 11:48:23,393 : INFO : optimized alpha [0.26960707, 0.20084271]\n",
      "2025-09-10 11:48:23,396 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:23,399 : INFO : topic #0 (0.270): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:23,400 : INFO : topic #1 (0.201): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"get\" + 0.004*\"said\" + 0.004*\"got\"\n",
      "2025-09-10 11:48:23,401 : INFO : topic diff=0.340248, rho=0.283792\n",
      "2025-09-10 11:48:23,401 : INFO : PROGRESS: pass 4, at document #4000/14833\n",
      "2025-09-10 11:48:24,164 : INFO : optimized alpha [0.30054685, 0.19822045]\n",
      "2025-09-10 11:48:24,167 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:24,170 : INFO : topic #0 (0.301): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:24,171 : INFO : topic #1 (0.198): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:24,171 : INFO : topic diff=0.241778, rho=0.283792\n",
      "2025-09-10 11:48:24,172 : INFO : PROGRESS: pass 4, at document #6000/14833\n",
      "2025-09-10 11:48:24,922 : INFO : optimized alpha [0.31237036, 0.2045058]\n",
      "2025-09-10 11:48:24,925 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:24,928 : INFO : topic #0 (0.312): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:24,928 : INFO : topic #1 (0.205): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"really\" + 0.004*\"get\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:24,929 : INFO : topic diff=0.224590, rho=0.283792\n",
      "2025-09-10 11:48:24,929 : INFO : PROGRESS: pass 4, at document #8000/14833\n",
      "2025-09-10 11:48:25,615 : INFO : optimized alpha [0.32870653, 0.20995405]\n",
      "2025-09-10 11:48:25,618 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:25,621 : INFO : topic #0 (0.329): 0.013*\"like\" + 0.011*\"think\" + 0.011*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:25,622 : INFO : topic #1 (0.210): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"really\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:25,622 : INFO : topic diff=0.214088, rho=0.283792\n",
      "2025-09-10 11:48:25,622 : INFO : PROGRESS: pass 4, at document #10000/14833\n",
      "2025-09-10 11:48:26,338 : INFO : optimized alpha [0.32131052, 0.21044824]\n",
      "2025-09-10 11:48:26,341 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:26,345 : INFO : topic #0 (0.321): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"would\"\n",
      "2025-09-10 11:48:26,345 : INFO : topic #1 (0.210): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:26,346 : INFO : topic diff=0.223964, rho=0.283792\n",
      "2025-09-10 11:48:26,346 : INFO : PROGRESS: pass 4, at document #12000/14833\n",
      "2025-09-10 11:48:27,058 : INFO : optimized alpha [0.30856484, 0.20856833]\n",
      "2025-09-10 11:48:27,061 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:27,064 : INFO : topic #0 (0.309): 0.012*\"like\" + 0.012*\"people\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"time\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:27,065 : INFO : topic #1 (0.209): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"day\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:27,065 : INFO : topic diff=0.231125, rho=0.283792\n",
      "2025-09-10 11:48:27,066 : INFO : PROGRESS: pass 4, at document #14000/14833\n",
      "2025-09-10 11:48:27,810 : INFO : optimized alpha [0.31042886, 0.2324748]\n",
      "2025-09-10 11:48:27,812 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:27,815 : INFO : topic #0 (0.310): 0.011*\"people\" + 0.011*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:27,816 : INFO : topic #1 (0.232): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"would\" + 0.004*\"years\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:27,816 : INFO : topic diff=0.495686, rho=0.283792\n",
      "2025-09-10 11:48:28,343 : INFO : -8.560 per-word bound, 377.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:28,344 : INFO : PROGRESS: pass 4, at document #14833/14833\n",
      "2025-09-10 11:48:28,633 : INFO : optimized alpha [0.26069254, 0.24744858]\n",
      "2025-09-10 11:48:28,635 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:28,639 : INFO : topic #0 (0.261): 0.012*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"would\" + 0.006*\"time\" + 0.005*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:28,640 : INFO : topic #1 (0.247): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.004*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:28,640 : INFO : topic diff=0.393136, rho=0.283792\n",
      "2025-09-10 11:48:28,641 : INFO : PROGRESS: pass 5, at document #2000/14833\n",
      "2025-09-10 11:48:29,318 : INFO : optimized alpha [0.28492507, 0.20333563]\n",
      "2025-09-10 11:48:29,320 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:29,323 : INFO : topic #0 (0.285): 0.011*\"like\" + 0.010*\"people\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"might\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:29,324 : INFO : topic #1 (0.203): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:29,325 : INFO : topic diff=0.323826, rho=0.273011\n",
      "2025-09-10 11:48:29,325 : INFO : PROGRESS: pass 5, at document #4000/14833\n",
      "2025-09-10 11:48:30,032 : INFO : optimized alpha [0.31568938, 0.19965649]\n",
      "2025-09-10 11:48:30,035 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:30,038 : INFO : topic #0 (0.316): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:30,039 : INFO : topic #1 (0.200): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:30,039 : INFO : topic diff=0.230924, rho=0.273011\n",
      "2025-09-10 11:48:30,040 : INFO : PROGRESS: pass 5, at document #6000/14833\n",
      "2025-09-10 11:48:30,764 : INFO : optimized alpha [0.32763714, 0.20558313]\n",
      "2025-09-10 11:48:30,767 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:30,770 : INFO : topic #0 (0.328): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:30,771 : INFO : topic #1 (0.206): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"really\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:30,771 : INFO : topic diff=0.214505, rho=0.273011\n",
      "2025-09-10 11:48:30,772 : INFO : PROGRESS: pass 5, at document #8000/14833\n",
      "2025-09-10 11:48:31,445 : INFO : optimized alpha [0.3425042, 0.21005407]\n",
      "2025-09-10 11:48:31,448 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:31,451 : INFO : topic #0 (0.343): 0.013*\"like\" + 0.011*\"think\" + 0.011*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:31,452 : INFO : topic #1 (0.210): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:31,452 : INFO : topic diff=0.204552, rho=0.273011\n",
      "2025-09-10 11:48:31,453 : INFO : PROGRESS: pass 5, at document #10000/14833\n",
      "2025-09-10 11:48:32,121 : INFO : optimized alpha [0.33619684, 0.21065736]\n",
      "2025-09-10 11:48:32,124 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:32,127 : INFO : topic #0 (0.336): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"would\"\n",
      "2025-09-10 11:48:32,128 : INFO : topic #1 (0.211): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:32,128 : INFO : topic diff=0.213597, rho=0.273011\n",
      "2025-09-10 11:48:32,129 : INFO : PROGRESS: pass 5, at document #12000/14833\n",
      "2025-09-10 11:48:32,772 : INFO : optimized alpha [0.32032835, 0.20795926]\n",
      "2025-09-10 11:48:32,774 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:32,777 : INFO : topic #0 (0.320): 0.012*\"like\" + 0.012*\"people\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.005*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:32,778 : INFO : topic #1 (0.208): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"day\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:32,778 : INFO : topic diff=0.221524, rho=0.273011\n",
      "2025-09-10 11:48:32,779 : INFO : PROGRESS: pass 5, at document #14000/14833\n",
      "2025-09-10 11:48:33,501 : INFO : optimized alpha [0.32245326, 0.2312169]\n",
      "2025-09-10 11:48:33,504 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:33,507 : INFO : topic #0 (0.322): 0.011*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\" + 0.005*\"someone\"\n",
      "2025-09-10 11:48:33,508 : INFO : topic #1 (0.231): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"would\" + 0.004*\"years\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:33,508 : INFO : topic diff=0.475191, rho=0.273011\n",
      "2025-09-10 11:48:34,033 : INFO : -8.554 per-word bound, 375.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:34,034 : INFO : PROGRESS: pass 5, at document #14833/14833\n",
      "2025-09-10 11:48:34,317 : INFO : optimized alpha [0.27223885, 0.24581201]\n",
      "2025-09-10 11:48:34,320 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:34,322 : INFO : topic #0 (0.272): 0.012*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"would\" + 0.006*\"time\" + 0.005*\"something\" + 0.005*\"life\" + 0.005*\"someone\"\n",
      "2025-09-10 11:48:34,323 : INFO : topic #1 (0.246): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:34,324 : INFO : topic diff=0.378036, rho=0.273011\n",
      "2025-09-10 11:48:34,324 : INFO : PROGRESS: pass 6, at document #2000/14833\n",
      "2025-09-10 11:48:34,982 : INFO : optimized alpha [0.2961435, 0.20420593]\n",
      "2025-09-10 11:48:34,985 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:34,987 : INFO : topic #0 (0.296): 0.011*\"like\" + 0.010*\"people\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:34,988 : INFO : topic #1 (0.204): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:34,988 : INFO : topic diff=0.309510, rho=0.263372\n",
      "2025-09-10 11:48:34,989 : INFO : PROGRESS: pass 6, at document #4000/14833\n",
      "2025-09-10 11:48:35,680 : INFO : optimized alpha [0.32647365, 0.19992904]\n",
      "2025-09-10 11:48:35,683 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:35,686 : INFO : topic #0 (0.326): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:35,686 : INFO : topic #1 (0.200): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:35,687 : INFO : topic diff=0.221538, rho=0.263372\n",
      "2025-09-10 11:48:35,687 : INFO : PROGRESS: pass 6, at document #6000/14833\n",
      "2025-09-10 11:48:36,412 : INFO : optimized alpha [0.33819237, 0.20527565]\n",
      "2025-09-10 11:48:36,414 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:36,417 : INFO : topic #0 (0.338): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"also\"\n",
      "2025-09-10 11:48:36,417 : INFO : topic #1 (0.205): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"really\" + 0.004*\"know\"\n",
      "2025-09-10 11:48:36,418 : INFO : topic diff=0.205771, rho=0.263372\n",
      "2025-09-10 11:48:36,418 : INFO : PROGRESS: pass 6, at document #8000/14833\n",
      "2025-09-10 11:48:37,123 : INFO : optimized alpha [0.35253555, 0.20917663]\n",
      "2025-09-10 11:48:37,125 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:37,128 : INFO : topic #0 (0.353): 0.013*\"like\" + 0.011*\"think\" + 0.011*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:37,129 : INFO : topic #1 (0.209): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:37,129 : INFO : topic diff=0.196016, rho=0.263372\n",
      "2025-09-10 11:48:37,130 : INFO : PROGRESS: pass 6, at document #10000/14833\n",
      "2025-09-10 11:48:37,789 : INFO : optimized alpha [0.3468393, 0.20978455]\n",
      "2025-09-10 11:48:37,791 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:37,794 : INFO : topic #0 (0.347): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"would\"\n",
      "2025-09-10 11:48:37,794 : INFO : topic #1 (0.210): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:37,795 : INFO : topic diff=0.204814, rho=0.263372\n",
      "2025-09-10 11:48:37,795 : INFO : PROGRESS: pass 6, at document #12000/14833\n",
      "2025-09-10 11:48:38,453 : INFO : optimized alpha [0.32988635, 0.20687076]\n",
      "2025-09-10 11:48:38,455 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:38,459 : INFO : topic #0 (0.330): 0.012*\"like\" + 0.012*\"people\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:38,459 : INFO : topic #1 (0.207): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"day\" + 0.004*\"still\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:38,460 : INFO : topic diff=0.212610, rho=0.263372\n",
      "2025-09-10 11:48:38,460 : INFO : PROGRESS: pass 6, at document #14000/14833\n",
      "2025-09-10 11:48:39,184 : INFO : optimized alpha [0.33189902, 0.22942083]\n",
      "2025-09-10 11:48:39,187 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:39,190 : INFO : topic #0 (0.332): 0.011*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:39,191 : INFO : topic #1 (0.229): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"would\" + 0.004*\"years\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:39,192 : INFO : topic diff=0.457135, rho=0.263372\n",
      "2025-09-10 11:48:39,717 : INFO : -8.549 per-word bound, 374.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:39,717 : INFO : PROGRESS: pass 6, at document #14833/14833\n",
      "2025-09-10 11:48:40,007 : INFO : optimized alpha [0.28090137, 0.24345662]\n",
      "2025-09-10 11:48:40,010 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:40,013 : INFO : topic #0 (0.281): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"life\" + 0.005*\"someone\"\n",
      "2025-09-10 11:48:40,014 : INFO : topic #1 (0.243): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:40,014 : INFO : topic diff=0.364876, rho=0.263372\n",
      "2025-09-10 11:48:40,015 : INFO : PROGRESS: pass 7, at document #2000/14833\n",
      "2025-09-10 11:48:40,663 : INFO : optimized alpha [0.30439866, 0.20416163]\n",
      "2025-09-10 11:48:40,666 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:40,669 : INFO : topic #0 (0.304): 0.011*\"like\" + 0.010*\"people\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:40,670 : INFO : topic #1 (0.204): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:40,670 : INFO : topic diff=0.296710, rho=0.254687\n",
      "2025-09-10 11:48:40,671 : INFO : PROGRESS: pass 7, at document #4000/14833\n",
      "2025-09-10 11:48:41,372 : INFO : optimized alpha [0.3340659, 0.19947636]\n",
      "2025-09-10 11:48:41,375 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:41,378 : INFO : topic #0 (0.334): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:41,378 : INFO : topic #1 (0.199): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:41,379 : INFO : topic diff=0.213258, rho=0.254687\n",
      "2025-09-10 11:48:41,380 : INFO : PROGRESS: pass 7, at document #6000/14833\n",
      "2025-09-10 11:48:42,146 : INFO : optimized alpha [0.34556544, 0.20437142]\n",
      "2025-09-10 11:48:42,150 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:42,153 : INFO : topic #0 (0.346): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"someone\"\n",
      "2025-09-10 11:48:42,154 : INFO : topic #1 (0.204): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"really\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:42,155 : INFO : topic diff=0.198100, rho=0.254687\n",
      "2025-09-10 11:48:42,155 : INFO : PROGRESS: pass 7, at document #8000/14833\n",
      "2025-09-10 11:48:42,821 : INFO : optimized alpha [0.359637, 0.20790361]\n",
      "2025-09-10 11:48:42,824 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:42,827 : INFO : topic #0 (0.360): 0.013*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:42,828 : INFO : topic #1 (0.208): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:42,828 : INFO : topic diff=0.188524, rho=0.254687\n",
      "2025-09-10 11:48:42,829 : INFO : PROGRESS: pass 7, at document #10000/14833\n",
      "2025-09-10 11:48:43,547 : INFO : optimized alpha [0.35443115, 0.20853017]\n",
      "2025-09-10 11:48:43,550 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:43,553 : INFO : topic #0 (0.354): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"would\"\n",
      "2025-09-10 11:48:43,554 : INFO : topic #1 (0.209): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:43,554 : INFO : topic diff=0.197127, rho=0.254687\n",
      "2025-09-10 11:48:43,556 : INFO : PROGRESS: pass 7, at document #12000/14833\n",
      "2025-09-10 11:48:44,245 : INFO : optimized alpha [0.33697534, 0.20551142]\n",
      "2025-09-10 11:48:44,248 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:44,251 : INFO : topic #0 (0.337): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:44,253 : INFO : topic #1 (0.206): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"day\" + 0.004*\"still\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:44,253 : INFO : topic diff=0.204806, rho=0.254687\n",
      "2025-09-10 11:48:44,254 : INFO : PROGRESS: pass 7, at document #14000/14833\n",
      "2025-09-10 11:48:44,946 : INFO : optimized alpha [0.33857018, 0.22715859]\n",
      "2025-09-10 11:48:44,948 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:44,951 : INFO : topic #0 (0.339): 0.011*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:44,952 : INFO : topic #1 (0.227): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"would\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:44,952 : INFO : topic diff=0.440909, rho=0.254687\n",
      "2025-09-10 11:48:45,477 : INFO : -8.545 per-word bound, 373.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:45,477 : INFO : PROGRESS: pass 7, at document #14833/14833\n",
      "2025-09-10 11:48:45,782 : INFO : optimized alpha [0.2874051, 0.24053486]\n",
      "2025-09-10 11:48:45,785 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:45,788 : INFO : topic #0 (0.287): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"life\" + 0.005*\"someone\"\n",
      "2025-09-10 11:48:45,789 : INFO : topic #1 (0.241): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:45,789 : INFO : topic diff=0.353050, rho=0.254687\n",
      "2025-09-10 11:48:45,790 : INFO : PROGRESS: pass 8, at document #2000/14833\n",
      "2025-09-10 11:48:46,493 : INFO : optimized alpha [0.31042534, 0.20346193]\n",
      "2025-09-10 11:48:46,496 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:46,499 : INFO : topic #0 (0.310): 0.011*\"like\" + 0.010*\"people\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:46,499 : INFO : topic #1 (0.203): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:46,500 : INFO : topic diff=0.285376, rho=0.246808\n",
      "2025-09-10 11:48:46,501 : INFO : PROGRESS: pass 8, at document #4000/14833\n",
      "2025-09-10 11:48:47,247 : INFO : optimized alpha [0.33943564, 0.19856328]\n",
      "2025-09-10 11:48:47,249 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:47,252 : INFO : topic #0 (0.339): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:47,253 : INFO : topic #1 (0.199): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:47,254 : INFO : topic diff=0.205910, rho=0.246808\n",
      "2025-09-10 11:48:47,255 : INFO : PROGRESS: pass 8, at document #6000/14833\n",
      "2025-09-10 11:48:48,024 : INFO : optimized alpha [0.35068512, 0.20308048]\n",
      "2025-09-10 11:48:48,026 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:48,030 : INFO : topic #0 (0.351): 0.014*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"someone\"\n",
      "2025-09-10 11:48:48,031 : INFO : topic #1 (0.203): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"get\" + 0.004*\"really\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:48,032 : INFO : topic diff=0.191251, rho=0.246808\n",
      "2025-09-10 11:48:48,033 : INFO : PROGRESS: pass 8, at document #8000/14833\n",
      "2025-09-10 11:48:48,744 : INFO : optimized alpha [0.36445022, 0.20642255]\n",
      "2025-09-10 11:48:48,746 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:48,749 : INFO : topic #0 (0.364): 0.013*\"like\" + 0.010*\"think\" + 0.010*\"people\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:48,750 : INFO : topic #1 (0.206): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:48,751 : INFO : topic diff=0.181906, rho=0.246808\n",
      "2025-09-10 11:48:48,751 : INFO : PROGRESS: pass 8, at document #10000/14833\n",
      "2025-09-10 11:48:49,469 : INFO : optimized alpha [0.3595486, 0.20697595]\n",
      "2025-09-10 11:48:49,472 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:49,475 : INFO : topic #0 (0.360): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"would\"\n",
      "2025-09-10 11:48:49,476 : INFO : topic #1 (0.207): 0.010*\"like\" + 0.009*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:49,477 : INFO : topic diff=0.190203, rho=0.246808\n",
      "2025-09-10 11:48:49,477 : INFO : PROGRESS: pass 8, at document #12000/14833\n",
      "2025-09-10 11:48:50,172 : INFO : optimized alpha [0.34189498, 0.20401055]\n",
      "2025-09-10 11:48:50,175 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:50,178 : INFO : topic #0 (0.342): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:50,179 : INFO : topic #1 (0.204): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"day\" + 0.004*\"still\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:50,179 : INFO : topic diff=0.197919, rho=0.246808\n",
      "2025-09-10 11:48:50,180 : INFO : PROGRESS: pass 8, at document #14000/14833\n",
      "2025-09-10 11:48:50,854 : INFO : optimized alpha [0.3430756, 0.22480232]\n",
      "2025-09-10 11:48:50,857 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:50,860 : INFO : topic #0 (0.343): 0.011*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:50,860 : INFO : topic #1 (0.225): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"would\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:50,861 : INFO : topic diff=0.426142, rho=0.246808\n",
      "2025-09-10 11:48:51,382 : INFO : -8.541 per-word bound, 372.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:51,383 : INFO : PROGRESS: pass 8, at document #14833/14833\n",
      "2025-09-10 11:48:51,661 : INFO : optimized alpha [0.29220724, 0.23760091]\n",
      "2025-09-10 11:48:51,663 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:51,666 : INFO : topic #0 (0.292): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:51,667 : INFO : topic #1 (0.238): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:51,667 : INFO : topic diff=0.342206, rho=0.246808\n",
      "2025-09-10 11:48:51,668 : INFO : PROGRESS: pass 9, at document #2000/14833\n",
      "2025-09-10 11:48:52,307 : INFO : optimized alpha [0.31469628, 0.20242977]\n",
      "2025-09-10 11:48:52,311 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:52,314 : INFO : topic #0 (0.315): 0.011*\"like\" + 0.010*\"people\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:52,315 : INFO : topic #1 (0.202): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"back\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:52,315 : INFO : topic diff=0.275244, rho=0.239618\n",
      "2025-09-10 11:48:52,319 : INFO : PROGRESS: pass 9, at document #4000/14833\n",
      "2025-09-10 11:48:53,007 : INFO : optimized alpha [0.34302154, 0.1974323]\n",
      "2025-09-10 11:48:53,009 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:53,012 : INFO : topic #0 (0.343): 0.012*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:48:53,013 : INFO : topic #1 (0.197): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:53,013 : INFO : topic diff=0.199278, rho=0.239618\n",
      "2025-09-10 11:48:53,014 : INFO : PROGRESS: pass 9, at document #6000/14833\n",
      "2025-09-10 11:48:53,705 : INFO : optimized alpha [0.35393625, 0.20162287]\n",
      "2025-09-10 11:48:53,709 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:53,712 : INFO : topic #0 (0.354): 0.014*\"like\" + 0.010*\"think\" + 0.010*\"people\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"might\" + 0.006*\"someone\"\n",
      "2025-09-10 11:48:53,713 : INFO : topic #1 (0.202): 0.011*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"get\" + 0.004*\"really\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:53,713 : INFO : topic diff=0.185082, rho=0.239618\n",
      "2025-09-10 11:48:53,714 : INFO : PROGRESS: pass 9, at document #8000/14833\n",
      "2025-09-10 11:48:54,415 : INFO : optimized alpha [0.367292, 0.20471537]\n",
      "2025-09-10 11:48:54,418 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:54,421 : INFO : topic #0 (0.367): 0.013*\"like\" + 0.010*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"someone\" + 0.006*\"really\"\n",
      "2025-09-10 11:48:54,422 : INFO : topic #1 (0.205): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:48:54,422 : INFO : topic diff=0.175923, rho=0.239618\n",
      "2025-09-10 11:48:54,423 : INFO : PROGRESS: pass 9, at document #10000/14833\n",
      "2025-09-10 11:48:55,136 : INFO : optimized alpha [0.3625026, 0.20507924]\n",
      "2025-09-10 11:48:55,139 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:55,142 : INFO : topic #0 (0.363): 0.012*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"someone\" + 0.005*\"might\" + 0.005*\"would\"\n",
      "2025-09-10 11:48:55,143 : INFO : topic #1 (0.205): 0.010*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"still\" + 0.004*\"get\"\n",
      "2025-09-10 11:48:55,144 : INFO : topic diff=0.183962, rho=0.239618\n",
      "2025-09-10 11:48:55,144 : INFO : PROGRESS: pass 9, at document #12000/14833\n",
      "2025-09-10 11:48:55,802 : INFO : optimized alpha [0.34474906, 0.20205912]\n",
      "2025-09-10 11:48:55,805 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:55,808 : INFO : topic #0 (0.345): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"time\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:48:55,809 : INFO : topic #1 (0.202): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"day\" + 0.004*\"still\" + 0.004*\"made\"\n",
      "2025-09-10 11:48:55,809 : INFO : topic diff=0.191660, rho=0.239618\n",
      "2025-09-10 11:48:55,810 : INFO : PROGRESS: pass 9, at document #14000/14833\n",
      "2025-09-10 11:48:56,478 : INFO : optimized alpha [0.3455546, 0.22207163]\n",
      "2025-09-10 11:48:56,481 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:56,484 : INFO : topic #0 (0.346): 0.011*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:56,485 : INFO : topic #1 (0.222): 0.010*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"years\" + 0.004*\"would\" + 0.004*\"said\"\n",
      "2025-09-10 11:48:56,486 : INFO : topic diff=0.412662, rho=0.239618\n",
      "2025-09-10 11:48:57,047 : INFO : -8.537 per-word bound, 371.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:48:57,048 : INFO : PROGRESS: pass 9, at document #14833/14833\n",
      "2025-09-10 11:48:57,350 : INFO : optimized alpha [0.29529706, 0.23442519]\n",
      "2025-09-10 11:48:57,352 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:48:57,355 : INFO : topic #0 (0.295): 0.011*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"life\"\n",
      "2025-09-10 11:48:57,356 : INFO : topic #1 (0.234): 0.009*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"back\" + 0.004*\"said\" + 0.004*\"even\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:48:57,356 : INFO : topic diff=0.332316, rho=0.239618\n",
      "2025-09-10 11:48:57,357 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=34546, num_topics=2, decay=0.5, chunksize=2000> in 62.54s', 'datetime': '2025-09-10T11:48:57.357404', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-09-10 11:48:57,359 : INFO : using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows\n",
      "2025-09-10 11:48:59,288 : INFO : 1 batches submitted to accumulate stats from 64 documents (2312 virtual)\n",
      "2025-09-10 11:48:59,301 : INFO : 2 batches submitted to accumulate stats from 128 documents (4776 virtual)\n",
      "2025-09-10 11:48:59,314 : INFO : 3 batches submitted to accumulate stats from 192 documents (8552 virtual)\n",
      "2025-09-10 11:48:59,326 : INFO : 4 batches submitted to accumulate stats from 256 documents (12770 virtual)\n",
      "2025-09-10 11:48:59,336 : INFO : 5 batches submitted to accumulate stats from 320 documents (15636 virtual)\n",
      "2025-09-10 11:48:59,347 : INFO : 6 batches submitted to accumulate stats from 384 documents (19247 virtual)\n",
      "2025-09-10 11:48:59,354 : INFO : 7 batches submitted to accumulate stats from 448 documents (20248 virtual)\n",
      "2025-09-10 11:48:59,362 : INFO : 8 batches submitted to accumulate stats from 512 documents (21583 virtual)\n",
      "2025-09-10 11:48:59,371 : INFO : 9 batches submitted to accumulate stats from 576 documents (24028 virtual)\n",
      "2025-09-10 11:48:59,379 : INFO : 10 batches submitted to accumulate stats from 640 documents (24977 virtual)\n",
      "2025-09-10 11:48:59,389 : INFO : 11 batches submitted to accumulate stats from 704 documents (26982 virtual)\n",
      "2025-09-10 11:48:59,398 : INFO : 12 batches submitted to accumulate stats from 768 documents (30059 virtual)\n",
      "2025-09-10 11:48:59,405 : INFO : 13 batches submitted to accumulate stats from 832 documents (30786 virtual)\n",
      "2025-09-10 11:48:59,415 : INFO : 14 batches submitted to accumulate stats from 896 documents (34101 virtual)\n",
      "2025-09-10 11:48:59,426 : INFO : 15 batches submitted to accumulate stats from 960 documents (34664 virtual)\n",
      "2025-09-10 11:48:59,440 : INFO : 16 batches submitted to accumulate stats from 1024 documents (38291 virtual)\n",
      "2025-09-10 11:48:59,448 : INFO : 17 batches submitted to accumulate stats from 1088 documents (40666 virtual)\n",
      "2025-09-10 11:48:59,460 : INFO : 18 batches submitted to accumulate stats from 1152 documents (42061 virtual)\n",
      "2025-09-10 11:48:59,469 : INFO : 19 batches submitted to accumulate stats from 1216 documents (44289 virtual)\n",
      "2025-09-10 11:48:59,479 : INFO : 20 batches submitted to accumulate stats from 1280 documents (47210 virtual)\n",
      "2025-09-10 11:48:59,487 : INFO : 21 batches submitted to accumulate stats from 1344 documents (48243 virtual)\n",
      "2025-09-10 11:48:59,494 : INFO : 22 batches submitted to accumulate stats from 1408 documents (48909 virtual)\n",
      "2025-09-10 11:48:59,505 : INFO : 23 batches submitted to accumulate stats from 1472 documents (49511 virtual)\n",
      "2025-09-10 11:48:59,517 : INFO : 24 batches submitted to accumulate stats from 1536 documents (51165 virtual)\n",
      "2025-09-10 11:48:59,527 : INFO : 25 batches submitted to accumulate stats from 1600 documents (51700 virtual)\n",
      "2025-09-10 11:48:59,537 : INFO : 26 batches submitted to accumulate stats from 1664 documents (54911 virtual)\n",
      "2025-09-10 11:48:59,553 : INFO : 27 batches submitted to accumulate stats from 1728 documents (58811 virtual)\n",
      "2025-09-10 11:48:59,568 : INFO : 28 batches submitted to accumulate stats from 1792 documents (62711 virtual)\n",
      "2025-09-10 11:48:59,577 : INFO : 29 batches submitted to accumulate stats from 1856 documents (64395 virtual)\n",
      "2025-09-10 11:48:59,586 : INFO : 30 batches submitted to accumulate stats from 1920 documents (67132 virtual)\n",
      "2025-09-10 11:48:59,598 : INFO : 31 batches submitted to accumulate stats from 1984 documents (69130 virtual)\n",
      "2025-09-10 11:48:59,613 : INFO : 32 batches submitted to accumulate stats from 2048 documents (71562 virtual)\n",
      "2025-09-10 11:48:59,621 : INFO : 33 batches submitted to accumulate stats from 2112 documents (72821 virtual)\n",
      "2025-09-10 11:48:59,629 : INFO : 34 batches submitted to accumulate stats from 2176 documents (73606 virtual)\n",
      "2025-09-10 11:48:59,636 : INFO : 35 batches submitted to accumulate stats from 2240 documents (73967 virtual)\n",
      "2025-09-10 11:48:59,645 : INFO : 36 batches submitted to accumulate stats from 2304 documents (75970 virtual)\n",
      "2025-09-10 11:48:59,654 : INFO : 37 batches submitted to accumulate stats from 2368 documents (77816 virtual)\n",
      "2025-09-10 11:48:59,661 : INFO : 38 batches submitted to accumulate stats from 2432 documents (78243 virtual)\n",
      "2025-09-10 11:48:59,669 : INFO : 39 batches submitted to accumulate stats from 2496 documents (80113 virtual)\n",
      "2025-09-10 11:48:59,685 : INFO : 40 batches submitted to accumulate stats from 2560 documents (83282 virtual)\n",
      "2025-09-10 11:48:59,693 : INFO : 41 batches submitted to accumulate stats from 2624 documents (84555 virtual)\n",
      "2025-09-10 11:48:59,700 : INFO : 42 batches submitted to accumulate stats from 2688 documents (85006 virtual)\n",
      "2025-09-10 11:48:59,709 : INFO : 43 batches submitted to accumulate stats from 2752 documents (85561 virtual)\n",
      "2025-09-10 11:48:59,718 : INFO : 44 batches submitted to accumulate stats from 2816 documents (87847 virtual)\n",
      "2025-09-10 11:48:59,725 : INFO : 45 batches submitted to accumulate stats from 2880 documents (88294 virtual)\n",
      "2025-09-10 11:48:59,732 : INFO : 46 batches submitted to accumulate stats from 2944 documents (89254 virtual)\n",
      "2025-09-10 11:48:59,739 : INFO : 47 batches submitted to accumulate stats from 3008 documents (90361 virtual)\n",
      "2025-09-10 11:48:59,752 : INFO : 49 batches submitted to accumulate stats from 3136 documents (90494 virtual)\n",
      "2025-09-10 11:48:59,759 : INFO : 50 batches submitted to accumulate stats from 3200 documents (91230 virtual)\n",
      "2025-09-10 11:48:59,766 : INFO : 51 batches submitted to accumulate stats from 3264 documents (92239 virtual)\n",
      "2025-09-10 11:48:59,774 : INFO : 52 batches submitted to accumulate stats from 3328 documents (93051 virtual)\n",
      "2025-09-10 11:48:59,781 : INFO : 53 batches submitted to accumulate stats from 3392 documents (93654 virtual)\n",
      "2025-09-10 11:48:59,818 : INFO : 59 batches submitted to accumulate stats from 3776 documents (90837 virtual)\n",
      "2025-09-10 11:48:59,830 : INFO : 61 batches submitted to accumulate stats from 3904 documents (89441 virtual)\n",
      "2025-09-10 11:48:59,843 : INFO : 63 batches submitted to accumulate stats from 4032 documents (90763 virtual)\n",
      "2025-09-10 11:48:59,867 : INFO : 67 batches submitted to accumulate stats from 4288 documents (89512 virtual)\n",
      "2025-09-10 11:48:59,899 : INFO : 73 batches submitted to accumulate stats from 4672 documents (84248 virtual)\n",
      "2025-09-10 11:48:59,907 : INFO : 74 batches submitted to accumulate stats from 4736 documents (84612 virtual)\n",
      "2025-09-10 11:48:59,914 : INFO : 75 batches submitted to accumulate stats from 4800 documents (85605 virtual)\n",
      "2025-09-10 11:48:59,927 : INFO : 77 batches submitted to accumulate stats from 4928 documents (86303 virtual)\n",
      "2025-09-10 11:48:59,938 : INFO : 78 batches submitted to accumulate stats from 4992 documents (86705 virtual)\n",
      "2025-09-10 11:48:59,945 : INFO : 79 batches submitted to accumulate stats from 5056 documents (87250 virtual)\n",
      "2025-09-10 11:48:59,955 : INFO : 80 batches submitted to accumulate stats from 5120 documents (87252 virtual)\n",
      "2025-09-10 11:48:59,973 : INFO : 83 batches submitted to accumulate stats from 5312 documents (85664 virtual)\n",
      "2025-09-10 11:48:59,980 : INFO : 84 batches submitted to accumulate stats from 5376 documents (85668 virtual)\n",
      "2025-09-10 11:49:00,008 : INFO : 87 batches submitted to accumulate stats from 5568 documents (85126 virtual)\n",
      "2025-09-10 11:49:00,035 : INFO : 91 batches submitted to accumulate stats from 5824 documents (83475 virtual)\n",
      "2025-09-10 11:49:00,047 : INFO : 93 batches submitted to accumulate stats from 5952 documents (83419 virtual)\n",
      "2025-09-10 11:49:00,201 : INFO : 120 batches submitted to accumulate stats from 7680 documents (53159 virtual)\n",
      "2025-09-10 11:49:00,328 : INFO : 144 batches submitted to accumulate stats from 9216 documents (25428 virtual)\n",
      "2025-09-10 11:49:00,377 : INFO : 153 batches submitted to accumulate stats from 9792 documents (18399 virtual)\n",
      "2025-09-10 11:49:00,400 : INFO : 157 batches submitted to accumulate stats from 10048 documents (15860 virtual)\n",
      "2025-09-10 11:49:00,403 : INFO : 158 batches submitted to accumulate stats from 10112 documents (16981 virtual)\n",
      "2025-09-10 11:49:00,407 : INFO : 160 batches submitted to accumulate stats from 10240 documents (14423 virtual)\n",
      "2025-09-10 11:49:00,413 : INFO : 165 batches submitted to accumulate stats from 10560 documents (7614 virtual)\n",
      "2025-09-10 11:49:00,417 : INFO : 167 batches submitted to accumulate stats from 10688 documents (8130 virtual)\n",
      "2025-09-10 11:49:00,421 : INFO : 169 batches submitted to accumulate stats from 10816 documents (9358 virtual)\n",
      "2025-09-10 11:49:00,425 : INFO : 171 batches submitted to accumulate stats from 10944 documents (10141 virtual)\n",
      "2025-09-10 11:49:00,434 : INFO : 177 batches submitted to accumulate stats from 11328 documents (598 virtual)\n",
      "2025-09-10 11:49:00,465 : INFO : 192 batches submitted to accumulate stats from 12288 documents (-20530 virtual)\n",
      "2025-09-10 11:49:00,469 : INFO : 194 batches submitted to accumulate stats from 12416 documents (-21842 virtual)\n",
      "2025-09-10 11:49:00,479 : INFO : 199 batches submitted to accumulate stats from 12736 documents (-21354 virtual)\n",
      "2025-09-10 11:49:00,482 : INFO : 200 batches submitted to accumulate stats from 12800 documents (-18732 virtual)\n",
      "2025-09-10 11:49:00,486 : INFO : 201 batches submitted to accumulate stats from 12864 documents (-16983 virtual)\n",
      "2025-09-10 11:49:00,489 : INFO : 202 batches submitted to accumulate stats from 12928 documents (-10231 virtual)\n",
      "2025-09-10 11:49:00,492 : INFO : 203 batches submitted to accumulate stats from 12992 documents (-2873 virtual)\n",
      "2025-09-10 11:49:00,496 : INFO : 204 batches submitted to accumulate stats from 13056 documents (2095 virtual)\n",
      "2025-09-10 11:49:00,501 : INFO : 206 batches submitted to accumulate stats from 13184 documents (1129 virtual)\n",
      "2025-09-10 11:49:00,510 : INFO : 210 batches submitted to accumulate stats from 13440 documents (33 virtual)\n",
      "2025-09-10 11:49:00,515 : INFO : 211 batches submitted to accumulate stats from 13504 documents (766 virtual)\n",
      "2025-09-10 11:49:00,518 : INFO : 212 batches submitted to accumulate stats from 13568 documents (2126 virtual)\n",
      "2025-09-10 11:49:00,521 : INFO : 213 batches submitted to accumulate stats from 13632 documents (2894 virtual)\n",
      "2025-09-10 11:49:00,525 : INFO : 215 batches submitted to accumulate stats from 13760 documents (4686 virtual)\n",
      "2025-09-10 11:49:00,530 : INFO : 216 batches submitted to accumulate stats from 13824 documents (5912 virtual)\n",
      "2025-09-10 11:49:00,532 : INFO : 217 batches submitted to accumulate stats from 13888 documents (9533 virtual)\n",
      "2025-09-10 11:49:00,540 : INFO : 222 batches submitted to accumulate stats from 14208 documents (9279 virtual)\n",
      "2025-09-10 11:49:00,543 : INFO : 223 batches submitted to accumulate stats from 14272 documents (14537 virtual)\n",
      "2025-09-10 11:49:00,545 : INFO : 224 batches submitted to accumulate stats from 14336 documents (16740 virtual)\n",
      "2025-09-10 11:49:00,554 : INFO : 225 batches submitted to accumulate stats from 14400 documents (17807 virtual)\n",
      "2025-09-10 11:49:00,565 : INFO : 226 batches submitted to accumulate stats from 14464 documents (18030 virtual)\n",
      "2025-09-10 11:49:00,569 : INFO : 228 batches submitted to accumulate stats from 14592 documents (19290 virtual)\n",
      "2025-09-10 11:49:00,573 : INFO : 230 batches submitted to accumulate stats from 14720 documents (21691 virtual)\n",
      "2025-09-10 11:49:00,575 : INFO : 231 batches submitted to accumulate stats from 14784 documents (23450 virtual)\n",
      "2025-09-10 11:49:00,577 : INFO : 232 batches submitted to accumulate stats from 14848 documents (23484 virtual)\n",
      "2025-09-10 11:49:00,646 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,648 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,653 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,650 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,652 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,654 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,654 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,654 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,654 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,654 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,659 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,658 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,659 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,660 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,657 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,656 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,655 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,655 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,655 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,659 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,657 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,659 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,656 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,659 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,659 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,659 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,660 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,662 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,658 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,664 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,665 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,663 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,668 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,668 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,668 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,668 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,668 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,665 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,666 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,669 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,668 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,669 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,670 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,671 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,672 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,671 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,672 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,672 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,667 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,673 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,672 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,672 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,674 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,670 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,675 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,675 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,678 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,680 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,673 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,678 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,685 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,672 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,674 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,672 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,680 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,682 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,671 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,674 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,686 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,681 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,686 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,694 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,685 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,690 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,724 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,698 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,706 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,700 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,713 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,709 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,711 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,702 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,734 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,714 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,701 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,693 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,734 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,720 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,753 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,737 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,751 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,738 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,715 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,771 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,734 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,737 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,741 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,794 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,794 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,757 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,758 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,763 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,758 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,758 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,759 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,766 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,762 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,772 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,780 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,778 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,763 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,785 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,784 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,786 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,786 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,798 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,813 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,806 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,789 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,811 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,813 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,810 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,814 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,815 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,878 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,673 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,808 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,661 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,672 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,821 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,834 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,832 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,814 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,839 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,833 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,840 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,839 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,843 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,841 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,847 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,844 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,842 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,844 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,833 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,845 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,887 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,866 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,866 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,843 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,935 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,878 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,882 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,881 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,879 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,879 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,814 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:49:00,662 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,687 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,676 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,673 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,848 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,670 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,669 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,818 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,664 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,818 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,667 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,668 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,667 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,839 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,792 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,675 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,704 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,671 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,812 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,850 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,674 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,847 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:00,843 : INFO : accumulator serialized\n",
      "2025-09-10 11:49:03,213 : INFO : 127 accumulators retrieved from output queue\n",
      "2025-09-10 11:49:03,252 : INFO : accumulated word occurrence stats for 296776 virtual documents\n",
      "2025-09-10 11:49:03,600 : INFO : using autotuned alpha, starting with [0.33333334, 0.33333334, 0.33333334]\n",
      "2025-09-10 11:49:03,601 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2025-09-10 11:49:03,606 : INFO : using serial LDA version on this node\n",
      "2025-09-10 11:49:03,614 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 14833 documents, updating model once every 2000 documents, evaluating perplexity every 14833 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-09-10 11:49:03,615 : INFO : PROGRESS: pass 0, at document #2000/14833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 2 - Coherence: 0.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:49:04,805 : INFO : optimized alpha [0.61386144, 0.5548838, 0.55519634]\n",
      "2025-09-10 11:49:04,809 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:04,813 : INFO : topic #0 (0.614): 0.013*\"like\" + 0.009*\"one\" + 0.009*\"might\" + 0.008*\"people\" + 0.007*\"think\" + 0.007*\"something\" + 0.006*\"really\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:04,814 : INFO : topic #1 (0.555): 0.011*\"like\" + 0.010*\"one\" + 0.009*\"people\" + 0.007*\"even\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"time\" + 0.006*\"might\" + 0.005*\"often\" + 0.005*\"way\"\n",
      "2025-09-10 11:49:04,815 : INFO : topic #2 (0.555): 0.012*\"like\" + 0.008*\"think\" + 0.007*\"something\" + 0.006*\"one\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"also\" + 0.005*\"could\" + 0.005*\"even\"\n",
      "2025-09-10 11:49:04,815 : INFO : topic diff=3.914534, rho=1.000000\n",
      "2025-09-10 11:49:04,816 : INFO : PROGRESS: pass 0, at document #4000/14833\n",
      "2025-09-10 11:49:05,959 : INFO : optimized alpha [0.78982055, 0.6772775, 0.6992193]\n",
      "2025-09-10 11:49:05,963 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:05,967 : INFO : topic #0 (0.790): 0.014*\"like\" + 0.010*\"one\" + 0.009*\"think\" + 0.009*\"people\" + 0.008*\"really\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:05,967 : INFO : topic #1 (0.677): 0.012*\"like\" + 0.011*\"one\" + 0.009*\"people\" + 0.007*\"think\" + 0.007*\"life\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"way\" + 0.004*\"get\"\n",
      "2025-09-10 11:49:05,968 : INFO : topic #2 (0.699): 0.013*\"like\" + 0.010*\"think\" + 0.008*\"would\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"also\" + 0.004*\"people\" + 0.004*\"things\"\n",
      "2025-09-10 11:49:05,968 : INFO : topic diff=0.642683, rho=0.707107\n",
      "2025-09-10 11:49:05,969 : INFO : PROGRESS: pass 0, at document #6000/14833\n",
      "2025-09-10 11:49:07,133 : INFO : optimized alpha [0.73325545, 0.6505111, 0.75163937]\n",
      "2025-09-10 11:49:07,136 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:07,140 : INFO : topic #0 (0.733): 0.017*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.009*\"one\" + 0.009*\"really\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"also\" + 0.006*\"might\"\n",
      "2025-09-10 11:49:07,141 : INFO : topic #1 (0.651): 0.015*\"like\" + 0.011*\"one\" + 0.009*\"people\" + 0.008*\"think\" + 0.008*\"even\" + 0.007*\"really\" + 0.006*\"time\" + 0.006*\"way\" + 0.005*\"get\" + 0.005*\"okay\"\n",
      "2025-09-10 11:49:07,141 : INFO : topic #2 (0.752): 0.016*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"something\" + 0.006*\"would\" + 0.006*\"know\" + 0.005*\"okay\" + 0.005*\"really\"\n",
      "2025-09-10 11:49:07,142 : INFO : topic diff=0.524022, rho=0.577350\n",
      "2025-09-10 11:49:07,143 : INFO : PROGRESS: pass 0, at document #8000/14833\n",
      "2025-09-10 11:49:08,279 : INFO : optimized alpha [0.60980403, 0.53638244, 0.62549424]\n",
      "2025-09-10 11:49:08,283 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:08,287 : INFO : topic #0 (0.610): 0.014*\"like\" + 0.011*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"really\" + 0.006*\"even\" + 0.006*\"might\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:08,288 : INFO : topic #1 (0.536): 0.012*\"like\" + 0.009*\"people\" + 0.009*\"one\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"time\" + 0.005*\"actually\" + 0.005*\"get\" + 0.005*\"life\"\n",
      "2025-09-10 11:49:08,289 : INFO : topic #2 (0.625): 0.014*\"like\" + 0.009*\"think\" + 0.008*\"would\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"know\" + 0.004*\"people\" + 0.004*\"actually\"\n",
      "2025-09-10 11:49:08,289 : INFO : topic diff=0.462698, rho=0.500000\n",
      "2025-09-10 11:49:08,291 : INFO : PROGRESS: pass 0, at document #10000/14833\n",
      "2025-09-10 11:49:09,450 : INFO : optimized alpha [0.47853172, 0.45071447, 0.54452777]\n",
      "2025-09-10 11:49:09,454 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:09,458 : INFO : topic #0 (0.479): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"might\" + 0.006*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:09,459 : INFO : topic #1 (0.451): 0.011*\"like\" + 0.009*\"people\" + 0.008*\"one\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"actually\" + 0.005*\"think\" + 0.005*\"way\" + 0.005*\"time\" + 0.005*\"really\"\n",
      "2025-09-10 11:49:09,460 : INFO : topic #2 (0.545): 0.012*\"like\" + 0.008*\"would\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"something\" + 0.006*\"time\" + 0.004*\"actually\" + 0.004*\"people\" + 0.004*\"back\"\n",
      "2025-09-10 11:49:09,460 : INFO : topic diff=0.405359, rho=0.447214\n",
      "2025-09-10 11:49:09,462 : INFO : PROGRESS: pass 0, at document #12000/14833\n",
      "2025-09-10 11:49:10,507 : INFO : optimized alpha [0.39208546, 0.36384597, 0.48308754]\n",
      "2025-09-10 11:49:10,511 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:10,516 : INFO : topic #0 (0.392): 0.014*\"people\" + 0.012*\"like\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.006*\"might\" + 0.005*\"way\" + 0.005*\"time\"\n",
      "2025-09-10 11:49:10,517 : INFO : topic #1 (0.364): 0.010*\"like\" + 0.009*\"people\" + 0.008*\"one\" + 0.007*\"even\" + 0.005*\"life\" + 0.005*\"way\" + 0.005*\"actually\" + 0.005*\"time\" + 0.004*\"think\" + 0.004*\"really\"\n",
      "2025-09-10 11:49:10,518 : INFO : topic #2 (0.483): 0.013*\"like\" + 0.009*\"one\" + 0.007*\"even\" + 0.007*\"would\" + 0.006*\"time\" + 0.005*\"something\" + 0.005*\"think\" + 0.004*\"people\" + 0.004*\"still\" + 0.004*\"way\"\n",
      "2025-09-10 11:49:10,518 : INFO : topic diff=0.370306, rho=0.408248\n",
      "2025-09-10 11:49:10,519 : INFO : PROGRESS: pass 0, at document #14000/14833\n",
      "2025-09-10 11:49:11,607 : INFO : optimized alpha [0.39416364, 0.37283817, 0.53077847]\n",
      "2025-09-10 11:49:11,612 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:11,616 : INFO : topic #0 (0.394): 0.013*\"people\" + 0.010*\"like\" + 0.008*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"think\" + 0.006*\"someone\" + 0.005*\"way\" + 0.005*\"might\"\n",
      "2025-09-10 11:49:11,617 : INFO : topic #1 (0.373): 0.008*\"like\" + 0.008*\"one\" + 0.008*\"people\" + 0.006*\"life\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"day\" + 0.005*\"get\" + 0.004*\"way\" + 0.004*\"really\"\n",
      "2025-09-10 11:49:11,618 : INFO : topic #2 (0.531): 0.010*\"like\" + 0.009*\"would\" + 0.009*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.004*\"said\" + 0.004*\"back\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"something\"\n",
      "2025-09-10 11:49:11,618 : INFO : topic diff=0.834904, rho=0.377964\n",
      "2025-09-10 11:49:12,331 : INFO : -8.775 per-word bound, 438.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:12,331 : INFO : PROGRESS: pass 0, at document #14833/14833\n",
      "2025-09-10 11:49:12,774 : INFO : optimized alpha [0.33922267, 0.3618974, 0.5569782]\n",
      "2025-09-10 11:49:12,777 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:12,781 : INFO : topic #0 (0.339): 0.013*\"people\" + 0.009*\"like\" + 0.008*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"someone\" + 0.006*\"think\" + 0.006*\"something\" + 0.005*\"know\" + 0.005*\"way\"\n",
      "2025-09-10 11:49:12,782 : INFO : topic #1 (0.362): 0.008*\"one\" + 0.007*\"people\" + 0.007*\"like\" + 0.005*\"get\" + 0.005*\"life\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"time\" + 0.004*\"way\" + 0.003*\"much\"\n",
      "2025-09-10 11:49:12,783 : INFO : topic #2 (0.557): 0.009*\"would\" + 0.009*\"one\" + 0.007*\"like\" + 0.007*\"time\" + 0.005*\"said\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"got\" + 0.004*\"could\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:12,783 : INFO : topic diff=0.604010, rho=0.353553\n",
      "2025-09-10 11:49:12,784 : INFO : PROGRESS: pass 1, at document #2000/14833\n",
      "2025-09-10 11:49:13,815 : INFO : optimized alpha [0.36525154, 0.25822058, 0.32484695]\n",
      "2025-09-10 11:49:13,819 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:13,822 : INFO : topic #0 (0.365): 0.011*\"like\" + 0.011*\"people\" + 0.008*\"might\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"someone\" + 0.005*\"also\"\n",
      "2025-09-10 11:49:13,824 : INFO : topic #1 (0.258): 0.008*\"one\" + 0.008*\"like\" + 0.007*\"people\" + 0.005*\"life\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"time\" + 0.004*\"really\" + 0.004*\"much\"\n",
      "2025-09-10 11:49:13,825 : INFO : topic #2 (0.325): 0.010*\"one\" + 0.009*\"would\" + 0.008*\"like\" + 0.007*\"time\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"back\" + 0.004*\"could\" + 0.004*\"got\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:13,825 : INFO : topic diff=0.455074, rho=0.325878\n",
      "2025-09-10 11:49:13,826 : INFO : PROGRESS: pass 1, at document #4000/14833\n",
      "2025-09-10 11:49:14,840 : INFO : optimized alpha [0.3859364, 0.22232491, 0.3003437]\n",
      "2025-09-10 11:49:14,844 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:14,848 : INFO : topic #0 (0.386): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:14,849 : INFO : topic #1 (0.222): 0.009*\"like\" + 0.008*\"one\" + 0.006*\"people\" + 0.005*\"life\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"time\" + 0.004*\"really\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:14,850 : INFO : topic #2 (0.300): 0.010*\"one\" + 0.009*\"like\" + 0.009*\"would\" + 0.007*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.004*\"could\" + 0.004*\"said\" + 0.004*\"got\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:14,850 : INFO : topic diff=0.317389, rho=0.325878\n",
      "2025-09-10 11:49:14,851 : INFO : PROGRESS: pass 1, at document #6000/14833\n",
      "2025-09-10 11:49:15,767 : INFO : optimized alpha [0.35379773, 0.18867102, 0.28752747]\n",
      "2025-09-10 11:49:15,771 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:15,775 : INFO : topic #0 (0.354): 0.015*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"someone\" + 0.007*\"really\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:15,776 : INFO : topic #1 (0.189): 0.010*\"like\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"even\" + 0.005*\"get\" + 0.005*\"day\" + 0.005*\"think\" + 0.005*\"life\" + 0.005*\"really\" + 0.005*\"time\"\n",
      "2025-09-10 11:49:15,776 : INFO : topic #2 (0.288): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"would\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"think\" + 0.004*\"little\"\n",
      "2025-09-10 11:49:15,777 : INFO : topic diff=0.286767, rho=0.325878\n",
      "2025-09-10 11:49:15,778 : INFO : PROGRESS: pass 1, at document #8000/14833\n",
      "2025-09-10 11:49:16,636 : INFO : optimized alpha [0.3301328, 0.17330264, 0.2722974]\n",
      "2025-09-10 11:49:16,640 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:16,643 : INFO : topic #0 (0.330): 0.013*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.009*\"something\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"really\" + 0.006*\"also\"\n",
      "2025-09-10 11:49:16,644 : INFO : topic #1 (0.173): 0.009*\"like\" + 0.007*\"people\" + 0.006*\"one\" + 0.005*\"think\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"actually\" + 0.005*\"life\" + 0.005*\"day\" + 0.005*\"really\"\n",
      "2025-09-10 11:49:16,645 : INFO : topic #2 (0.272): 0.012*\"like\" + 0.010*\"one\" + 0.008*\"would\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"think\" + 0.004*\"really\" + 0.004*\"know\" + 0.004*\"something\"\n",
      "2025-09-10 11:49:16,646 : INFO : topic diff=0.272142, rho=0.325878\n",
      "2025-09-10 11:49:16,647 : INFO : PROGRESS: pass 1, at document #10000/14833\n",
      "2025-09-10 11:49:17,483 : INFO : optimized alpha [0.2928739, 0.15905753, 0.25638422]\n",
      "2025-09-10 11:49:17,487 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:17,492 : INFO : topic #0 (0.293): 0.013*\"like\" + 0.013*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:17,492 : INFO : topic #1 (0.159): 0.009*\"like\" + 0.007*\"people\" + 0.006*\"one\" + 0.005*\"even\" + 0.005*\"actually\" + 0.005*\"life\" + 0.005*\"get\" + 0.004*\"day\" + 0.004*\"think\" + 0.004*\"way\"\n",
      "2025-09-10 11:49:17,493 : INFO : topic #2 (0.256): 0.011*\"like\" + 0.010*\"one\" + 0.007*\"would\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.004*\"years\" + 0.004*\"something\" + 0.004*\"still\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:17,494 : INFO : topic diff=0.278800, rho=0.325878\n",
      "2025-09-10 11:49:17,495 : INFO : PROGRESS: pass 1, at document #12000/14833\n",
      "2025-09-10 11:49:18,315 : INFO : optimized alpha [0.26053435, 0.14213207, 0.2420087]\n",
      "2025-09-10 11:49:18,319 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:18,323 : INFO : topic #0 (0.261): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:18,324 : INFO : topic #1 (0.142): 0.009*\"like\" + 0.007*\"people\" + 0.006*\"even\" + 0.006*\"one\" + 0.005*\"actually\" + 0.004*\"get\" + 0.004*\"life\" + 0.004*\"think\" + 0.004*\"day\" + 0.004*\"way\"\n",
      "2025-09-10 11:49:18,325 : INFO : topic #2 (0.242): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"would\" + 0.005*\"back\" + 0.004*\"still\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"something\"\n",
      "2025-09-10 11:49:18,325 : INFO : topic diff=0.276459, rho=0.325878\n",
      "2025-09-10 11:49:18,326 : INFO : PROGRESS: pass 1, at document #14000/14833\n",
      "2025-09-10 11:49:19,194 : INFO : optimized alpha [0.25723082, 0.14766501, 0.26766562]\n",
      "2025-09-10 11:49:19,197 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:19,201 : INFO : topic #0 (0.257): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\" + 0.005*\"way\"\n",
      "2025-09-10 11:49:19,202 : INFO : topic #1 (0.148): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"one\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"day\" + 0.005*\"life\" + 0.004*\"time\" + 0.004*\"money\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:19,203 : INFO : topic #2 (0.268): 0.010*\"one\" + 0.009*\"like\" + 0.008*\"would\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"got\" + 0.004*\"still\"\n",
      "2025-09-10 11:49:19,204 : INFO : topic diff=0.608975, rho=0.325878\n",
      "2025-09-10 11:49:19,780 : INFO : -8.609 per-word bound, 390.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:19,781 : INFO : PROGRESS: pass 1, at document #14833/14833\n",
      "2025-09-10 11:49:20,117 : INFO : optimized alpha [0.21832162, 0.14762767, 0.2852751]\n",
      "2025-09-10 11:49:20,121 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:20,125 : INFO : topic #0 (0.218): 0.014*\"people\" + 0.010*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"something\" + 0.006*\"life\" + 0.005*\"way\"\n",
      "2025-09-10 11:49:20,126 : INFO : topic #1 (0.148): 0.006*\"people\" + 0.006*\"like\" + 0.006*\"one\" + 0.005*\"get\" + 0.005*\"money\" + 0.004*\"even\" + 0.004*\"day\" + 0.004*\"time\" + 0.004*\"life\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:49:20,127 : INFO : topic #2 (0.285): 0.010*\"one\" + 0.008*\"would\" + 0.007*\"like\" + 0.007*\"time\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"even\" + 0.004*\"got\" + 0.004*\"day\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:20,128 : INFO : topic diff=0.472148, rho=0.325878\n",
      "2025-09-10 11:49:20,128 : INFO : PROGRESS: pass 2, at document #2000/14833\n",
      "2025-09-10 11:49:20,959 : INFO : optimized alpha [0.24988356, 0.13153967, 0.2244825]\n",
      "2025-09-10 11:49:20,962 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:20,966 : INFO : topic #0 (0.250): 0.012*\"like\" + 0.011*\"people\" + 0.008*\"think\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:20,967 : INFO : topic #1 (0.132): 0.007*\"like\" + 0.006*\"one\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"day\" + 0.004*\"life\" + 0.004*\"also\" + 0.004*\"time\" + 0.004*\"money\"\n",
      "2025-09-10 11:49:20,968 : INFO : topic #2 (0.224): 0.011*\"one\" + 0.008*\"like\" + 0.008*\"would\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.004*\"day\" + 0.004*\"could\" + 0.004*\"got\"\n",
      "2025-09-10 11:49:20,968 : INFO : topic diff=0.386156, rho=0.309841\n",
      "2025-09-10 11:49:20,969 : INFO : PROGRESS: pass 2, at document #4000/14833\n",
      "2025-09-10 11:49:21,782 : INFO : optimized alpha [0.27967772, 0.12303792, 0.22020222]\n",
      "2025-09-10 11:49:21,786 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:21,790 : INFO : topic #0 (0.280): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:21,791 : INFO : topic #1 (0.123): 0.008*\"like\" + 0.006*\"one\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"even\" + 0.004*\"day\" + 0.004*\"life\" + 0.004*\"money\" + 0.004*\"language\"\n",
      "2025-09-10 11:49:21,791 : INFO : topic #2 (0.220): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"would\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.004*\"day\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"could\"\n",
      "2025-09-10 11:49:21,792 : INFO : topic diff=0.272691, rho=0.309841\n",
      "2025-09-10 11:49:21,793 : INFO : PROGRESS: pass 2, at document #6000/14833\n",
      "2025-09-10 11:49:22,601 : INFO : optimized alpha [0.28265315, 0.1146933, 0.22279763]\n",
      "2025-09-10 11:49:22,604 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:22,608 : INFO : topic #0 (0.283): 0.015*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"someone\" + 0.007*\"really\" + 0.006*\"things\"\n",
      "2025-09-10 11:49:22,609 : INFO : topic #1 (0.115): 0.009*\"like\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"one\" + 0.005*\"get\" + 0.005*\"think\" + 0.005*\"day\" + 0.005*\"also\" + 0.005*\"money\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:22,610 : INFO : topic #2 (0.223): 0.012*\"like\" + 0.012*\"one\" + 0.007*\"time\" + 0.006*\"would\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"know\" + 0.004*\"got\" + 0.004*\"little\"\n",
      "2025-09-10 11:49:22,610 : INFO : topic diff=0.250086, rho=0.309841\n",
      "2025-09-10 11:49:22,611 : INFO : PROGRESS: pass 2, at document #8000/14833\n",
      "2025-09-10 11:49:23,389 : INFO : optimized alpha [0.28296712, 0.113368616, 0.22249755]\n",
      "2025-09-10 11:49:23,396 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:23,401 : INFO : topic #0 (0.283): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.009*\"something\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:23,402 : INFO : topic #1 (0.113): 0.008*\"like\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"think\" + 0.005*\"one\" + 0.005*\"even\" + 0.005*\"actually\" + 0.005*\"also\" + 0.005*\"money\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:23,402 : INFO : topic #2 (0.222): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"would\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:23,403 : INFO : topic diff=0.242405, rho=0.309841\n",
      "2025-09-10 11:49:23,404 : INFO : PROGRESS: pass 2, at document #10000/14833\n",
      "2025-09-10 11:49:24,150 : INFO : optimized alpha [0.26851174, 0.11036843, 0.21829236]\n",
      "2025-09-10 11:49:24,153 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:24,157 : INFO : topic #0 (0.269): 0.013*\"like\" + 0.013*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:24,158 : INFO : topic #1 (0.110): 0.008*\"like\" + 0.007*\"people\" + 0.005*\"actually\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"money\" + 0.005*\"one\" + 0.004*\"think\" + 0.004*\"day\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:24,159 : INFO : topic #2 (0.218): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"years\" + 0.004*\"got\" + 0.004*\"still\" + 0.004*\"really\"\n",
      "2025-09-10 11:49:24,159 : INFO : topic diff=0.250566, rho=0.309841\n",
      "2025-09-10 11:49:24,160 : INFO : PROGRESS: pass 2, at document #12000/14833\n",
      "2025-09-10 11:49:24,889 : INFO : optimized alpha [0.24851361, 0.104581915, 0.21192408]\n",
      "2025-09-10 11:49:24,893 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:24,897 : INFO : topic #0 (0.249): 0.014*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:24,898 : INFO : topic #1 (0.105): 0.008*\"like\" + 0.007*\"people\" + 0.006*\"even\" + 0.005*\"one\" + 0.005*\"money\" + 0.005*\"actually\" + 0.004*\"sleep\" + 0.004*\"get\" + 0.004*\"think\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:24,898 : INFO : topic #2 (0.212): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.005*\"years\" + 0.004*\"made\"\n",
      "2025-09-10 11:49:24,899 : INFO : topic diff=0.250804, rho=0.309841\n",
      "2025-09-10 11:49:24,899 : INFO : PROGRESS: pass 2, at document #14000/14833\n",
      "2025-09-10 11:49:25,666 : INFO : optimized alpha [0.24762706, 0.11066946, 0.23594923]\n",
      "2025-09-10 11:49:25,669 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:25,673 : INFO : topic #0 (0.248): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:25,674 : INFO : topic #1 (0.111): 0.007*\"like\" + 0.007*\"people\" + 0.005*\"money\" + 0.005*\"one\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"day\" + 0.004*\"sleep\" + 0.004*\"life\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:25,675 : INFO : topic #2 (0.236): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.007*\"would\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"years\" + 0.005*\"got\" + 0.005*\"said\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:25,675 : INFO : topic diff=0.549565, rho=0.309841\n",
      "2025-09-10 11:49:26,216 : INFO : -8.556 per-word bound, 376.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:26,217 : INFO : PROGRESS: pass 2, at document #14833/14833\n",
      "2025-09-10 11:49:26,515 : INFO : optimized alpha [0.2110329, 0.11256564, 0.25220844]\n",
      "2025-09-10 11:49:26,518 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:26,521 : INFO : topic #0 (0.211): 0.013*\"people\" + 0.010*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"something\" + 0.006*\"life\" + 0.005*\"way\"\n",
      "2025-09-10 11:49:26,522 : INFO : topic #1 (0.113): 0.006*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.005*\"one\" + 0.004*\"even\" + 0.004*\"teeth\" + 0.004*\"jobs\" + 0.004*\"also\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:26,523 : INFO : topic #2 (0.252): 0.010*\"one\" + 0.007*\"like\" + 0.007*\"would\" + 0.006*\"time\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:26,523 : INFO : topic diff=0.420620, rho=0.309841\n",
      "2025-09-10 11:49:26,524 : INFO : PROGRESS: pass 3, at document #2000/14833\n",
      "2025-09-10 11:49:27,292 : INFO : optimized alpha [0.24223119, 0.10579054, 0.2092365]\n",
      "2025-09-10 11:49:27,296 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:27,301 : INFO : topic #0 (0.242): 0.012*\"like\" + 0.011*\"people\" + 0.008*\"think\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:27,302 : INFO : topic #1 (0.106): 0.007*\"like\" + 0.006*\"people\" + 0.005*\"one\" + 0.005*\"money\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"even\" + 0.004*\"day\" + 0.004*\"jobs\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:27,303 : INFO : topic #2 (0.209): 0.011*\"one\" + 0.008*\"like\" + 0.007*\"would\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:27,303 : INFO : topic diff=0.350031, rho=0.295960\n",
      "2025-09-10 11:49:27,304 : INFO : PROGRESS: pass 3, at document #4000/14833\n",
      "2025-09-10 11:49:28,088 : INFO : optimized alpha [0.27347302, 0.10195771, 0.208073]\n",
      "2025-09-10 11:49:28,091 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:28,096 : INFO : topic #0 (0.273): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:28,097 : INFO : topic #1 (0.102): 0.007*\"like\" + 0.006*\"people\" + 0.005*\"one\" + 0.005*\"money\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"language\" + 0.004*\"day\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:28,098 : INFO : topic #2 (0.208): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"would\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.004*\"got\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:28,098 : INFO : topic diff=0.248315, rho=0.295960\n",
      "2025-09-10 11:49:28,099 : INFO : PROGRESS: pass 3, at document #6000/14833\n",
      "2025-09-10 11:49:28,937 : INFO : optimized alpha [0.28390777, 0.097747914, 0.21391387]\n",
      "2025-09-10 11:49:28,941 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:28,945 : INFO : topic #0 (0.284): 0.015*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:28,945 : INFO : topic #1 (0.098): 0.008*\"like\" + 0.006*\"people\" + 0.005*\"money\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"one\" + 0.005*\"think\" + 0.004*\"day\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:28,946 : INFO : topic #2 (0.214): 0.012*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"would\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:28,946 : INFO : topic diff=0.227665, rho=0.295960\n",
      "2025-09-10 11:49:28,948 : INFO : PROGRESS: pass 3, at document #8000/14833\n",
      "2025-09-10 11:49:29,739 : INFO : optimized alpha [0.29014698, 0.09865287, 0.21653424]\n",
      "2025-09-10 11:49:29,743 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:29,746 : INFO : topic #0 (0.290): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.009*\"something\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:29,747 : INFO : topic #1 (0.099): 0.008*\"like\" + 0.006*\"people\" + 0.005*\"money\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"think\" + 0.005*\"actually\" + 0.005*\"even\" + 0.004*\"one\" + 0.004*\"jobs\"\n",
      "2025-09-10 11:49:29,748 : INFO : topic #2 (0.217): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"really\" + 0.004*\"years\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:29,748 : INFO : topic diff=0.221769, rho=0.295960\n",
      "2025-09-10 11:49:29,749 : INFO : PROGRESS: pass 3, at document #10000/14833\n",
      "2025-09-10 11:49:30,505 : INFO : optimized alpha [0.2803261, 0.09781153, 0.21456088]\n",
      "2025-09-10 11:49:30,509 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:30,512 : INFO : topic #0 (0.280): 0.013*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:30,513 : INFO : topic #1 (0.098): 0.008*\"like\" + 0.007*\"people\" + 0.005*\"money\" + 0.005*\"actually\" + 0.005*\"even\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"think\" + 0.004*\"one\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:30,514 : INFO : topic #2 (0.215): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"years\" + 0.004*\"still\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:30,515 : INFO : topic diff=0.230990, rho=0.295960\n",
      "2025-09-10 11:49:30,516 : INFO : PROGRESS: pass 3, at document #12000/14833\n",
      "2025-09-10 11:49:31,211 : INFO : optimized alpha [0.26229513, 0.09463672, 0.20992932]\n",
      "2025-09-10 11:49:31,215 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:31,220 : INFO : topic #0 (0.262): 0.013*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:31,220 : INFO : topic #1 (0.095): 0.007*\"like\" + 0.007*\"people\" + 0.005*\"even\" + 0.005*\"money\" + 0.005*\"actually\" + 0.005*\"one\" + 0.004*\"sleep\" + 0.004*\"get\" + 0.004*\"think\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:31,221 : INFO : topic #2 (0.210): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.005*\"years\" + 0.004*\"would\" + 0.004*\"made\"\n",
      "2025-09-10 11:49:31,221 : INFO : topic diff=0.233284, rho=0.295960\n",
      "2025-09-10 11:49:31,222 : INFO : PROGRESS: pass 3, at document #14000/14833\n",
      "2025-09-10 11:49:32,024 : INFO : optimized alpha [0.26099795, 0.10061719, 0.23290867]\n",
      "2025-09-10 11:49:32,027 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:32,032 : INFO : topic #0 (0.261): 0.012*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:32,033 : INFO : topic #1 (0.101): 0.007*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"one\" + 0.004*\"sleep\" + 0.004*\"day\" + 0.004*\"would\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:32,034 : INFO : topic #2 (0.233): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"would\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"day\"\n",
      "2025-09-10 11:49:32,034 : INFO : topic diff=0.512219, rho=0.295960\n",
      "2025-09-10 11:49:32,594 : INFO : -8.534 per-word bound, 370.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:32,595 : INFO : PROGRESS: pass 3, at document #14833/14833\n",
      "2025-09-10 11:49:32,885 : INFO : optimized alpha [0.22205825, 0.10305247, 0.24808255]\n",
      "2025-09-10 11:49:32,888 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:32,892 : INFO : topic #0 (0.222): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"life\" + 0.006*\"would\"\n",
      "2025-09-10 11:49:32,893 : INFO : topic #1 (0.103): 0.006*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.005*\"one\" + 0.004*\"even\" + 0.004*\"teeth\" + 0.004*\"would\" + 0.004*\"jobs\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:32,894 : INFO : topic #2 (0.248): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:32,894 : INFO : topic diff=0.390622, rho=0.295960\n",
      "2025-09-10 11:49:32,895 : INFO : PROGRESS: pass 4, at document #2000/14833\n",
      "2025-09-10 11:49:33,670 : INFO : optimized alpha [0.2532234, 0.098837465, 0.20894352]\n",
      "2025-09-10 11:49:33,673 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:33,679 : INFO : topic #0 (0.253): 0.012*\"like\" + 0.011*\"people\" + 0.008*\"think\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:33,680 : INFO : topic #1 (0.099): 0.006*\"like\" + 0.006*\"people\" + 0.005*\"money\" + 0.005*\"one\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"even\" + 0.004*\"jobs\" + 0.004*\"company\" + 0.004*\"would\"\n",
      "2025-09-10 11:49:33,681 : INFO : topic #2 (0.209): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.006*\"would\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"day\" + 0.005*\"even\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:33,681 : INFO : topic diff=0.325868, rho=0.283792\n",
      "2025-09-10 11:49:33,682 : INFO : PROGRESS: pass 4, at document #4000/14833\n",
      "2025-09-10 11:49:34,486 : INFO : optimized alpha [0.285735, 0.09651187, 0.20829207]\n",
      "2025-09-10 11:49:34,491 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:34,495 : INFO : topic #0 (0.286): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:34,496 : INFO : topic #1 (0.097): 0.007*\"like\" + 0.006*\"people\" + 0.005*\"money\" + 0.005*\"also\" + 0.005*\"one\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"language\" + 0.004*\"company\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:34,497 : INFO : topic #2 (0.208): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:34,497 : INFO : topic diff=0.232083, rho=0.283792\n",
      "2025-09-10 11:49:34,498 : INFO : PROGRESS: pass 4, at document #6000/14833\n",
      "2025-09-10 11:49:35,323 : INFO : optimized alpha [0.29911804, 0.09335311, 0.21478395]\n",
      "2025-09-10 11:49:35,327 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:35,332 : INFO : topic #0 (0.299): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:35,332 : INFO : topic #1 (0.093): 0.008*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"even\" + 0.004*\"think\" + 0.004*\"one\" + 0.004*\"day\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:35,333 : INFO : topic #2 (0.215): 0.012*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"would\" + 0.005*\"know\" + 0.005*\"day\"\n",
      "2025-09-10 11:49:35,333 : INFO : topic diff=0.212499, rho=0.283792\n",
      "2025-09-10 11:49:35,334 : INFO : PROGRESS: pass 4, at document #8000/14833\n",
      "2025-09-10 11:49:36,100 : INFO : optimized alpha [0.30711752, 0.094934374, 0.21833585]\n",
      "2025-09-10 11:49:36,103 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:36,107 : INFO : topic #0 (0.307): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.009*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:49:36,108 : INFO : topic #1 (0.095): 0.007*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"think\" + 0.005*\"actually\" + 0.004*\"even\" + 0.004*\"jobs\" + 0.004*\"one\"\n",
      "2025-09-10 11:49:36,108 : INFO : topic #2 (0.218): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"would\" + 0.004*\"really\" + 0.004*\"years\" + 0.004*\"day\"\n",
      "2025-09-10 11:49:36,109 : INFO : topic diff=0.207451, rho=0.283792\n",
      "2025-09-10 11:49:36,109 : INFO : PROGRESS: pass 4, at document #10000/14833\n",
      "2025-09-10 11:49:36,822 : INFO : optimized alpha [0.29850945, 0.09461758, 0.21695569]\n",
      "2025-09-10 11:49:36,825 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:36,829 : INFO : topic #0 (0.299): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:36,830 : INFO : topic #1 (0.095): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"money\" + 0.005*\"actually\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"also\" + 0.004*\"sleep\" + 0.004*\"think\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:36,831 : INFO : topic #2 (0.217): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.004*\"still\" + 0.004*\"day\" + 0.004*\"would\"\n",
      "2025-09-10 11:49:36,831 : INFO : topic diff=0.217100, rho=0.283792\n",
      "2025-09-10 11:49:36,833 : INFO : PROGRESS: pass 4, at document #12000/14833\n",
      "2025-09-10 11:49:37,492 : INFO : optimized alpha [0.27998802, 0.09236484, 0.21258475]\n",
      "2025-09-10 11:49:37,496 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:37,500 : INFO : topic #0 (0.280): 0.013*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:37,500 : INFO : topic #1 (0.092): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"money\" + 0.005*\"even\" + 0.005*\"actually\" + 0.004*\"sleep\" + 0.004*\"one\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:37,501 : INFO : topic #2 (0.213): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.005*\"years\" + 0.004*\"made\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:37,502 : INFO : topic diff=0.220931, rho=0.283792\n",
      "2025-09-10 11:49:37,503 : INFO : PROGRESS: pass 4, at document #14000/14833\n",
      "2025-09-10 11:49:38,291 : INFO : optimized alpha [0.2777974, 0.098163486, 0.23472707]\n",
      "2025-09-10 11:49:38,295 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:38,299 : INFO : topic #0 (0.278): 0.012*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"life\" + 0.006*\"would\"\n",
      "2025-09-10 11:49:38,300 : INFO : topic #1 (0.098): 0.007*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"one\" + 0.004*\"would\" + 0.004*\"sleep\" + 0.004*\"work\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:38,301 : INFO : topic #2 (0.235): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"day\"\n",
      "2025-09-10 11:49:38,301 : INFO : topic diff=0.483926, rho=0.283792\n",
      "2025-09-10 11:49:38,864 : INFO : -8.521 per-word bound, 367.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:38,864 : INFO : PROGRESS: pass 4, at document #14833/14833\n",
      "2025-09-10 11:49:39,150 : INFO : optimized alpha [0.23461822, 0.10035816, 0.24899249]\n",
      "2025-09-10 11:49:39,153 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:39,157 : INFO : topic #0 (0.235): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:39,158 : INFO : topic #1 (0.100): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.004*\"would\" + 0.004*\"one\" + 0.004*\"teeth\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:39,159 : INFO : topic #2 (0.249): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:39,159 : INFO : topic diff=0.369017, rho=0.283792\n",
      "2025-09-10 11:49:39,160 : INFO : PROGRESS: pass 5, at document #2000/14833\n",
      "2025-09-10 11:49:39,906 : INFO : optimized alpha [0.2656462, 0.09716418, 0.21084416]\n",
      "2025-09-10 11:49:39,910 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:39,914 : INFO : topic #0 (0.266): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:39,915 : INFO : topic #1 (0.097): 0.006*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"one\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"company\" + 0.004*\"would\"\n",
      "2025-09-10 11:49:39,916 : INFO : topic #2 (0.211): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"day\" + 0.005*\"even\" + 0.004*\"got\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:39,916 : INFO : topic diff=0.307602, rho=0.273011\n",
      "2025-09-10 11:49:39,917 : INFO : PROGRESS: pass 5, at document #4000/14833\n",
      "2025-09-10 11:49:40,700 : INFO : optimized alpha [0.29856098, 0.0953171, 0.2096639]\n",
      "2025-09-10 11:49:40,704 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:40,709 : INFO : topic #0 (0.299): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:40,709 : INFO : topic #1 (0.095): 0.006*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.005*\"also\" + 0.004*\"one\" + 0.004*\"get\" + 0.004*\"even\" + 0.004*\"language\" + 0.004*\"would\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:40,710 : INFO : topic #2 (0.210): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:40,710 : INFO : topic diff=0.219853, rho=0.273011\n",
      "2025-09-10 11:49:40,711 : INFO : PROGRESS: pass 5, at document #6000/14833\n",
      "2025-09-10 11:49:41,515 : INFO : optimized alpha [0.31347066, 0.0926274, 0.21622387]\n",
      "2025-09-10 11:49:41,518 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:41,522 : INFO : topic #0 (0.313): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:41,523 : INFO : topic #1 (0.093): 0.007*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"even\" + 0.004*\"think\" + 0.004*\"one\" + 0.004*\"jobs\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:41,524 : INFO : topic #2 (0.216): 0.012*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.004*\"little\"\n",
      "2025-09-10 11:49:41,525 : INFO : topic diff=0.200896, rho=0.273011\n",
      "2025-09-10 11:49:41,526 : INFO : PROGRESS: pass 5, at document #8000/14833\n",
      "2025-09-10 11:49:42,238 : INFO : optimized alpha [0.32248276, 0.094485916, 0.21951458]\n",
      "2025-09-10 11:49:42,242 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:42,246 : INFO : topic #0 (0.322): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:49:42,246 : INFO : topic #1 (0.094): 0.007*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.005*\"get\" + 0.005*\"also\" + 0.004*\"actually\" + 0.004*\"think\" + 0.004*\"even\" + 0.004*\"jobs\" + 0.004*\"one\"\n",
      "2025-09-10 11:49:42,247 : INFO : topic #2 (0.220): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.004*\"day\" + 0.004*\"years\" + 0.004*\"really\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:42,247 : INFO : topic diff=0.196259, rho=0.273011\n",
      "2025-09-10 11:49:42,248 : INFO : PROGRESS: pass 5, at document #10000/14833\n",
      "2025-09-10 11:49:42,936 : INFO : optimized alpha [0.31453133, 0.09433539, 0.21825707]\n",
      "2025-09-10 11:49:42,939 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:42,943 : INFO : topic #0 (0.315): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:42,944 : INFO : topic #1 (0.094): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"money\" + 0.005*\"actually\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"also\" + 0.004*\"sleep\" + 0.004*\"work\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:42,944 : INFO : topic #2 (0.218): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.004*\"day\" + 0.004*\"still\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:42,945 : INFO : topic diff=0.205721, rho=0.273011\n",
      "2025-09-10 11:49:42,945 : INFO : PROGRESS: pass 5, at document #12000/14833\n",
      "2025-09-10 11:49:43,583 : INFO : optimized alpha [0.29511482, 0.09242042, 0.21393959]\n",
      "2025-09-10 11:49:43,586 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:43,590 : INFO : topic #0 (0.295): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:43,591 : INFO : topic #1 (0.092): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"money\" + 0.005*\"even\" + 0.004*\"actually\" + 0.004*\"sleep\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"also\" + 0.004*\"jobs\"\n",
      "2025-09-10 11:49:43,592 : INFO : topic #2 (0.214): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.005*\"years\" + 0.004*\"made\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:43,592 : INFO : topic diff=0.210751, rho=0.273011\n",
      "2025-09-10 11:49:43,593 : INFO : PROGRESS: pass 5, at document #14000/14833\n",
      "2025-09-10 11:49:44,318 : INFO : optimized alpha [0.2916458, 0.09810994, 0.2355033]\n",
      "2025-09-10 11:49:44,323 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:44,327 : INFO : topic #0 (0.292): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:44,328 : INFO : topic #1 (0.098): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"sleep\" + 0.004*\"one\" + 0.004*\"work\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:44,329 : INFO : topic #2 (0.236): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"day\"\n",
      "2025-09-10 11:49:44,330 : INFO : topic diff=0.460367, rho=0.273011\n",
      "2025-09-10 11:49:44,874 : INFO : -8.512 per-word bound, 365.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:44,874 : INFO : PROGRESS: pass 5, at document #14833/14833\n",
      "2025-09-10 11:49:45,166 : INFO : optimized alpha [0.24587476, 0.100156225, 0.2491867]\n",
      "2025-09-10 11:49:45,170 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:45,174 : INFO : topic #0 (0.246): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:45,175 : INFO : topic #1 (0.100): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"one\" + 0.004*\"jobs\" + 0.004*\"teeth\" + 0.004*\"even\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:45,176 : INFO : topic #2 (0.249): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:45,176 : INFO : topic diff=0.351595, rho=0.273011\n",
      "2025-09-10 11:49:45,177 : INFO : PROGRESS: pass 6, at document #2000/14833\n",
      "2025-09-10 11:49:45,885 : INFO : optimized alpha [0.27671155, 0.097292244, 0.21237952]\n",
      "2025-09-10 11:49:45,888 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:45,892 : INFO : topic #0 (0.277): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:45,893 : INFO : topic #1 (0.097): 0.006*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.004*\"one\" + 0.004*\"also\" + 0.004*\"jobs\" + 0.004*\"would\" + 0.004*\"even\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:45,893 : INFO : topic #2 (0.212): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:45,894 : INFO : topic diff=0.292559, rho=0.263372\n",
      "2025-09-10 11:49:45,894 : INFO : PROGRESS: pass 6, at document #4000/14833\n",
      "2025-09-10 11:49:46,608 : INFO : optimized alpha [0.30962962, 0.095620304, 0.21082497]\n",
      "2025-09-10 11:49:46,611 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:46,615 : INFO : topic #0 (0.310): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:46,615 : INFO : topic #1 (0.096): 0.006*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"also\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"would\" + 0.004*\"even\" + 0.004*\"company\" + 0.004*\"language\"\n",
      "2025-09-10 11:49:46,616 : INFO : topic #2 (0.211): 0.011*\"one\" + 0.008*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:46,617 : INFO : topic diff=0.209689, rho=0.263372\n",
      "2025-09-10 11:49:46,617 : INFO : PROGRESS: pass 6, at document #6000/14833\n",
      "2025-09-10 11:49:47,365 : INFO : optimized alpha [0.3254455, 0.093119465, 0.21746504]\n",
      "2025-09-10 11:49:47,369 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:47,374 : INFO : topic #0 (0.325): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.006*\"time\"\n",
      "2025-09-10 11:49:47,375 : INFO : topic #1 (0.093): 0.007*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"even\" + 0.004*\"one\" + 0.004*\"jobs\" + 0.004*\"think\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:47,376 : INFO : topic #2 (0.217): 0.012*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.004*\"little\"\n",
      "2025-09-10 11:49:47,376 : INFO : topic diff=0.191357, rho=0.263372\n",
      "2025-09-10 11:49:47,377 : INFO : PROGRESS: pass 6, at document #8000/14833\n",
      "2025-09-10 11:49:48,120 : INFO : optimized alpha [0.33472437, 0.095028825, 0.22008544]\n",
      "2025-09-10 11:49:48,124 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:48,128 : INFO : topic #0 (0.335): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:49:48,129 : INFO : topic #1 (0.095): 0.006*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"actually\" + 0.004*\"think\" + 0.004*\"even\" + 0.004*\"jobs\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:48,130 : INFO : topic #2 (0.220): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.004*\"day\" + 0.004*\"years\" + 0.004*\"really\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:48,131 : INFO : topic diff=0.186880, rho=0.263372\n",
      "2025-09-10 11:49:48,132 : INFO : PROGRESS: pass 6, at document #10000/14833\n",
      "2025-09-10 11:49:48,878 : INFO : optimized alpha [0.32684836, 0.09479301, 0.21851975]\n",
      "2025-09-10 11:49:48,882 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:48,886 : INFO : topic #0 (0.327): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:48,887 : INFO : topic #1 (0.095): 0.007*\"people\" + 0.007*\"like\" + 0.006*\"money\" + 0.005*\"actually\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:48,888 : INFO : topic #2 (0.219): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"day\" + 0.004*\"still\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:48,888 : INFO : topic diff=0.195891, rho=0.263372\n",
      "2025-09-10 11:49:48,889 : INFO : PROGRESS: pass 6, at document #12000/14833\n",
      "2025-09-10 11:49:49,586 : INFO : optimized alpha [0.3069484, 0.092979655, 0.21441834]\n",
      "2025-09-10 11:49:49,590 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:49,594 : INFO : topic #0 (0.307): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:49,595 : INFO : topic #1 (0.093): 0.007*\"people\" + 0.007*\"like\" + 0.006*\"money\" + 0.005*\"even\" + 0.004*\"actually\" + 0.004*\"sleep\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"also\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:49,596 : INFO : topic #2 (0.214): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"made\"\n",
      "2025-09-10 11:49:49,596 : INFO : topic diff=0.202028, rho=0.263372\n",
      "2025-09-10 11:49:49,597 : INFO : PROGRESS: pass 6, at document #14000/14833\n",
      "2025-09-10 11:49:50,374 : INFO : optimized alpha [0.30266201, 0.098434925, 0.2353353]\n",
      "2025-09-10 11:49:50,377 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:50,381 : INFO : topic #0 (0.303): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:50,382 : INFO : topic #1 (0.098): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"one\" + 0.004*\"many\"\n",
      "2025-09-10 11:49:50,383 : INFO : topic #2 (0.235): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"would\"\n",
      "2025-09-10 11:49:50,384 : INFO : topic diff=0.440029, rho=0.263372\n",
      "2025-09-10 11:49:50,926 : INFO : -8.506 per-word bound, 363.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:50,926 : INFO : PROGRESS: pass 6, at document #14833/14833\n",
      "2025-09-10 11:49:51,208 : INFO : optimized alpha [0.25524455, 0.10038732, 0.24850108]\n",
      "2025-09-10 11:49:51,211 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:51,214 : INFO : topic #0 (0.255): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:51,215 : INFO : topic #1 (0.100): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"jobs\" + 0.004*\"one\" + 0.004*\"even\" + 0.004*\"teeth\" + 0.004*\"also\"\n",
      "2025-09-10 11:49:51,216 : INFO : topic #2 (0.249): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:51,216 : INFO : topic diff=0.336916, rho=0.263372\n",
      "2025-09-10 11:49:51,217 : INFO : PROGRESS: pass 7, at document #2000/14833\n",
      "2025-09-10 11:49:51,906 : INFO : optimized alpha [0.28578115, 0.09784842, 0.2129005]\n",
      "2025-09-10 11:49:51,909 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:51,913 : INFO : topic #0 (0.286): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:51,914 : INFO : topic #1 (0.098): 0.006*\"people\" + 0.006*\"money\" + 0.005*\"like\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"one\" + 0.004*\"would\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:51,914 : INFO : topic #2 (0.213): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"would\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:51,915 : INFO : topic diff=0.279850, rho=0.254687\n",
      "2025-09-10 11:49:51,916 : INFO : PROGRESS: pass 7, at document #4000/14833\n",
      "2025-09-10 11:49:52,619 : INFO : optimized alpha [0.31840432, 0.09618905, 0.2108152]\n",
      "2025-09-10 11:49:52,623 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:52,627 : INFO : topic #0 (0.318): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:52,628 : INFO : topic #1 (0.096): 0.006*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"also\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"would\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:52,628 : INFO : topic #2 (0.211): 0.011*\"one\" + 0.008*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"would\" + 0.004*\"even\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:52,629 : INFO : topic diff=0.201086, rho=0.254687\n",
      "2025-09-10 11:49:52,630 : INFO : PROGRESS: pass 7, at document #6000/14833\n",
      "2025-09-10 11:49:53,373 : INFO : optimized alpha [0.33483604, 0.093891636, 0.21745]\n",
      "2025-09-10 11:49:53,376 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:53,380 : INFO : topic #0 (0.335): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"time\"\n",
      "2025-09-10 11:49:53,381 : INFO : topic #1 (0.094): 0.007*\"like\" + 0.007*\"money\" + 0.006*\"people\" + 0.005*\"get\" + 0.005*\"also\" + 0.005*\"even\" + 0.004*\"jobs\" + 0.004*\"one\" + 0.004*\"business\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:53,382 : INFO : topic #2 (0.217): 0.012*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"know\" + 0.004*\"really\" + 0.004*\"little\"\n",
      "2025-09-10 11:49:53,382 : INFO : topic diff=0.183182, rho=0.254687\n",
      "2025-09-10 11:49:53,383 : INFO : PROGRESS: pass 7, at document #8000/14833\n",
      "2025-09-10 11:49:54,068 : INFO : optimized alpha [0.34420854, 0.095755495, 0.2195003]\n",
      "2025-09-10 11:49:54,071 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:54,075 : INFO : topic #0 (0.344): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:49:54,076 : INFO : topic #1 (0.096): 0.006*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"actually\" + 0.004*\"jobs\" + 0.004*\"think\" + 0.004*\"even\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:54,077 : INFO : topic #2 (0.220): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"really\"\n",
      "2025-09-10 11:49:54,077 : INFO : topic diff=0.178934, rho=0.254687\n",
      "2025-09-10 11:49:54,077 : INFO : PROGRESS: pass 7, at document #10000/14833\n",
      "2025-09-10 11:49:54,752 : INFO : optimized alpha [0.3363839, 0.09550719, 0.21751606]\n",
      "2025-09-10 11:49:54,755 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:54,759 : INFO : topic #0 (0.336): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:54,760 : INFO : topic #1 (0.096): 0.007*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"actually\" + 0.005*\"get\" + 0.004*\"also\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:54,761 : INFO : topic #2 (0.218): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.004*\"still\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:54,761 : INFO : topic diff=0.187481, rho=0.254687\n",
      "2025-09-10 11:49:54,761 : INFO : PROGRESS: pass 7, at document #12000/14833\n",
      "2025-09-10 11:49:55,393 : INFO : optimized alpha [0.31621224, 0.093638346, 0.21376048]\n",
      "2025-09-10 11:49:55,396 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:55,400 : INFO : topic #0 (0.316): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:49:55,401 : INFO : topic #1 (0.094): 0.007*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"even\" + 0.004*\"sleep\" + 0.004*\"actually\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"also\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:55,402 : INFO : topic #2 (0.214): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"made\"\n",
      "2025-09-10 11:49:55,402 : INFO : topic diff=0.194511, rho=0.254687\n",
      "2025-09-10 11:49:55,403 : INFO : PROGRESS: pass 7, at document #14000/14833\n",
      "2025-09-10 11:49:56,153 : INFO : optimized alpha [0.3109871, 0.09886602, 0.23398228]\n",
      "2025-09-10 11:49:56,158 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:56,161 : INFO : topic #0 (0.311): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:56,162 : INFO : topic #1 (0.099): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.004*\"would\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"one\" + 0.004*\"many\"\n",
      "2025-09-10 11:49:56,163 : INFO : topic #2 (0.234): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"day\" + 0.005*\"years\" + 0.004*\"would\"\n",
      "2025-09-10 11:49:56,164 : INFO : topic diff=0.421962, rho=0.254687\n",
      "2025-09-10 11:49:56,716 : INFO : -8.500 per-word bound, 362.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:49:56,716 : INFO : PROGRESS: pass 7, at document #14833/14833\n",
      "2025-09-10 11:49:56,999 : INFO : optimized alpha [0.26247162, 0.10085169, 0.24677308]\n",
      "2025-09-10 11:49:57,002 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:57,006 : INFO : topic #0 (0.262): 0.013*\"people\" + 0.011*\"like\" + 0.008*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:57,007 : INFO : topic #1 (0.101): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"jobs\" + 0.004*\"one\" + 0.004*\"even\" + 0.004*\"teeth\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:57,007 : INFO : topic #2 (0.247): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:57,008 : INFO : topic diff=0.324077, rho=0.254687\n",
      "2025-09-10 11:49:57,008 : INFO : PROGRESS: pass 8, at document #2000/14833\n",
      "2025-09-10 11:49:57,685 : INFO : optimized alpha [0.29242098, 0.09855069, 0.21218862]\n",
      "2025-09-10 11:49:57,689 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:57,693 : INFO : topic #0 (0.292): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:57,693 : INFO : topic #1 (0.099): 0.006*\"people\" + 0.006*\"money\" + 0.005*\"like\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"would\" + 0.004*\"one\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"work\"\n",
      "2025-09-10 11:49:57,694 : INFO : topic #2 (0.212): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:57,695 : INFO : topic diff=0.268929, rho=0.246808\n",
      "2025-09-10 11:49:57,695 : INFO : PROGRESS: pass 8, at document #4000/14833\n",
      "2025-09-10 11:49:58,389 : INFO : optimized alpha [0.3246001, 0.096898854, 0.20971991]\n",
      "2025-09-10 11:49:58,393 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:58,397 : INFO : topic #0 (0.325): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:49:58,398 : INFO : topic #1 (0.097): 0.006*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.004*\"get\" + 0.004*\"would\" + 0.004*\"one\" + 0.004*\"work\" + 0.004*\"even\" + 0.004*\"company\"\n",
      "2025-09-10 11:49:58,399 : INFO : topic #2 (0.210): 0.011*\"one\" + 0.008*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"would\" + 0.004*\"even\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:49:58,399 : INFO : topic diff=0.193467, rho=0.246808\n",
      "2025-09-10 11:49:58,399 : INFO : PROGRESS: pass 8, at document #6000/14833\n",
      "2025-09-10 11:49:59,129 : INFO : optimized alpha [0.34134242, 0.094624706, 0.21628171]\n",
      "2025-09-10 11:49:59,133 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:59,137 : INFO : topic #0 (0.341): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"really\" + 0.007*\"might\" + 0.007*\"time\"\n",
      "2025-09-10 11:49:59,138 : INFO : topic #1 (0.095): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"jobs\" + 0.004*\"business\" + 0.004*\"one\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:49:59,139 : INFO : topic #2 (0.216): 0.012*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"know\" + 0.004*\"really\" + 0.004*\"said\"\n",
      "2025-09-10 11:49:59,140 : INFO : topic diff=0.176063, rho=0.246808\n",
      "2025-09-10 11:49:59,140 : INFO : PROGRESS: pass 8, at document #8000/14833\n",
      "2025-09-10 11:49:59,893 : INFO : optimized alpha [0.35081843, 0.09653093, 0.21827404]\n",
      "2025-09-10 11:49:59,897 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:49:59,902 : INFO : topic #0 (0.351): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:49:59,903 : INFO : topic #1 (0.097): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"jobs\" + 0.004*\"actually\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"think\"\n",
      "2025-09-10 11:49:59,903 : INFO : topic #2 (0.218): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"know\"\n",
      "2025-09-10 11:49:59,904 : INFO : topic diff=0.172030, rho=0.246808\n",
      "2025-09-10 11:49:59,905 : INFO : PROGRESS: pass 8, at document #10000/14833\n",
      "2025-09-10 11:50:00,650 : INFO : optimized alpha [0.34313023, 0.096297614, 0.21585356]\n",
      "2025-09-10 11:50:00,654 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:00,658 : INFO : topic #0 (0.343): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:00,659 : INFO : topic #1 (0.096): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"actually\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"would\"\n",
      "2025-09-10 11:50:00,659 : INFO : topic #2 (0.216): 0.011*\"like\" + 0.010*\"one\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.004*\"still\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:00,660 : INFO : topic diff=0.180074, rho=0.246808\n",
      "2025-09-10 11:50:00,661 : INFO : PROGRESS: pass 8, at document #12000/14833\n",
      "2025-09-10 11:50:01,367 : INFO : optimized alpha [0.3230462, 0.09444269, 0.21238789]\n",
      "2025-09-10 11:50:01,370 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:01,374 : INFO : topic #0 (0.323): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:01,375 : INFO : topic #1 (0.094): 0.007*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.005*\"even\" + 0.004*\"sleep\" + 0.004*\"actually\" + 0.004*\"get\" + 0.004*\"one\" + 0.004*\"work\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:01,376 : INFO : topic #2 (0.212): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"made\"\n",
      "2025-09-10 11:50:01,377 : INFO : topic diff=0.187860, rho=0.246808\n",
      "2025-09-10 11:50:01,378 : INFO : PROGRESS: pass 8, at document #14000/14833\n",
      "2025-09-10 11:50:02,112 : INFO : optimized alpha [0.31707782, 0.09954675, 0.23189119]\n",
      "2025-09-10 11:50:02,116 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:02,120 : INFO : topic #0 (0.317): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:02,121 : INFO : topic #1 (0.100): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"even\" + 0.004*\"work\" + 0.004*\"sleep\" + 0.004*\"many\" + 0.004*\"one\"\n",
      "2025-09-10 11:50:02,122 : INFO : topic #2 (0.232): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"day\" + 0.005*\"years\" + 0.004*\"would\"\n",
      "2025-09-10 11:50:02,122 : INFO : topic diff=0.405847, rho=0.246808\n",
      "2025-09-10 11:50:02,646 : INFO : -8.495 per-word bound, 360.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:02,646 : INFO : PROGRESS: pass 8, at document #14833/14833\n",
      "2025-09-10 11:50:02,925 : INFO : optimized alpha [0.267051, 0.10146502, 0.244102]\n",
      "2025-09-10 11:50:02,928 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:02,932 : INFO : topic #0 (0.267): 0.013*\"people\" + 0.012*\"like\" + 0.008*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:02,933 : INFO : topic #1 (0.101): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"jobs\" + 0.004*\"work\" + 0.004*\"even\" + 0.004*\"one\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:50:02,934 : INFO : topic #2 (0.244): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:02,934 : INFO : topic diff=0.312962, rho=0.246808\n",
      "2025-09-10 11:50:02,935 : INFO : PROGRESS: pass 9, at document #2000/14833\n",
      "2025-09-10 11:50:03,608 : INFO : optimized alpha [0.2964135, 0.09929788, 0.21103755]\n",
      "2025-09-10 11:50:03,611 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:03,615 : INFO : topic #0 (0.296): 0.012*\"like\" + 0.011*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:03,615 : INFO : topic #1 (0.099): 0.006*\"people\" + 0.006*\"money\" + 0.005*\"like\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"would\" + 0.004*\"one\" + 0.004*\"work\" + 0.004*\"jobs\" + 0.004*\"even\"\n",
      "2025-09-10 11:50:03,616 : INFO : topic #2 (0.211): 0.011*\"one\" + 0.008*\"like\" + 0.006*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:03,617 : INFO : topic diff=0.259267, rho=0.239618\n",
      "2025-09-10 11:50:03,617 : INFO : PROGRESS: pass 9, at document #4000/14833\n",
      "2025-09-10 11:50:04,314 : INFO : optimized alpha [0.32812026, 0.09774098, 0.2083777]\n",
      "2025-09-10 11:50:04,318 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:04,321 : INFO : topic #0 (0.328): 0.013*\"like\" + 0.011*\"people\" + 0.010*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"might\" + 0.006*\"someone\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:04,322 : INFO : topic #1 (0.098): 0.006*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"also\" + 0.004*\"get\" + 0.004*\"would\" + 0.004*\"work\" + 0.004*\"one\" + 0.004*\"even\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:04,323 : INFO : topic #2 (0.208): 0.011*\"one\" + 0.008*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"would\" + 0.004*\"even\" + 0.004*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:04,324 : INFO : topic diff=0.186657, rho=0.239618\n",
      "2025-09-10 11:50:04,325 : INFO : PROGRESS: pass 9, at document #6000/14833\n",
      "2025-09-10 11:50:05,028 : INFO : optimized alpha [0.34495193, 0.0954722, 0.2147367]\n",
      "2025-09-10 11:50:05,031 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:05,035 : INFO : topic #0 (0.345): 0.015*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"really\" + 0.007*\"might\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:05,037 : INFO : topic #1 (0.095): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"jobs\" + 0.004*\"business\" + 0.004*\"would\" + 0.004*\"work\"\n",
      "2025-09-10 11:50:05,037 : INFO : topic #2 (0.215): 0.012*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"know\" + 0.004*\"really\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:05,038 : INFO : topic diff=0.169816, rho=0.239618\n",
      "2025-09-10 11:50:05,038 : INFO : PROGRESS: pass 9, at document #8000/14833\n",
      "2025-09-10 11:50:05,719 : INFO : optimized alpha [0.3544962, 0.097330876, 0.21671526]\n",
      "2025-09-10 11:50:05,722 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:05,726 : INFO : topic #0 (0.354): 0.014*\"like\" + 0.012*\"think\" + 0.012*\"people\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:05,727 : INFO : topic #1 (0.097): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"work\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"actually\" + 0.004*\"think\"\n",
      "2025-09-10 11:50:05,728 : INFO : topic #2 (0.217): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"know\"\n",
      "2025-09-10 11:50:05,728 : INFO : topic diff=0.165878, rho=0.239618\n",
      "2025-09-10 11:50:05,729 : INFO : PROGRESS: pass 9, at document #10000/14833\n",
      "2025-09-10 11:50:06,397 : INFO : optimized alpha [0.34751955, 0.09702675, 0.21446954]\n",
      "2025-09-10 11:50:06,401 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:06,404 : INFO : topic #0 (0.348): 0.014*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:06,405 : INFO : topic #1 (0.097): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"actually\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"work\" + 0.004*\"even\" + 0.004*\"sleep\" + 0.004*\"would\"\n",
      "2025-09-10 11:50:06,406 : INFO : topic #2 (0.214): 0.010*\"one\" + 0.010*\"like\" + 0.006*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.004*\"still\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:06,406 : INFO : topic diff=0.173430, rho=0.239618\n",
      "2025-09-10 11:50:06,407 : INFO : PROGRESS: pass 9, at document #12000/14833\n",
      "2025-09-10 11:50:07,040 : INFO : optimized alpha [0.3279415, 0.09515424, 0.21134365]\n",
      "2025-09-10 11:50:07,043 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:07,047 : INFO : topic #0 (0.328): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"someone\" + 0.006*\"might\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:07,048 : INFO : topic #1 (0.095): 0.007*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"even\" + 0.004*\"sleep\" + 0.004*\"get\" + 0.004*\"actually\" + 0.004*\"work\" + 0.004*\"one\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:07,049 : INFO : topic #2 (0.211): 0.011*\"one\" + 0.011*\"like\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"still\" + 0.004*\"years\" + 0.004*\"said\" + 0.004*\"made\"\n",
      "2025-09-10 11:50:07,049 : INFO : topic diff=0.181973, rho=0.239618\n",
      "2025-09-10 11:50:07,050 : INFO : PROGRESS: pass 9, at document #14000/14833\n",
      "2025-09-10 11:50:07,793 : INFO : optimized alpha [0.32135162, 0.100180104, 0.23013747]\n",
      "2025-09-10 11:50:07,798 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:07,802 : INFO : topic #0 (0.321): 0.012*\"like\" + 0.012*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:07,803 : INFO : topic #1 (0.100): 0.007*\"people\" + 0.006*\"money\" + 0.005*\"like\" + 0.005*\"would\" + 0.005*\"get\" + 0.004*\"work\" + 0.004*\"even\" + 0.004*\"sleep\" + 0.004*\"many\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:07,804 : INFO : topic #2 (0.230): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.004*\"would\"\n",
      "2025-09-10 11:50:07,804 : INFO : topic diff=0.391305, rho=0.239618\n",
      "2025-09-10 11:50:08,374 : INFO : -8.490 per-word bound, 359.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:08,374 : INFO : PROGRESS: pass 9, at document #14833/14833\n",
      "2025-09-10 11:50:08,681 : INFO : optimized alpha [0.27085486, 0.10189456, 0.24185248]\n",
      "2025-09-10 11:50:08,684 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:08,688 : INFO : topic #0 (0.271): 0.013*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"someone\" + 0.006*\"would\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:08,689 : INFO : topic #1 (0.102): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"work\" + 0.004*\"jobs\" + 0.004*\"even\" + 0.004*\"one\" + 0.004*\"many\"\n",
      "2025-09-10 11:50:08,690 : INFO : topic #2 (0.242): 0.010*\"one\" + 0.007*\"like\" + 0.006*\"time\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.004*\"even\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:08,690 : INFO : topic diff=0.302824, rho=0.239618\n",
      "2025-09-10 11:50:08,690 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=34546, num_topics=3, decay=0.5, chunksize=2000> in 65.08s', 'datetime': '2025-09-10T11:50:08.690860', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-09-10 11:50:08,693 : INFO : using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows\n",
      "2025-09-10 11:50:12,499 : INFO : 1 batches submitted to accumulate stats from 64 documents (2312 virtual)\n",
      "2025-09-10 11:50:12,504 : INFO : 2 batches submitted to accumulate stats from 128 documents (4776 virtual)\n",
      "2025-09-10 11:50:12,508 : INFO : 3 batches submitted to accumulate stats from 192 documents (8552 virtual)\n",
      "2025-09-10 11:50:12,511 : INFO : 4 batches submitted to accumulate stats from 256 documents (12770 virtual)\n",
      "2025-09-10 11:50:12,514 : INFO : 5 batches submitted to accumulate stats from 320 documents (15636 virtual)\n",
      "2025-09-10 11:50:12,517 : INFO : 6 batches submitted to accumulate stats from 384 documents (19247 virtual)\n",
      "2025-09-10 11:50:12,525 : INFO : 7 batches submitted to accumulate stats from 448 documents (20248 virtual)\n",
      "2025-09-10 11:50:12,528 : INFO : 8 batches submitted to accumulate stats from 512 documents (21583 virtual)\n",
      "2025-09-10 11:50:12,530 : INFO : 9 batches submitted to accumulate stats from 576 documents (24028 virtual)\n",
      "2025-09-10 11:50:12,532 : INFO : 10 batches submitted to accumulate stats from 640 documents (24977 virtual)\n",
      "2025-09-10 11:50:12,535 : INFO : 11 batches submitted to accumulate stats from 704 documents (26982 virtual)\n",
      "2025-09-10 11:50:12,538 : INFO : 12 batches submitted to accumulate stats from 768 documents (30059 virtual)\n",
      "2025-09-10 11:50:12,540 : INFO : 13 batches submitted to accumulate stats from 832 documents (30786 virtual)\n",
      "2025-09-10 11:50:12,543 : INFO : 14 batches submitted to accumulate stats from 896 documents (34101 virtual)\n",
      "2025-09-10 11:50:12,545 : INFO : 15 batches submitted to accumulate stats from 960 documents (34664 virtual)\n",
      "2025-09-10 11:50:12,548 : INFO : 16 batches submitted to accumulate stats from 1024 documents (38291 virtual)\n",
      "2025-09-10 11:50:12,551 : INFO : 17 batches submitted to accumulate stats from 1088 documents (40666 virtual)\n",
      "2025-09-10 11:50:12,553 : INFO : 18 batches submitted to accumulate stats from 1152 documents (42061 virtual)\n",
      "2025-09-10 11:50:12,555 : INFO : 19 batches submitted to accumulate stats from 1216 documents (44289 virtual)\n",
      "2025-09-10 11:50:12,557 : INFO : 20 batches submitted to accumulate stats from 1280 documents (47210 virtual)\n",
      "2025-09-10 11:50:12,561 : INFO : 21 batches submitted to accumulate stats from 1344 documents (48243 virtual)\n",
      "2025-09-10 11:50:12,569 : INFO : 22 batches submitted to accumulate stats from 1408 documents (48909 virtual)\n",
      "2025-09-10 11:50:12,571 : INFO : 23 batches submitted to accumulate stats from 1472 documents (49511 virtual)\n",
      "2025-09-10 11:50:12,574 : INFO : 24 batches submitted to accumulate stats from 1536 documents (51165 virtual)\n",
      "2025-09-10 11:50:12,576 : INFO : 25 batches submitted to accumulate stats from 1600 documents (51700 virtual)\n",
      "2025-09-10 11:50:12,578 : INFO : 26 batches submitted to accumulate stats from 1664 documents (54911 virtual)\n",
      "2025-09-10 11:50:12,581 : INFO : 27 batches submitted to accumulate stats from 1728 documents (58811 virtual)\n",
      "2025-09-10 11:50:12,584 : INFO : 28 batches submitted to accumulate stats from 1792 documents (62711 virtual)\n",
      "2025-09-10 11:50:12,586 : INFO : 29 batches submitted to accumulate stats from 1856 documents (64395 virtual)\n",
      "2025-09-10 11:50:12,589 : INFO : 30 batches submitted to accumulate stats from 1920 documents (67132 virtual)\n",
      "2025-09-10 11:50:12,596 : INFO : 31 batches submitted to accumulate stats from 1984 documents (69130 virtual)\n",
      "2025-09-10 11:50:12,599 : INFO : 32 batches submitted to accumulate stats from 2048 documents (71562 virtual)\n",
      "2025-09-10 11:50:12,601 : INFO : 33 batches submitted to accumulate stats from 2112 documents (72821 virtual)\n",
      "2025-09-10 11:50:12,604 : INFO : 34 batches submitted to accumulate stats from 2176 documents (73606 virtual)\n",
      "2025-09-10 11:50:12,610 : INFO : 35 batches submitted to accumulate stats from 2240 documents (73967 virtual)\n",
      "2025-09-10 11:50:12,613 : INFO : 36 batches submitted to accumulate stats from 2304 documents (75970 virtual)\n",
      "2025-09-10 11:50:12,616 : INFO : 37 batches submitted to accumulate stats from 2368 documents (77816 virtual)\n",
      "2025-09-10 11:50:12,619 : INFO : 38 batches submitted to accumulate stats from 2432 documents (78243 virtual)\n",
      "2025-09-10 11:50:12,622 : INFO : 39 batches submitted to accumulate stats from 2496 documents (80113 virtual)\n",
      "2025-09-10 11:50:12,625 : INFO : 40 batches submitted to accumulate stats from 2560 documents (83282 virtual)\n",
      "2025-09-10 11:50:12,628 : INFO : 41 batches submitted to accumulate stats from 2624 documents (84555 virtual)\n",
      "2025-09-10 11:50:12,637 : INFO : 42 batches submitted to accumulate stats from 2688 documents (85006 virtual)\n",
      "2025-09-10 11:50:12,644 : INFO : 43 batches submitted to accumulate stats from 2752 documents (85561 virtual)\n",
      "2025-09-10 11:50:12,648 : INFO : 44 batches submitted to accumulate stats from 2816 documents (87847 virtual)\n",
      "2025-09-10 11:50:12,655 : INFO : 45 batches submitted to accumulate stats from 2880 documents (88294 virtual)\n",
      "2025-09-10 11:50:12,657 : INFO : 46 batches submitted to accumulate stats from 2944 documents (89254 virtual)\n",
      "2025-09-10 11:50:12,660 : INFO : 47 batches submitted to accumulate stats from 3008 documents (90361 virtual)\n",
      "2025-09-10 11:50:12,665 : INFO : 49 batches submitted to accumulate stats from 3136 documents (90494 virtual)\n",
      "2025-09-10 11:50:12,668 : INFO : 50 batches submitted to accumulate stats from 3200 documents (91230 virtual)\n",
      "2025-09-10 11:50:12,670 : INFO : 51 batches submitted to accumulate stats from 3264 documents (92239 virtual)\n",
      "2025-09-10 11:50:12,672 : INFO : 52 batches submitted to accumulate stats from 3328 documents (93051 virtual)\n",
      "2025-09-10 11:50:12,674 : INFO : 53 batches submitted to accumulate stats from 3392 documents (93654 virtual)\n",
      "2025-09-10 11:50:12,685 : INFO : 59 batches submitted to accumulate stats from 3776 documents (90837 virtual)\n",
      "2025-09-10 11:50:12,688 : INFO : 61 batches submitted to accumulate stats from 3904 documents (89441 virtual)\n",
      "2025-09-10 11:50:12,693 : INFO : 63 batches submitted to accumulate stats from 4032 documents (90763 virtual)\n",
      "2025-09-10 11:50:12,701 : INFO : 67 batches submitted to accumulate stats from 4288 documents (89512 virtual)\n",
      "2025-09-10 11:50:12,711 : INFO : 73 batches submitted to accumulate stats from 4672 documents (84248 virtual)\n",
      "2025-09-10 11:50:12,713 : INFO : 74 batches submitted to accumulate stats from 4736 documents (84612 virtual)\n",
      "2025-09-10 11:50:12,716 : INFO : 75 batches submitted to accumulate stats from 4800 documents (85605 virtual)\n",
      "2025-09-10 11:50:12,719 : INFO : 77 batches submitted to accumulate stats from 4928 documents (86303 virtual)\n",
      "2025-09-10 11:50:12,722 : INFO : 78 batches submitted to accumulate stats from 4992 documents (86705 virtual)\n",
      "2025-09-10 11:50:12,729 : INFO : 79 batches submitted to accumulate stats from 5056 documents (87250 virtual)\n",
      "2025-09-10 11:50:12,732 : INFO : 80 batches submitted to accumulate stats from 5120 documents (87252 virtual)\n",
      "2025-09-10 11:50:12,737 : INFO : 83 batches submitted to accumulate stats from 5312 documents (85664 virtual)\n",
      "2025-09-10 11:50:12,739 : INFO : 84 batches submitted to accumulate stats from 5376 documents (85668 virtual)\n",
      "2025-09-10 11:50:12,744 : INFO : 87 batches submitted to accumulate stats from 5568 documents (85126 virtual)\n",
      "2025-09-10 11:50:12,754 : INFO : 91 batches submitted to accumulate stats from 5824 documents (83475 virtual)\n",
      "2025-09-10 11:50:12,760 : INFO : 93 batches submitted to accumulate stats from 5952 documents (83419 virtual)\n",
      "2025-09-10 11:50:12,811 : INFO : 120 batches submitted to accumulate stats from 7680 documents (53159 virtual)\n",
      "2025-09-10 11:50:12,847 : INFO : 144 batches submitted to accumulate stats from 9216 documents (25428 virtual)\n",
      "2025-09-10 11:50:12,861 : INFO : 153 batches submitted to accumulate stats from 9792 documents (18399 virtual)\n",
      "2025-09-10 11:50:12,870 : INFO : 157 batches submitted to accumulate stats from 10048 documents (15860 virtual)\n",
      "2025-09-10 11:50:12,874 : INFO : 158 batches submitted to accumulate stats from 10112 documents (16981 virtual)\n",
      "2025-09-10 11:50:12,878 : INFO : 160 batches submitted to accumulate stats from 10240 documents (14423 virtual)\n",
      "2025-09-10 11:50:12,888 : INFO : 165 batches submitted to accumulate stats from 10560 documents (7614 virtual)\n",
      "2025-09-10 11:50:12,892 : INFO : 167 batches submitted to accumulate stats from 10688 documents (8130 virtual)\n",
      "2025-09-10 11:50:12,896 : INFO : 169 batches submitted to accumulate stats from 10816 documents (9358 virtual)\n",
      "2025-09-10 11:50:12,899 : INFO : 171 batches submitted to accumulate stats from 10944 documents (10141 virtual)\n",
      "2025-09-10 11:50:12,912 : INFO : 177 batches submitted to accumulate stats from 11328 documents (598 virtual)\n",
      "2025-09-10 11:50:12,935 : INFO : 192 batches submitted to accumulate stats from 12288 documents (-20530 virtual)\n",
      "2025-09-10 11:50:12,940 : INFO : 194 batches submitted to accumulate stats from 12416 documents (-21842 virtual)\n",
      "2025-09-10 11:50:12,948 : INFO : 199 batches submitted to accumulate stats from 12736 documents (-21354 virtual)\n",
      "2025-09-10 11:50:12,951 : INFO : 200 batches submitted to accumulate stats from 12800 documents (-18732 virtual)\n",
      "2025-09-10 11:50:12,953 : INFO : 201 batches submitted to accumulate stats from 12864 documents (-16983 virtual)\n",
      "2025-09-10 11:50:12,958 : INFO : 202 batches submitted to accumulate stats from 12928 documents (-10231 virtual)\n",
      "2025-09-10 11:50:12,963 : INFO : 203 batches submitted to accumulate stats from 12992 documents (-2873 virtual)\n",
      "2025-09-10 11:50:12,967 : INFO : 204 batches submitted to accumulate stats from 13056 documents (2095 virtual)\n",
      "2025-09-10 11:50:12,972 : INFO : 206 batches submitted to accumulate stats from 13184 documents (1129 virtual)\n",
      "2025-09-10 11:50:12,980 : INFO : 210 batches submitted to accumulate stats from 13440 documents (33 virtual)\n",
      "2025-09-10 11:50:12,983 : INFO : 211 batches submitted to accumulate stats from 13504 documents (766 virtual)\n",
      "2025-09-10 11:50:12,986 : INFO : 212 batches submitted to accumulate stats from 13568 documents (2126 virtual)\n",
      "2025-09-10 11:50:12,988 : INFO : 213 batches submitted to accumulate stats from 13632 documents (2894 virtual)\n",
      "2025-09-10 11:50:12,992 : INFO : 215 batches submitted to accumulate stats from 13760 documents (4686 virtual)\n",
      "2025-09-10 11:50:12,995 : INFO : 216 batches submitted to accumulate stats from 13824 documents (5912 virtual)\n",
      "2025-09-10 11:50:13,006 : INFO : 217 batches submitted to accumulate stats from 13888 documents (9533 virtual)\n",
      "2025-09-10 11:50:13,015 : INFO : 222 batches submitted to accumulate stats from 14208 documents (9279 virtual)\n",
      "2025-09-10 11:50:13,022 : INFO : 223 batches submitted to accumulate stats from 14272 documents (14537 virtual)\n",
      "2025-09-10 11:50:13,028 : INFO : 224 batches submitted to accumulate stats from 14336 documents (16740 virtual)\n",
      "2025-09-10 11:50:13,030 : INFO : 225 batches submitted to accumulate stats from 14400 documents (17807 virtual)\n",
      "2025-09-10 11:50:13,032 : INFO : 226 batches submitted to accumulate stats from 14464 documents (18030 virtual)\n",
      "2025-09-10 11:50:13,036 : INFO : 228 batches submitted to accumulate stats from 14592 documents (19290 virtual)\n",
      "2025-09-10 11:50:13,040 : INFO : 230 batches submitted to accumulate stats from 14720 documents (21691 virtual)\n",
      "2025-09-10 11:50:13,042 : INFO : 231 batches submitted to accumulate stats from 14784 documents (23450 virtual)\n",
      "2025-09-10 11:50:13,044 : INFO : 232 batches submitted to accumulate stats from 14848 documents (23484 virtual)\n",
      "2025-09-10 11:50:13,375 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,378 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,382 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,378 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,384 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,384 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,382 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,382 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,384 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,384 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,385 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,385 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,382 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,387 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,382 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,391 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,392 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,392 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,391 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,394 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,387 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,390 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,389 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,386 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,390 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,393 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,394 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,391 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,388 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,392 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,399 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,395 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,398 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,397 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,396 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,394 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,398 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,401 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,398 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,400 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,401 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,400 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,400 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,401 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,396 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,400 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,408 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,407 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,401 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,406 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,406 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,405 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,400 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,398 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,395 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,406 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,403 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,402 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,398 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,410 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,408 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,410 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,402 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,403 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,400 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,402 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,401 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,399 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,404 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,410 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,410 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,412 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,405 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,412 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,414 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,403 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,405 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,412 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,412 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,415 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,414 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,418 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,417 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,419 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,414 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,421 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,418 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,420 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,419 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,416 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,419 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,421 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,422 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,418 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,414 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,420 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,418 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,427 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,422 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,427 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,429 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,427 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,429 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,420 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,427 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,413 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,430 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,430 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,414 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,420 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,431 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,434 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,434 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,431 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,420 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,433 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,432 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,432 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,430 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,433 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,439 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,439 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,441 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,426 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,430 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,440 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,442 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,433 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,453 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,434 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,445 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,440 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,432 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,434 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,442 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,413 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,447 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,442 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,445 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,445 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,443 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,442 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,446 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,443 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,441 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,478 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,478 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,419 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,466 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,477 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,478 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,477 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,471 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,480 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,481 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,478 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,476 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,487 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,425 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,445 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,471 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,477 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,482 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,393 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,484 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,491 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,493 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,486 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,493 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,493 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,504 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,493 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,492 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,490 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,485 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,483 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,441 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,504 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,506 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,492 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,504 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,503 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,492 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,493 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,481 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,514 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,492 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,512 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,515 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,523 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,506 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,522 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,524 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,500 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,515 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,527 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,569 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,524 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,530 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,518 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:50:13,530 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,525 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,524 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,538 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,547 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,548 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,440 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,455 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,434 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,477 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,479 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,499 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,503 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,402 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:13,463 : INFO : accumulator serialized\n",
      "2025-09-10 11:50:16,347 : INFO : 127 accumulators retrieved from output queue\n",
      "2025-09-10 11:50:16,389 : INFO : accumulated word occurrence stats for 296776 virtual documents\n",
      "2025-09-10 11:50:16,777 : INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "2025-09-10 11:50:16,777 : INFO : using symmetric eta at 0.25\n",
      "2025-09-10 11:50:16,782 : INFO : using serial LDA version on this node\n",
      "2025-09-10 11:50:16,793 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 14833 documents, updating model once every 2000 documents, evaluating perplexity every 14833 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-09-10 11:50:16,794 : INFO : PROGRESS: pass 0, at document #2000/14833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 3 - Coherence: 0.3506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:50:17,987 : INFO : optimized alpha [0.460262, 0.41442758, 0.41854858, 0.4246746]\n",
      "2025-09-10 11:50:17,992 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:17,997 : INFO : topic #0 (0.460): 0.012*\"like\" + 0.009*\"one\" + 0.009*\"might\" + 0.008*\"people\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"also\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:17,998 : INFO : topic #1 (0.414): 0.010*\"like\" + 0.010*\"one\" + 0.009*\"people\" + 0.007*\"even\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"might\" + 0.005*\"often\" + 0.005*\"way\"\n",
      "2025-09-10 11:50:17,999 : INFO : topic #2 (0.419): 0.011*\"like\" + 0.008*\"think\" + 0.008*\"something\" + 0.006*\"one\" + 0.006*\"also\" + 0.006*\"time\" + 0.006*\"would\" + 0.006*\"might\" + 0.006*\"could\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:17,999 : INFO : topic #3 (0.425): 0.014*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.007*\"might\" + 0.007*\"people\" + 0.006*\"really\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"way\" + 0.005*\"someone\"\n",
      "2025-09-10 11:50:18,000 : INFO : topic diff=4.406633, rho=1.000000\n",
      "2025-09-10 11:50:18,001 : INFO : PROGRESS: pass 0, at document #4000/14833\n",
      "2025-09-10 11:50:19,226 : INFO : optimized alpha [0.46518153, 0.36519155, 0.40556145, 0.5567116]\n",
      "2025-09-10 11:50:19,233 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:19,238 : INFO : topic #0 (0.465): 0.013*\"like\" + 0.009*\"one\" + 0.009*\"think\" + 0.009*\"people\" + 0.009*\"something\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"really\" + 0.007*\"even\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:19,238 : INFO : topic #1 (0.365): 0.010*\"like\" + 0.009*\"one\" + 0.009*\"people\" + 0.007*\"life\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"might\" + 0.005*\"way\" + 0.005*\"really\"\n",
      "2025-09-10 11:50:19,239 : INFO : topic #2 (0.406): 0.011*\"like\" + 0.011*\"think\" + 0.010*\"would\" + 0.008*\"something\" + 0.006*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"people\"\n",
      "2025-09-10 11:50:19,240 : INFO : topic #3 (0.557): 0.016*\"like\" + 0.011*\"one\" + 0.009*\"think\" + 0.008*\"really\" + 0.008*\"time\" + 0.007*\"people\" + 0.006*\"way\" + 0.006*\"even\" + 0.005*\"know\" + 0.005*\"someone\"\n",
      "2025-09-10 11:50:19,240 : INFO : topic diff=0.729743, rho=0.707107\n",
      "2025-09-10 11:50:19,241 : INFO : PROGRESS: pass 0, at document #6000/14833\n",
      "2025-09-10 11:50:20,338 : INFO : optimized alpha [0.2940317, 0.20184031, 0.23876332, 0.6128277]\n",
      "2025-09-10 11:50:20,342 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:20,347 : INFO : topic #0 (0.294): 0.014*\"like\" + 0.012*\"think\" + 0.011*\"people\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"also\" + 0.007*\"might\" + 0.007*\"really\" + 0.006*\"someone\"\n",
      "2025-09-10 11:50:20,348 : INFO : topic #1 (0.202): 0.011*\"like\" + 0.010*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.007*\"even\" + 0.006*\"life\" + 0.005*\"get\" + 0.005*\"really\" + 0.005*\"way\" + 0.005*\"time\"\n",
      "2025-09-10 11:50:20,348 : INFO : topic #2 (0.239): 0.012*\"like\" + 0.012*\"think\" + 0.010*\"would\" + 0.008*\"something\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"people\"\n",
      "2025-09-10 11:50:20,349 : INFO : topic #3 (0.613): 0.020*\"like\" + 0.011*\"one\" + 0.010*\"think\" + 0.009*\"really\" + 0.008*\"even\" + 0.008*\"okay\" + 0.007*\"time\" + 0.007*\"people\" + 0.007*\"know\" + 0.006*\"little\"\n",
      "2025-09-10 11:50:20,349 : INFO : topic diff=0.563366, rho=0.577350\n",
      "2025-09-10 11:50:20,350 : INFO : PROGRESS: pass 0, at document #8000/14833\n",
      "2025-09-10 11:50:21,385 : INFO : optimized alpha [0.23737942, 0.16337621, 0.19322261, 0.49187994]\n",
      "2025-09-10 11:50:21,390 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:21,395 : INFO : topic #0 (0.237): 0.012*\"like\" + 0.011*\"people\" + 0.011*\"think\" + 0.010*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"also\" + 0.006*\"even\" + 0.006*\"someone\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:21,396 : INFO : topic #1 (0.163): 0.009*\"people\" + 0.008*\"like\" + 0.008*\"think\" + 0.006*\"one\" + 0.005*\"actually\" + 0.005*\"even\" + 0.005*\"life\" + 0.004*\"way\" + 0.004*\"get\" + 0.004*\"really\"\n",
      "2025-09-10 11:50:21,396 : INFO : topic #2 (0.193): 0.013*\"would\" + 0.010*\"think\" + 0.009*\"like\" + 0.006*\"something\" + 0.005*\"even\" + 0.005*\"one\" + 0.005*\"people\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"might\"\n",
      "2025-09-10 11:50:21,397 : INFO : topic #3 (0.492): 0.017*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.007*\"people\" + 0.007*\"really\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"someone\" + 0.006*\"know\" + 0.006*\"something\"\n",
      "2025-09-10 11:50:21,397 : INFO : topic diff=0.471828, rho=0.500000\n",
      "2025-09-10 11:50:21,399 : INFO : PROGRESS: pass 0, at document #10000/14833\n",
      "2025-09-10 11:50:22,416 : INFO : optimized alpha [0.20364872, 0.14275602, 0.16618745, 0.46450788]\n",
      "2025-09-10 11:50:22,421 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:22,426 : INFO : topic #0 (0.204): 0.013*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.010*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"someone\" + 0.006*\"time\" + 0.005*\"also\"\n",
      "2025-09-10 11:50:22,427 : INFO : topic #1 (0.143): 0.010*\"people\" + 0.008*\"like\" + 0.007*\"think\" + 0.006*\"actually\" + 0.006*\"even\" + 0.006*\"life\" + 0.005*\"one\" + 0.005*\"way\" + 0.004*\"language\" + 0.004*\"time\"\n",
      "2025-09-10 11:50:22,427 : INFO : topic #2 (0.166): 0.015*\"would\" + 0.009*\"like\" + 0.008*\"think\" + 0.006*\"even\" + 0.006*\"people\" + 0.006*\"one\" + 0.005*\"something\" + 0.005*\"might\" + 0.005*\"time\" + 0.005*\"could\"\n",
      "2025-09-10 11:50:22,428 : INFO : topic #3 (0.465): 0.015*\"like\" + 0.009*\"one\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"way\"\n",
      "2025-09-10 11:50:22,428 : INFO : topic diff=0.399294, rho=0.447214\n",
      "2025-09-10 11:50:22,429 : INFO : PROGRESS: pass 0, at document #12000/14833\n",
      "2025-09-10 11:50:23,305 : INFO : optimized alpha [0.17907836, 0.12716894, 0.14504038, 0.43334502]\n",
      "2025-09-10 11:50:23,310 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:23,315 : INFO : topic #0 (0.179): 0.014*\"people\" + 0.010*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"someone\" + 0.005*\"often\" + 0.005*\"way\"\n",
      "2025-09-10 11:50:23,316 : INFO : topic #1 (0.127): 0.009*\"people\" + 0.007*\"like\" + 0.007*\"even\" + 0.006*\"one\" + 0.005*\"think\" + 0.005*\"actually\" + 0.005*\"sleep\" + 0.005*\"life\" + 0.004*\"way\" + 0.004*\"real\"\n",
      "2025-09-10 11:50:23,317 : INFO : topic #2 (0.145): 0.013*\"would\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"think\" + 0.005*\"people\" + 0.004*\"time\" + 0.004*\"might\" + 0.004*\"something\" + 0.004*\"could\"\n",
      "2025-09-10 11:50:23,318 : INFO : topic #3 (0.433): 0.015*\"like\" + 0.011*\"one\" + 0.008*\"even\" + 0.007*\"people\" + 0.007*\"time\" + 0.006*\"someone\" + 0.005*\"something\" + 0.005*\"way\" + 0.005*\"really\" + 0.005*\"think\"\n",
      "2025-09-10 11:50:23,318 : INFO : topic diff=0.373987, rho=0.408248\n",
      "2025-09-10 11:50:23,319 : INFO : PROGRESS: pass 0, at document #14000/14833\n",
      "2025-09-10 11:50:24,248 : INFO : optimized alpha [0.18367718, 0.12681478, 0.15451828, 0.4735398]\n",
      "2025-09-10 11:50:24,252 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:24,257 : INFO : topic #0 (0.184): 0.012*\"people\" + 0.009*\"like\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"think\" + 0.005*\"might\" + 0.005*\"someone\" + 0.005*\"way\"\n",
      "2025-09-10 11:50:24,257 : INFO : topic #1 (0.127): 0.009*\"people\" + 0.006*\"like\" + 0.006*\"one\" + 0.005*\"even\" + 0.005*\"life\" + 0.005*\"language\" + 0.005*\"sleep\" + 0.004*\"get\" + 0.004*\"think\" + 0.004*\"time\"\n",
      "2025-09-10 11:50:24,258 : INFO : topic #2 (0.155): 0.018*\"would\" + 0.007*\"one\" + 0.006*\"like\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"people\" + 0.004*\"think\" + 0.004*\"could\" + 0.004*\"travel\" + 0.003*\"something\"\n",
      "2025-09-10 11:50:24,259 : INFO : topic #3 (0.474): 0.011*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.007*\"people\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"life\" + 0.005*\"get\" + 0.004*\"someone\" + 0.004*\"back\"\n",
      "2025-09-10 11:50:24,259 : INFO : topic diff=0.787283, rho=0.377964\n",
      "2025-09-10 11:50:24,868 : INFO : -8.815 per-word bound, 450.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:24,868 : INFO : PROGRESS: pass 0, at document #14833/14833\n",
      "2025-09-10 11:50:25,247 : INFO : optimized alpha [0.17112139, 0.12389042, 0.15631366, 0.51251715]\n",
      "2025-09-10 11:50:25,253 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:25,260 : INFO : topic #0 (0.171): 0.013*\"people\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"think\" + 0.006*\"something\" + 0.005*\"might\" + 0.005*\"someone\" + 0.005*\"know\"\n",
      "2025-09-10 11:50:25,261 : INFO : topic #1 (0.124): 0.008*\"people\" + 0.007*\"teeth\" + 0.006*\"jobs\" + 0.005*\"like\" + 0.005*\"one\" + 0.005*\"money\" + 0.005*\"even\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"apple\"\n",
      "2025-09-10 11:50:25,262 : INFO : topic #2 (0.156): 0.017*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"like\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"people\" + 0.003*\"think\" + 0.003*\"war\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:25,263 : INFO : topic #3 (0.513): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"would\"\n",
      "2025-09-10 11:50:25,263 : INFO : topic diff=0.590993, rho=0.353553\n",
      "2025-09-10 11:50:25,266 : INFO : PROGRESS: pass 1, at document #2000/14833\n",
      "2025-09-10 11:50:26,273 : INFO : optimized alpha [0.19504558, 0.11392751, 0.13403118, 0.35379484]\n",
      "2025-09-10 11:50:26,278 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:26,283 : INFO : topic #0 (0.195): 0.011*\"people\" + 0.011*\"like\" + 0.009*\"might\" + 0.008*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"often\" + 0.006*\"also\"\n",
      "2025-09-10 11:50:26,284 : INFO : topic #1 (0.114): 0.007*\"people\" + 0.006*\"like\" + 0.005*\"one\" + 0.005*\"jobs\" + 0.005*\"language\" + 0.005*\"even\" + 0.005*\"apple\" + 0.005*\"life\" + 0.005*\"company\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:26,285 : INFO : topic #2 (0.134): 0.017*\"would\" + 0.007*\"one\" + 0.006*\"like\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"time\" + 0.004*\"think\" + 0.004*\"people\" + 0.003*\"might\" + 0.003*\"also\"\n",
      "2025-09-10 11:50:26,286 : INFO : topic #3 (0.354): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"get\" + 0.005*\"back\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:26,286 : INFO : topic diff=0.476961, rho=0.325878\n",
      "2025-09-10 11:50:26,287 : INFO : PROGRESS: pass 1, at document #4000/14833\n",
      "2025-09-10 11:50:27,221 : INFO : optimized alpha [0.21294428, 0.106777065, 0.117856294, 0.35150576]\n",
      "2025-09-10 11:50:27,226 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:27,231 : INFO : topic #0 (0.213): 0.012*\"like\" + 0.012*\"people\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.007*\"someone\" + 0.006*\"also\"\n",
      "2025-09-10 11:50:27,231 : INFO : topic #1 (0.107): 0.007*\"like\" + 0.007*\"people\" + 0.006*\"language\" + 0.005*\"one\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"think\" + 0.005*\"life\" + 0.004*\"business\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:50:27,232 : INFO : topic #2 (0.118): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"think\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"time\" + 0.004*\"people\" + 0.004*\"might\" + 0.003*\"also\"\n",
      "2025-09-10 11:50:27,233 : INFO : topic #3 (0.352): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.005*\"people\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:27,233 : INFO : topic diff=0.299567, rho=0.325878\n",
      "2025-09-10 11:50:27,234 : INFO : PROGRESS: pass 1, at document #6000/14833\n",
      "2025-09-10 11:50:28,065 : INFO : optimized alpha [0.20721631, 0.099055566, 0.102487646, 0.37134054]\n",
      "2025-09-10 11:50:28,069 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:28,074 : INFO : topic #0 (0.207): 0.014*\"like\" + 0.013*\"think\" + 0.012*\"people\" + 0.009*\"something\" + 0.008*\"might\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"also\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:28,075 : INFO : topic #1 (0.099): 0.008*\"like\" + 0.007*\"people\" + 0.007*\"think\" + 0.006*\"even\" + 0.005*\"also\" + 0.005*\"jobs\" + 0.005*\"one\" + 0.005*\"money\" + 0.005*\"sleep\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:28,076 : INFO : topic #2 (0.102): 0.018*\"would\" + 0.007*\"one\" + 0.007*\"like\" + 0.007*\"think\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"time\" + 0.004*\"people\" + 0.003*\"might\" + 0.003*\"war\"\n",
      "2025-09-10 11:50:28,077 : INFO : topic #3 (0.371): 0.015*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"people\" + 0.005*\"think\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:28,077 : INFO : topic diff=0.260751, rho=0.325878\n",
      "2025-09-10 11:50:28,078 : INFO : PROGRESS: pass 1, at document #8000/14833\n",
      "2025-09-10 11:50:28,889 : INFO : optimized alpha [0.1997441, 0.09503749, 0.09790675, 0.34664336]\n",
      "2025-09-10 11:50:28,893 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:28,898 : INFO : topic #0 (0.200): 0.013*\"think\" + 0.013*\"like\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"might\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"also\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:28,899 : INFO : topic #1 (0.095): 0.007*\"people\" + 0.007*\"like\" + 0.007*\"think\" + 0.005*\"jobs\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"actually\" + 0.005*\"money\" + 0.005*\"apple\" + 0.004*\"language\"\n",
      "2025-09-10 11:50:28,900 : INFO : topic #2 (0.098): 0.018*\"would\" + 0.007*\"think\" + 0.007*\"like\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"people\" + 0.004*\"time\" + 0.004*\"might\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:28,900 : INFO : topic #3 (0.347): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"know\" + 0.005*\"think\" + 0.005*\"something\" + 0.005*\"back\"\n",
      "2025-09-10 11:50:28,901 : INFO : topic diff=0.274582, rho=0.325878\n",
      "2025-09-10 11:50:28,901 : INFO : PROGRESS: pass 1, at document #10000/14833\n",
      "2025-09-10 11:50:29,702 : INFO : optimized alpha [0.1871772, 0.09278644, 0.09288619, 0.33740172]\n",
      "2025-09-10 11:50:29,706 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:29,711 : INFO : topic #0 (0.187): 0.014*\"people\" + 0.012*\"like\" + 0.012*\"think\" + 0.010*\"something\" + 0.007*\"someone\" + 0.007*\"might\" + 0.007*\"even\" + 0.006*\"one\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:29,711 : INFO : topic #1 (0.093): 0.008*\"people\" + 0.007*\"like\" + 0.006*\"think\" + 0.006*\"actually\" + 0.005*\"even\" + 0.005*\"sleep\" + 0.005*\"money\" + 0.005*\"language\" + 0.005*\"also\" + 0.004*\"one\"\n",
      "2025-09-10 11:50:29,712 : INFO : topic #2 (0.093): 0.019*\"would\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"people\" + 0.005*\"might\" + 0.004*\"time\" + 0.003*\"travel\"\n",
      "2025-09-10 11:50:29,713 : INFO : topic #3 (0.337): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"really\" + 0.005*\"people\" + 0.005*\"something\" + 0.005*\"back\" + 0.005*\"think\" + 0.005*\"day\"\n",
      "2025-09-10 11:50:29,713 : INFO : topic diff=0.257385, rho=0.325878\n",
      "2025-09-10 11:50:29,714 : INFO : PROGRESS: pass 1, at document #12000/14833\n",
      "2025-09-10 11:50:30,446 : INFO : optimized alpha [0.17536767, 0.08906648, 0.08761195, 0.32113513]\n",
      "2025-09-10 11:50:30,451 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:30,456 : INFO : topic #0 (0.175): 0.015*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:30,457 : INFO : topic #1 (0.089): 0.008*\"people\" + 0.007*\"like\" + 0.006*\"even\" + 0.006*\"sleep\" + 0.005*\"money\" + 0.005*\"actually\" + 0.005*\"think\" + 0.005*\"one\" + 0.004*\"jobs\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:30,458 : INFO : topic #2 (0.088): 0.016*\"would\" + 0.008*\"one\" + 0.007*\"like\" + 0.007*\"even\" + 0.005*\"think\" + 0.005*\"people\" + 0.005*\"could\" + 0.004*\"might\" + 0.004*\"time\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:30,459 : INFO : topic #3 (0.321): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"people\" + 0.005*\"something\" + 0.005*\"really\" + 0.005*\"way\" + 0.005*\"someone\" + 0.004*\"back\"\n",
      "2025-09-10 11:50:30,459 : INFO : topic diff=0.265356, rho=0.325878\n",
      "2025-09-10 11:50:30,460 : INFO : PROGRESS: pass 1, at document #14000/14833\n",
      "2025-09-10 11:50:31,265 : INFO : optimized alpha [0.17682475, 0.09163469, 0.09538987, 0.35046256]\n",
      "2025-09-10 11:50:31,270 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:31,275 : INFO : topic #0 (0.177): 0.014*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:50:31,275 : INFO : topic #1 (0.092): 0.008*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"even\" + 0.004*\"one\" + 0.004*\"get\" + 0.004*\"business\" + 0.004*\"think\"\n",
      "2025-09-10 11:50:31,276 : INFO : topic #2 (0.095): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"people\" + 0.004*\"travel\" + 0.004*\"think\" + 0.003*\"years\"\n",
      "2025-09-10 11:50:31,277 : INFO : topic #3 (0.350): 0.011*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"people\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"back\" + 0.005*\"got\" + 0.004*\"really\"\n",
      "2025-09-10 11:50:31,277 : INFO : topic diff=0.577450, rho=0.325878\n",
      "2025-09-10 11:50:31,825 : INFO : -8.620 per-word bound, 393.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:31,825 : INFO : PROGRESS: pass 1, at document #14833/14833\n",
      "2025-09-10 11:50:32,125 : INFO : optimized alpha [0.16085947, 0.092216656, 0.099126555, 0.378559]\n",
      "2025-09-10 11:50:32,129 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:32,134 : INFO : topic #0 (0.161): 0.015*\"people\" + 0.009*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"someone\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:50:32,135 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"apple\"\n",
      "2025-09-10 11:50:32,136 : INFO : topic #2 (0.099): 0.017*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"like\" + 0.004*\"people\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"years\"\n",
      "2025-09-10 11:50:32,137 : INFO : topic #3 (0.379): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"day\" + 0.005*\"people\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"back\"\n",
      "2025-09-10 11:50:32,137 : INFO : topic diff=0.464472, rho=0.325878\n",
      "2025-09-10 11:50:32,138 : INFO : PROGRESS: pass 2, at document #2000/14833\n",
      "2025-09-10 11:50:32,990 : INFO : optimized alpha [0.1876238, 0.08845297, 0.09263051, 0.29762048]\n",
      "2025-09-10 11:50:32,994 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:32,999 : INFO : topic #0 (0.188): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.007*\"life\" + 0.007*\"someone\"\n",
      "2025-09-10 11:50:32,999 : INFO : topic #1 (0.088): 0.006*\"people\" + 0.006*\"like\" + 0.006*\"money\" + 0.006*\"jobs\" + 0.005*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"business\" + 0.005*\"even\" + 0.005*\"also\"\n",
      "2025-09-10 11:50:33,000 : INFO : topic #2 (0.093): 0.017*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"even\" + 0.004*\"time\" + 0.003*\"people\" + 0.003*\"travel\" + 0.003*\"think\" + 0.003*\"war\"\n",
      "2025-09-10 11:50:33,001 : INFO : topic #3 (0.298): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"people\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"really\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:33,001 : INFO : topic diff=0.382931, rho=0.309841\n",
      "2025-09-10 11:50:33,002 : INFO : PROGRESS: pass 2, at document #4000/14833\n",
      "2025-09-10 11:50:33,830 : INFO : optimized alpha [0.21252513, 0.08662927, 0.08636475, 0.30258358]\n",
      "2025-09-10 11:50:33,834 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:33,839 : INFO : topic #0 (0.213): 0.013*\"like\" + 0.012*\"people\" + 0.012*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"life\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:33,840 : INFO : topic #1 (0.087): 0.007*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.006*\"language\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"sleep\" + 0.004*\"one\"\n",
      "2025-09-10 11:50:33,841 : INFO : topic #2 (0.086): 0.020*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"even\" + 0.004*\"time\" + 0.004*\"think\" + 0.004*\"travel\" + 0.004*\"people\" + 0.003*\"back\"\n",
      "2025-09-10 11:50:33,841 : INFO : topic #3 (0.303): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"people\"\n",
      "2025-09-10 11:50:33,842 : INFO : topic diff=0.248998, rho=0.309841\n",
      "2025-09-10 11:50:33,843 : INFO : PROGRESS: pass 2, at document #6000/14833\n",
      "2025-09-10 11:50:34,611 : INFO : optimized alpha [0.21780038, 0.08366648, 0.079229645, 0.32509273]\n",
      "2025-09-10 11:50:34,615 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:34,620 : INFO : topic #0 (0.218): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"might\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"also\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:34,620 : INFO : topic #1 (0.084): 0.007*\"like\" + 0.007*\"money\" + 0.007*\"people\" + 0.006*\"think\" + 0.006*\"even\" + 0.006*\"also\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"sleep\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:34,621 : INFO : topic #2 (0.079): 0.019*\"would\" + 0.008*\"one\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"think\" + 0.004*\"time\" + 0.004*\"war\" + 0.003*\"people\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:34,622 : INFO : topic #3 (0.325): 0.014*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"little\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:34,622 : INFO : topic diff=0.225651, rho=0.309841\n",
      "2025-09-10 11:50:34,623 : INFO : PROGRESS: pass 2, at document #8000/14833\n",
      "2025-09-10 11:50:35,368 : INFO : optimized alpha [0.2205008, 0.082864136, 0.07836214, 0.31892776]\n",
      "2025-09-10 11:50:35,373 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:35,378 : INFO : topic #0 (0.221): 0.014*\"like\" + 0.013*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"really\"\n",
      "2025-09-10 11:50:35,379 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.006*\"think\" + 0.005*\"also\" + 0.005*\"jobs\" + 0.005*\"even\" + 0.005*\"actually\" + 0.005*\"business\" + 0.005*\"apple\"\n",
      "2025-09-10 11:50:35,379 : INFO : topic #2 (0.078): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"time\" + 0.004*\"people\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:50:35,380 : INFO : topic #3 (0.319): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"know\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"something\"\n",
      "2025-09-10 11:50:35,380 : INFO : topic diff=0.238151, rho=0.309841\n",
      "2025-09-10 11:50:35,381 : INFO : PROGRESS: pass 2, at document #10000/14833\n",
      "2025-09-10 11:50:36,095 : INFO : optimized alpha [0.21269713, 0.08242825, 0.076105885, 0.31671733]\n",
      "2025-09-10 11:50:36,099 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:36,104 : INFO : topic #0 (0.213): 0.014*\"people\" + 0.013*\"like\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:36,106 : INFO : topic #1 (0.082): 0.007*\"people\" + 0.007*\"money\" + 0.007*\"like\" + 0.006*\"actually\" + 0.005*\"think\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"language\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:36,107 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.005*\"think\" + 0.004*\"people\" + 0.004*\"time\" + 0.004*\"might\" + 0.004*\"travel\"\n",
      "2025-09-10 11:50:36,107 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"something\" + 0.005*\"day\" + 0.005*\"people\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:36,108 : INFO : topic diff=0.224839, rho=0.309841\n",
      "2025-09-10 11:50:36,109 : INFO : PROGRESS: pass 2, at document #12000/14833\n",
      "2025-09-10 11:50:36,786 : INFO : optimized alpha [0.2017885, 0.08059133, 0.07350729, 0.3056044]\n",
      "2025-09-10 11:50:36,791 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:36,796 : INFO : topic #0 (0.202): 0.015*\"people\" + 0.012*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:36,797 : INFO : topic #1 (0.081): 0.008*\"people\" + 0.007*\"like\" + 0.007*\"money\" + 0.006*\"even\" + 0.006*\"sleep\" + 0.005*\"actually\" + 0.005*\"think\" + 0.004*\"business\" + 0.004*\"jobs\" + 0.004*\"one\"\n",
      "2025-09-10 11:50:36,798 : INFO : topic #2 (0.074): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.004*\"think\" + 0.004*\"people\" + 0.004*\"time\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:50:36,799 : INFO : topic #3 (0.306): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"people\" + 0.005*\"really\" + 0.005*\"something\" + 0.005*\"back\" + 0.004*\"way\" + 0.004*\"got\"\n",
      "2025-09-10 11:50:36,799 : INFO : topic diff=0.236286, rho=0.309841\n",
      "2025-09-10 11:50:36,800 : INFO : PROGRESS: pass 2, at document #14000/14833\n",
      "2025-09-10 11:50:37,544 : INFO : optimized alpha [0.20110849, 0.08371707, 0.080377795, 0.33255824]\n",
      "2025-09-10 11:50:37,549 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:37,554 : INFO : topic #0 (0.201): 0.014*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"time\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:37,555 : INFO : topic #1 (0.084): 0.007*\"people\" + 0.007*\"money\" + 0.006*\"like\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"even\" + 0.004*\"get\" + 0.004*\"business\" + 0.004*\"also\" + 0.004*\"work\"\n",
      "2025-09-10 11:50:37,556 : INFO : topic #2 (0.080): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"travel\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"people\" + 0.003*\"years\" + 0.003*\"back\"\n",
      "2025-09-10 11:50:37,557 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"people\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:50:37,557 : INFO : topic diff=0.515947, rho=0.309841\n",
      "2025-09-10 11:50:38,079 : INFO : -8.550 per-word bound, 374.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:38,080 : INFO : PROGRESS: pass 2, at document #14833/14833\n",
      "2025-09-10 11:50:38,364 : INFO : optimized alpha [0.17838603, 0.08540488, 0.084394984, 0.3567381]\n",
      "2025-09-10 11:50:38,368 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:38,373 : INFO : topic #0 (0.178): 0.015*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.007*\"something\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:38,374 : INFO : topic #1 (0.085): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.005*\"business\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"apple\"\n",
      "2025-09-10 11:50:38,375 : INFO : topic #2 (0.084): 0.017*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"like\" + 0.003*\"people\" + 0.003*\"students\" + 0.003*\"war\" + 0.003*\"years\"\n",
      "2025-09-10 11:50:38,376 : INFO : topic #3 (0.357): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:38,376 : INFO : topic diff=0.409711, rho=0.309841\n",
      "2025-09-10 11:50:38,377 : INFO : PROGRESS: pass 3, at document #2000/14833\n",
      "2025-09-10 11:50:39,155 : INFO : optimized alpha [0.20780952, 0.08309098, 0.08100876, 0.2901203]\n",
      "2025-09-10 11:50:39,159 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:39,164 : INFO : topic #0 (0.208): 0.012*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"life\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:39,165 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"language\" + 0.005*\"also\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:39,166 : INFO : topic #2 (0.081): 0.017*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"travel\" + 0.003*\"back\" + 0.003*\"students\" + 0.003*\"war\"\n",
      "2025-09-10 11:50:39,167 : INFO : topic #3 (0.290): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"really\"\n",
      "2025-09-10 11:50:39,167 : INFO : topic diff=0.336579, rho=0.295960\n",
      "2025-09-10 11:50:39,168 : INFO : PROGRESS: pass 3, at document #4000/14833\n",
      "2025-09-10 11:50:39,925 : INFO : optimized alpha [0.23613775, 0.08255253, 0.07699845, 0.2952609]\n",
      "2025-09-10 11:50:39,929 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:39,934 : INFO : topic #0 (0.236): 0.013*\"like\" + 0.012*\"people\" + 0.012*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:39,935 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"language\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"even\" + 0.005*\"sleep\" + 0.004*\"jobs\"\n",
      "2025-09-10 11:50:39,936 : INFO : topic #2 (0.077): 0.020*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"time\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"back\" + 0.003*\"think\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:39,936 : INFO : topic #3 (0.295): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.004*\"got\"\n",
      "2025-09-10 11:50:39,937 : INFO : topic diff=0.223879, rho=0.295960\n",
      "2025-09-10 11:50:39,938 : INFO : PROGRESS: pass 3, at document #6000/14833\n",
      "2025-09-10 11:50:40,673 : INFO : optimized alpha [0.24552089, 0.08080282, 0.07202876, 0.31730974]\n",
      "2025-09-10 11:50:40,677 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:40,682 : INFO : topic #0 (0.246): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:50:40,682 : INFO : topic #1 (0.081): 0.007*\"money\" + 0.007*\"like\" + 0.006*\"people\" + 0.006*\"think\" + 0.006*\"also\" + 0.006*\"even\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"sleep\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:40,683 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"time\" + 0.004*\"think\" + 0.004*\"war\" + 0.004*\"travel\" + 0.003*\"back\"\n",
      "2025-09-10 11:50:40,684 : INFO : topic #3 (0.317): 0.014*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:40,684 : INFO : topic diff=0.204965, rho=0.295960\n",
      "2025-09-10 11:50:40,685 : INFO : PROGRESS: pass 3, at document #8000/14833\n",
      "2025-09-10 11:50:41,393 : INFO : optimized alpha [0.2510494, 0.08111957, 0.0719462, 0.31575084]\n",
      "2025-09-10 11:50:41,397 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:41,402 : INFO : topic #0 (0.251): 0.014*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"really\" + 0.006*\"also\"\n",
      "2025-09-10 11:50:41,403 : INFO : topic #1 (0.081): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.006*\"think\" + 0.006*\"also\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"even\" + 0.005*\"actually\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:41,404 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"like\" + 0.005*\"think\" + 0.005*\"time\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"people\"\n",
      "2025-09-10 11:50:41,405 : INFO : topic #3 (0.316): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:41,405 : INFO : topic diff=0.214770, rho=0.295960\n",
      "2025-09-10 11:50:41,406 : INFO : PROGRESS: pass 3, at document #10000/14833\n",
      "2025-09-10 11:50:42,117 : INFO : optimized alpha [0.24461903, 0.08132265, 0.07044601, 0.31520665]\n",
      "2025-09-10 11:50:42,121 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:42,126 : INFO : topic #0 (0.245): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:42,127 : INFO : topic #1 (0.081): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"actually\" + 0.005*\"sleep\" + 0.005*\"think\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"language\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:42,128 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"time\" + 0.005*\"travel\" + 0.004*\"think\" + 0.004*\"back\" + 0.004*\"people\"\n",
      "2025-09-10 11:50:42,129 : INFO : topic #3 (0.315): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"something\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:42,129 : INFO : topic diff=0.204374, rho=0.295960\n",
      "2025-09-10 11:50:42,130 : INFO : PROGRESS: pass 3, at document #12000/14833\n",
      "2025-09-10 11:50:42,790 : INFO : optimized alpha [0.23243347, 0.07999461, 0.06849595, 0.3058022]\n",
      "2025-09-10 11:50:42,794 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:42,799 : INFO : topic #0 (0.232): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:42,800 : INFO : topic #1 (0.080): 0.007*\"people\" + 0.007*\"money\" + 0.006*\"like\" + 0.006*\"even\" + 0.005*\"sleep\" + 0.005*\"actually\" + 0.005*\"business\" + 0.004*\"jobs\" + 0.004*\"think\" + 0.004*\"get\"\n",
      "2025-09-10 11:50:42,801 : INFO : topic #2 (0.068): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"time\" + 0.004*\"students\" + 0.004*\"people\" + 0.004*\"travel\" + 0.004*\"think\"\n",
      "2025-09-10 11:50:42,801 : INFO : topic #3 (0.306): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"something\" + 0.005*\"people\" + 0.004*\"day\"\n",
      "2025-09-10 11:50:42,802 : INFO : topic diff=0.217107, rho=0.295960\n",
      "2025-09-10 11:50:42,802 : INFO : PROGRESS: pass 3, at document #14000/14833\n",
      "2025-09-10 11:50:43,532 : INFO : optimized alpha [0.22880273, 0.0834523, 0.0747692, 0.33094075]\n",
      "2025-09-10 11:50:43,537 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:43,542 : INFO : topic #0 (0.229): 0.014*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"someone\" + 0.007*\"time\" + 0.007*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:43,543 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"language\" + 0.005*\"get\" + 0.004*\"work\" + 0.004*\"business\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:43,544 : INFO : topic #2 (0.075): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"could\" + 0.004*\"like\" + 0.003*\"people\" + 0.003*\"years\" + 0.003*\"back\"\n",
      "2025-09-10 11:50:43,544 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"said\" + 0.004*\"people\"\n",
      "2025-09-10 11:50:43,545 : INFO : topic diff=0.476847, rho=0.295960\n",
      "2025-09-10 11:50:44,060 : INFO : -8.516 per-word bound, 366.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:44,061 : INFO : PROGRESS: pass 3, at document #14833/14833\n",
      "2025-09-10 11:50:44,338 : INFO : optimized alpha [0.19861442, 0.08526737, 0.07832097, 0.35201117]\n",
      "2025-09-10 11:50:44,342 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:44,347 : INFO : topic #0 (0.199): 0.015*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.007*\"someone\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:44,348 : INFO : topic #1 (0.085): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.005*\"business\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"company\" + 0.004*\"even\" + 0.004*\"work\"\n",
      "2025-09-10 11:50:44,349 : INFO : topic #2 (0.078): 0.017*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"like\" + 0.003*\"travel\" + 0.003*\"many\"\n",
      "2025-09-10 11:50:44,350 : INFO : topic #3 (0.352): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:44,350 : INFO : topic diff=0.373352, rho=0.295960\n",
      "2025-09-10 11:50:44,351 : INFO : PROGRESS: pass 4, at document #2000/14833\n",
      "2025-09-10 11:50:45,092 : INFO : optimized alpha [0.22962414, 0.08345417, 0.07615084, 0.29071587]\n",
      "2025-09-10 11:50:45,096 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:45,102 : INFO : topic #0 (0.230): 0.013*\"like\" + 0.012*\"people\" + 0.010*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:45,103 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"also\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:45,103 : INFO : topic #2 (0.076): 0.017*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"time\" + 0.004*\"even\" + 0.004*\"travel\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"students\" + 0.003*\"war\"\n",
      "2025-09-10 11:50:45,104 : INFO : topic #3 (0.291): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:45,105 : INFO : topic diff=0.308084, rho=0.283792\n",
      "2025-09-10 11:50:45,105 : INFO : PROGRESS: pass 4, at document #4000/14833\n",
      "2025-09-10 11:50:45,829 : INFO : optimized alpha [0.2603082, 0.08337832, 0.073184684, 0.29518247]\n",
      "2025-09-10 11:50:45,834 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:45,839 : INFO : topic #0 (0.260): 0.014*\"like\" + 0.012*\"people\" + 0.012*\"think\" + 0.008*\"something\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:45,839 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"language\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"even\" + 0.004*\"sleep\" + 0.004*\"financial\"\n",
      "2025-09-10 11:50:45,840 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"like\" + 0.004*\"back\" + 0.003*\"students\" + 0.003*\"human\"\n",
      "2025-09-10 11:50:45,841 : INFO : topic #3 (0.295): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"know\"\n",
      "2025-09-10 11:50:45,842 : INFO : topic diff=0.207844, rho=0.283792\n",
      "2025-09-10 11:50:45,842 : INFO : PROGRESS: pass 4, at document #6000/14833\n",
      "2025-09-10 11:50:46,564 : INFO : optimized alpha [0.27236417, 0.08194841, 0.069065936, 0.31657568]\n",
      "2025-09-10 11:50:46,568 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:46,573 : INFO : topic #0 (0.272): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:50:46,574 : INFO : topic #1 (0.082): 0.008*\"money\" + 0.007*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"even\" + 0.005*\"think\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"get\" + 0.005*\"sleep\"\n",
      "2025-09-10 11:50:46,575 : INFO : topic #2 (0.069): 0.019*\"would\" + 0.008*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"time\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:46,576 : INFO : topic #3 (0.317): 0.014*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:46,576 : INFO : topic diff=0.190503, rho=0.283792\n",
      "2025-09-10 11:50:46,577 : INFO : PROGRESS: pass 4, at document #8000/14833\n",
      "2025-09-10 11:50:47,273 : INFO : optimized alpha [0.27929482, 0.082586996, 0.06943399, 0.316647]\n",
      "2025-09-10 11:50:47,278 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:47,284 : INFO : topic #0 (0.279): 0.014*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"really\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:47,285 : INFO : topic #1 (0.083): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.006*\"also\" + 0.005*\"jobs\" + 0.005*\"think\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"actually\"\n",
      "2025-09-10 11:50:47,285 : INFO : topic #2 (0.069): 0.019*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:50:47,286 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:47,286 : INFO : topic diff=0.198336, rho=0.283792\n",
      "2025-09-10 11:50:47,287 : INFO : PROGRESS: pass 4, at document #10000/14833\n",
      "2025-09-10 11:50:47,965 : INFO : optimized alpha [0.2732403, 0.083076924, 0.068255335, 0.31644204]\n",
      "2025-09-10 11:50:47,969 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:47,974 : INFO : topic #0 (0.273): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:47,975 : INFO : topic #1 (0.083): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"actually\" + 0.005*\"sleep\" + 0.005*\"also\" + 0.005*\"even\" + 0.005*\"think\" + 0.005*\"work\" + 0.004*\"language\"\n",
      "2025-09-10 11:50:47,976 : INFO : topic #2 (0.068): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"travel\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"think\" + 0.003*\"people\"\n",
      "2025-09-10 11:50:47,977 : INFO : topic #3 (0.316): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"something\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:47,978 : INFO : topic diff=0.189757, rho=0.283792\n",
      "2025-09-10 11:50:47,978 : INFO : PROGRESS: pass 4, at document #12000/14833\n",
      "2025-09-10 11:50:48,626 : INFO : optimized alpha [0.25978568, 0.081873134, 0.066539176, 0.3078228]\n",
      "2025-09-10 11:50:48,630 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:48,635 : INFO : topic #0 (0.260): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:48,637 : INFO : topic #1 (0.082): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.006*\"even\" + 0.005*\"sleep\" + 0.005*\"actually\" + 0.005*\"business\" + 0.004*\"jobs\" + 0.004*\"work\" + 0.004*\"get\"\n",
      "2025-09-10 11:50:48,637 : INFO : topic #2 (0.067): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"travel\" + 0.004*\"students\" + 0.003*\"people\" + 0.003*\"back\"\n",
      "2025-09-10 11:50:48,638 : INFO : topic #3 (0.308): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"day\" + 0.004*\"made\" + 0.004*\"something\"\n",
      "2025-09-10 11:50:48,639 : INFO : topic diff=0.203082, rho=0.283792\n",
      "2025-09-10 11:50:48,639 : INFO : PROGRESS: pass 4, at document #14000/14833\n",
      "2025-09-10 11:50:49,358 : INFO : optimized alpha [0.25411963, 0.08544473, 0.07244065, 0.33190218]\n",
      "2025-09-10 11:50:49,363 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:49,368 : INFO : topic #0 (0.254): 0.014*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"life\" + 0.007*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:49,370 : INFO : topic #1 (0.085): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"sleep\" + 0.005*\"work\" + 0.005*\"even\" + 0.005*\"language\" + 0.005*\"get\" + 0.004*\"business\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:49,370 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"years\" + 0.003*\"people\"\n",
      "2025-09-10 11:50:49,371 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:49,371 : INFO : topic diff=0.447297, rho=0.283792\n",
      "2025-09-10 11:50:49,885 : INFO : -8.496 per-word bound, 361.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:49,886 : INFO : PROGRESS: pass 4, at document #14833/14833\n",
      "2025-09-10 11:50:50,163 : INFO : optimized alpha [0.21782821, 0.08717065, 0.07585909, 0.35047045]\n",
      "2025-09-10 11:50:50,167 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:50,171 : INFO : topic #0 (0.218): 0.015*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:50,172 : INFO : topic #1 (0.087): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"jobs\" + 0.005*\"teeth\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"work\" + 0.004*\"even\"\n",
      "2025-09-10 11:50:50,173 : INFO : topic #2 (0.076): 0.017*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.003*\"travel\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"many\" + 0.003*\"years\"\n",
      "2025-09-10 11:50:50,174 : INFO : topic #3 (0.350): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:50,174 : INFO : topic diff=0.346716, rho=0.283792\n",
      "2025-09-10 11:50:50,175 : INFO : PROGRESS: pass 5, at document #2000/14833\n",
      "2025-09-10 11:50:50,892 : INFO : optimized alpha [0.24974126, 0.085523024, 0.074278854, 0.2926406]\n",
      "2025-09-10 11:50:50,896 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:50,901 : INFO : topic #0 (0.250): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:50,902 : INFO : topic #1 (0.086): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"work\" + 0.005*\"also\" + 0.005*\"apple\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:50,902 : INFO : topic #2 (0.074): 0.017*\"would\" + 0.008*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"travel\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"war\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:50,903 : INFO : topic #3 (0.293): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"would\"\n",
      "2025-09-10 11:50:50,904 : INFO : topic diff=0.287858, rho=0.273011\n",
      "2025-09-10 11:50:50,904 : INFO : PROGRESS: pass 5, at document #4000/14833\n",
      "2025-09-10 11:50:51,619 : INFO : optimized alpha [0.2818893, 0.08559866, 0.07175354, 0.2963238]\n",
      "2025-09-10 11:50:51,623 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:51,627 : INFO : topic #0 (0.282): 0.014*\"like\" + 0.012*\"people\" + 0.012*\"think\" + 0.008*\"something\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:50:51,628 : INFO : topic #1 (0.086): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"language\" + 0.005*\"work\" + 0.005*\"company\" + 0.004*\"financial\" + 0.004*\"even\"\n",
      "2025-09-10 11:50:51,629 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.008*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"students\" + 0.003*\"human\"\n",
      "2025-09-10 11:50:51,630 : INFO : topic #3 (0.296): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\"\n",
      "2025-09-10 11:50:51,630 : INFO : topic diff=0.195811, rho=0.273011\n",
      "2025-09-10 11:50:51,630 : INFO : PROGRESS: pass 5, at document #6000/14833\n",
      "2025-09-10 11:50:52,344 : INFO : optimized alpha [0.29672313, 0.08433279, 0.06806059, 0.31719506]\n",
      "2025-09-10 11:50:52,348 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:52,353 : INFO : topic #0 (0.297): 0.016*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:50:52,354 : INFO : topic #1 (0.084): 0.008*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"even\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"get\" + 0.005*\"think\" + 0.005*\"sleep\"\n",
      "2025-09-10 11:50:52,355 : INFO : topic #2 (0.068): 0.019*\"would\" + 0.008*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:52,356 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:52,356 : INFO : topic diff=0.179304, rho=0.273011\n",
      "2025-09-10 11:50:52,356 : INFO : PROGRESS: pass 5, at document #8000/14833\n",
      "2025-09-10 11:50:53,046 : INFO : optimized alpha [0.3043157, 0.085167006, 0.068589106, 0.31775934]\n",
      "2025-09-10 11:50:53,050 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:53,055 : INFO : topic #0 (0.304): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"really\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:53,056 : INFO : topic #1 (0.085): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.006*\"also\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"think\" + 0.005*\"work\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:53,056 : INFO : topic #2 (0.069): 0.019*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.003*\"think\"\n",
      "2025-09-10 11:50:53,057 : INFO : topic #3 (0.318): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:53,058 : INFO : topic diff=0.185729, rho=0.273011\n",
      "2025-09-10 11:50:53,058 : INFO : PROGRESS: pass 5, at document #10000/14833\n",
      "2025-09-10 11:50:53,729 : INFO : optimized alpha [0.29819134, 0.085628, 0.067602806, 0.31745353]\n",
      "2025-09-10 11:50:53,733 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:53,739 : INFO : topic #0 (0.298): 0.014*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:53,740 : INFO : topic #1 (0.086): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"actually\" + 0.005*\"also\" + 0.005*\"work\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"get\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:53,741 : INFO : topic #2 (0.068): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"travel\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"back\" + 0.003*\"imagine\" + 0.003*\"think\"\n",
      "2025-09-10 11:50:53,742 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.004*\"get\" + 0.004*\"made\"\n",
      "2025-09-10 11:50:53,742 : INFO : topic diff=0.178782, rho=0.273011\n",
      "2025-09-10 11:50:53,743 : INFO : PROGRESS: pass 5, at document #12000/14833\n",
      "2025-09-10 11:50:54,384 : INFO : optimized alpha [0.2826776, 0.08445931, 0.06597888, 0.3091249]\n",
      "2025-09-10 11:50:54,389 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:54,393 : INFO : topic #0 (0.283): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:54,395 : INFO : topic #1 (0.084): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.006*\"even\" + 0.005*\"sleep\" + 0.005*\"work\" + 0.005*\"business\" + 0.004*\"actually\" + 0.004*\"jobs\" + 0.004*\"get\"\n",
      "2025-09-10 11:50:54,396 : INFO : topic #2 (0.066): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.003*\"back\" + 0.003*\"history\"\n",
      "2025-09-10 11:50:54,396 : INFO : topic #3 (0.309): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"made\" + 0.004*\"still\"\n",
      "2025-09-10 11:50:54,397 : INFO : topic diff=0.191899, rho=0.273011\n",
      "2025-09-10 11:50:54,397 : INFO : PROGRESS: pass 5, at document #14000/14833\n",
      "2025-09-10 11:50:55,109 : INFO : optimized alpha [0.27554017, 0.08801769, 0.07170481, 0.3322879]\n",
      "2025-09-10 11:50:55,114 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:55,119 : INFO : topic #0 (0.276): 0.015*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"life\" + 0.006*\"way\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:55,120 : INFO : topic #1 (0.088): 0.008*\"money\" + 0.007*\"people\" + 0.005*\"like\" + 0.005*\"work\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"language\" + 0.004*\"business\" + 0.004*\"also\"\n",
      "2025-09-10 11:50:55,121 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"back\" + 0.003*\"years\" + 0.003*\"like\" + 0.003*\"many\"\n",
      "2025-09-10 11:50:55,122 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:55,122 : INFO : topic diff=0.423237, rho=0.273011\n",
      "2025-09-10 11:50:55,639 : INFO : -8.483 per-word bound, 357.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:50:55,639 : INFO : PROGRESS: pass 5, at document #14833/14833\n",
      "2025-09-10 11:50:55,913 : INFO : optimized alpha [0.23324497, 0.08952683, 0.07480734, 0.34903622]\n",
      "2025-09-10 11:50:55,917 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:55,922 : INFO : topic #0 (0.233): 0.015*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:50:55,923 : INFO : topic #1 (0.090): 0.008*\"money\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"teeth\" + 0.005*\"business\" + 0.005*\"work\" + 0.005*\"get\" + 0.005*\"like\" + 0.005*\"company\" + 0.004*\"even\"\n",
      "2025-09-10 11:50:55,924 : INFO : topic #2 (0.075): 0.017*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.003*\"travel\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"many\" + 0.003*\"years\"\n",
      "2025-09-10 11:50:55,925 : INFO : topic #3 (0.349): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:55,926 : INFO : topic diff=0.326013, rho=0.273011\n",
      "2025-09-10 11:50:55,926 : INFO : PROGRESS: pass 6, at document #2000/14833\n",
      "2025-09-10 11:50:56,635 : INFO : optimized alpha [0.26563552, 0.087983206, 0.07342093, 0.2937521]\n",
      "2025-09-10 11:50:56,639 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:56,644 : INFO : topic #0 (0.266): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"time\"\n",
      "2025-09-10 11:50:56,646 : INFO : topic #1 (0.088): 0.007*\"money\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"work\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"get\" + 0.005*\"apple\"\n",
      "2025-09-10 11:50:56,647 : INFO : topic #2 (0.073): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"human\"\n",
      "2025-09-10 11:50:56,647 : INFO : topic #3 (0.294): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:50:56,648 : INFO : topic diff=0.271899, rho=0.263372\n",
      "2025-09-10 11:50:56,648 : INFO : PROGRESS: pass 6, at document #4000/14833\n",
      "2025-09-10 11:50:57,349 : INFO : optimized alpha [0.29863766, 0.088161685, 0.07111644, 0.29688597]\n",
      "2025-09-10 11:50:57,353 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:57,358 : INFO : topic #0 (0.299): 0.014*\"like\" + 0.013*\"people\" + 0.012*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:50:57,359 : INFO : topic #1 (0.088): 0.007*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"work\" + 0.005*\"business\" + 0.005*\"language\" + 0.005*\"company\" + 0.005*\"financial\" + 0.004*\"even\"\n",
      "2025-09-10 11:50:57,360 : INFO : topic #2 (0.071): 0.019*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"students\" + 0.003*\"human\"\n",
      "2025-09-10 11:50:57,360 : INFO : topic #3 (0.297): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"really\" + 0.005*\"know\"\n",
      "2025-09-10 11:50:57,361 : INFO : topic diff=0.186223, rho=0.263372\n",
      "2025-09-10 11:50:57,362 : INFO : PROGRESS: pass 6, at document #6000/14833\n",
      "2025-09-10 11:50:58,106 : INFO : optimized alpha [0.3148601, 0.08685955, 0.06762673, 0.31665066]\n",
      "2025-09-10 11:50:58,111 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:58,116 : INFO : topic #0 (0.315): 0.016*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:50:58,117 : INFO : topic #1 (0.087): 0.008*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"even\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"jobs\" + 0.005*\"work\" + 0.005*\"sleep\"\n",
      "2025-09-10 11:50:58,118 : INFO : topic #2 (0.068): 0.019*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"like\" + 0.003*\"students\"\n",
      "2025-09-10 11:50:58,118 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"little\" + 0.005*\"got\"\n",
      "2025-09-10 11:50:58,119 : INFO : topic diff=0.170263, rho=0.263372\n",
      "2025-09-10 11:50:58,120 : INFO : PROGRESS: pass 6, at document #8000/14833\n",
      "2025-09-10 11:50:58,838 : INFO : optimized alpha [0.3224193, 0.08771213, 0.06812546, 0.31739166]\n",
      "2025-09-10 11:50:58,843 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:58,847 : INFO : topic #0 (0.322): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:50:58,848 : INFO : topic #1 (0.088): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"also\" + 0.006*\"like\" + 0.005*\"jobs\" + 0.005*\"work\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"think\" + 0.005*\"even\"\n",
      "2025-09-10 11:50:58,849 : INFO : topic #2 (0.068): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"like\" + 0.003*\"war\"\n",
      "2025-09-10 11:50:58,850 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:50:58,850 : INFO : topic diff=0.175496, rho=0.263372\n",
      "2025-09-10 11:50:58,851 : INFO : PROGRESS: pass 6, at document #10000/14833\n",
      "2025-09-10 11:50:59,525 : INFO : optimized alpha [0.31599835, 0.08821284, 0.06731704, 0.3171885]\n",
      "2025-09-10 11:50:59,529 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:50:59,534 : INFO : topic #0 (0.316): 0.014*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:50:59,535 : INFO : topic #1 (0.088): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"work\" + 0.005*\"also\" + 0.005*\"actually\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"get\" + 0.004*\"business\"\n",
      "2025-09-10 11:50:59,536 : INFO : topic #2 (0.067): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"imagine\" + 0.003*\"human\"\n",
      "2025-09-10 11:50:59,537 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.004*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:50:59,538 : INFO : topic diff=0.169886, rho=0.263372\n",
      "2025-09-10 11:50:59,538 : INFO : PROGRESS: pass 6, at document #12000/14833\n",
      "2025-09-10 11:51:00,181 : INFO : optimized alpha [0.29978195, 0.086953744, 0.06572632, 0.30959255]\n",
      "2025-09-10 11:51:00,185 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:00,190 : INFO : topic #0 (0.300): 0.015*\"people\" + 0.014*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:00,191 : INFO : topic #1 (0.087): 0.007*\"money\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"sleep\" + 0.005*\"work\" + 0.005*\"business\" + 0.004*\"actually\" + 0.004*\"jobs\" + 0.004*\"get\"\n",
      "2025-09-10 11:51:00,191 : INFO : topic #2 (0.066): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.004*\"could\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.003*\"history\"\n",
      "2025-09-10 11:51:00,192 : INFO : topic #3 (0.310): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"made\" + 0.004*\"still\"\n",
      "2025-09-10 11:51:00,193 : INFO : topic diff=0.182737, rho=0.263372\n",
      "2025-09-10 11:51:00,193 : INFO : PROGRESS: pass 6, at document #14000/14833\n",
      "2025-09-10 11:51:00,906 : INFO : optimized alpha [0.29141465, 0.09042678, 0.07122853, 0.33192587]\n",
      "2025-09-10 11:51:00,911 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:00,915 : INFO : topic #0 (0.291): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:00,916 : INFO : topic #1 (0.090): 0.008*\"money\" + 0.007*\"people\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"language\" + 0.004*\"business\" + 0.004*\"also\"\n",
      "2025-09-10 11:51:00,917 : INFO : topic #2 (0.071): 0.019*\"would\" + 0.007*\"time\" + 0.007*\"one\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"back\" + 0.003*\"years\" + 0.003*\"like\" + 0.003*\"many\"\n",
      "2025-09-10 11:51:00,918 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:51:00,918 : INFO : topic diff=0.403119, rho=0.263372\n",
      "2025-09-10 11:51:01,436 : INFO : -8.472 per-word bound, 355.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:01,436 : INFO : PROGRESS: pass 6, at document #14833/14833\n",
      "2025-09-10 11:51:01,714 : INFO : optimized alpha [0.24590543, 0.09167317, 0.074143544, 0.34749502]\n",
      "2025-09-10 11:51:01,718 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:01,723 : INFO : topic #0 (0.246): 0.015*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:01,724 : INFO : topic #1 (0.092): 0.008*\"money\" + 0.006*\"people\" + 0.005*\"work\" + 0.005*\"jobs\" + 0.005*\"teeth\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"like\" + 0.005*\"company\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:01,725 : INFO : topic #2 (0.074): 0.017*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.003*\"travel\" + 0.003*\"war\" + 0.003*\"many\" + 0.003*\"students\" + 0.003*\"back\"\n",
      "2025-09-10 11:51:01,726 : INFO : topic #3 (0.347): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:01,726 : INFO : topic diff=0.309267, rho=0.263372\n",
      "2025-09-10 11:51:01,727 : INFO : PROGRESS: pass 7, at document #2000/14833\n",
      "2025-09-10 11:51:02,427 : INFO : optimized alpha [0.27826536, 0.09013786, 0.0728106, 0.29429707]\n",
      "2025-09-10 11:51:02,432 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:02,437 : INFO : topic #0 (0.278): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"time\"\n",
      "2025-09-10 11:51:02,438 : INFO : topic #1 (0.090): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"get\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:02,439 : INFO : topic #2 (0.073): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"students\" + 0.003*\"human\"\n",
      "2025-09-10 11:51:02,440 : INFO : topic #3 (0.294): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:02,440 : INFO : topic diff=0.258968, rho=0.254687\n",
      "2025-09-10 11:51:02,440 : INFO : PROGRESS: pass 7, at document #4000/14833\n",
      "2025-09-10 11:51:03,141 : INFO : optimized alpha [0.31176445, 0.090297595, 0.07074619, 0.29714885]\n",
      "2025-09-10 11:51:03,145 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:03,150 : INFO : topic #0 (0.312): 0.014*\"like\" + 0.013*\"people\" + 0.012*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"way\"\n",
      "2025-09-10 11:51:03,150 : INFO : topic #1 (0.090): 0.007*\"money\" + 0.006*\"like\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"language\" + 0.005*\"company\" + 0.004*\"financial\" + 0.004*\"get\"\n",
      "2025-09-10 11:51:03,151 : INFO : topic #2 (0.071): 0.019*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"students\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:03,152 : INFO : topic #3 (0.297): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"really\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:03,153 : INFO : topic diff=0.178188, rho=0.254687\n",
      "2025-09-10 11:51:03,153 : INFO : PROGRESS: pass 7, at document #6000/14833\n",
      "2025-09-10 11:51:03,856 : INFO : optimized alpha [0.3289356, 0.08892574, 0.06731657, 0.31556457]\n",
      "2025-09-10 11:51:03,861 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:03,866 : INFO : topic #0 (0.329): 0.016*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:03,866 : INFO : topic #1 (0.089): 0.008*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"business\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"work\" + 0.005*\"jobs\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:51:03,867 : INFO : topic #2 (0.067): 0.018*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"students\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:03,868 : INFO : topic #3 (0.316): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:51:03,868 : INFO : topic diff=0.162618, rho=0.254687\n",
      "2025-09-10 11:51:03,869 : INFO : PROGRESS: pass 7, at document #8000/14833\n",
      "2025-09-10 11:51:04,552 : INFO : optimized alpha [0.33675525, 0.08978933, 0.06780021, 0.31673536]\n",
      "2025-09-10 11:51:04,557 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:04,562 : INFO : topic #0 (0.337): 0.015*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:04,562 : INFO : topic #1 (0.090): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"like\" + 0.005*\"work\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:04,563 : INFO : topic #2 (0.068): 0.019*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.003*\"like\" + 0.003*\"war\"\n",
      "2025-09-10 11:51:04,564 : INFO : topic #3 (0.317): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:04,564 : INFO : topic diff=0.167190, rho=0.254687\n",
      "2025-09-10 11:51:04,565 : INFO : PROGRESS: pass 7, at document #10000/14833\n",
      "2025-09-10 11:51:05,228 : INFO : optimized alpha [0.3302005, 0.09024452, 0.06711856, 0.31638873]\n",
      "2025-09-10 11:51:05,233 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:05,238 : INFO : topic #0 (0.330): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:51:05,239 : INFO : topic #1 (0.090): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"work\" + 0.005*\"also\" + 0.005*\"actually\" + 0.005*\"get\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.004*\"business\"\n",
      "2025-09-10 11:51:05,239 : INFO : topic #2 (0.067): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"imagine\" + 0.003*\"human\"\n",
      "2025-09-10 11:51:05,240 : INFO : topic #3 (0.316): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.004*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:51:05,241 : INFO : topic diff=0.162494, rho=0.254687\n",
      "2025-09-10 11:51:05,241 : INFO : PROGRESS: pass 7, at document #12000/14833\n",
      "2025-09-10 11:51:05,880 : INFO : optimized alpha [0.3138668, 0.088942535, 0.06556445, 0.30946818]\n",
      "2025-09-10 11:51:05,885 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:05,891 : INFO : topic #0 (0.314): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.010*\"something\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:05,892 : INFO : topic #1 (0.089): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"work\" + 0.005*\"sleep\" + 0.005*\"business\" + 0.004*\"get\" + 0.004*\"jobs\" + 0.004*\"actually\"\n",
      "2025-09-10 11:51:05,893 : INFO : topic #2 (0.066): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"travel\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"history\"\n",
      "2025-09-10 11:51:05,894 : INFO : topic #3 (0.309): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"really\" + 0.005*\"still\"\n",
      "2025-09-10 11:51:05,894 : INFO : topic diff=0.174905, rho=0.254687\n",
      "2025-09-10 11:51:05,894 : INFO : PROGRESS: pass 7, at document #14000/14833\n",
      "2025-09-10 11:51:06,600 : INFO : optimized alpha [0.30442476, 0.09229126, 0.07088353, 0.3309192]\n",
      "2025-09-10 11:51:06,604 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:06,609 : INFO : topic #0 (0.304): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:06,611 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"business\" + 0.004*\"language\" + 0.004*\"also\"\n",
      "2025-09-10 11:51:06,612 : INFO : topic #2 (0.071): 0.018*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"back\" + 0.003*\"years\" + 0.003*\"many\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:06,613 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"years\"\n",
      "2025-09-10 11:51:06,613 : INFO : topic diff=0.385815, rho=0.254687\n",
      "2025-09-10 11:51:07,131 : INFO : -8.465 per-word bound, 353.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:07,132 : INFO : PROGRESS: pass 7, at document #14833/14833\n",
      "2025-09-10 11:51:07,409 : INFO : optimized alpha [0.25596216, 0.093553446, 0.07353372, 0.34537166]\n",
      "2025-09-10 11:51:07,413 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:07,418 : INFO : topic #0 (0.256): 0.015*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:07,419 : INFO : topic #1 (0.094): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"jobs\" + 0.005*\"get\" + 0.005*\"business\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:07,420 : INFO : topic #2 (0.074): 0.017*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"travel\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"many\" + 0.003*\"students\"\n",
      "2025-09-10 11:51:07,420 : INFO : topic #3 (0.345): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:07,421 : INFO : topic diff=0.295459, rho=0.254687\n",
      "2025-09-10 11:51:07,421 : INFO : PROGRESS: pass 8, at document #2000/14833\n",
      "2025-09-10 11:51:08,109 : INFO : optimized alpha [0.28809977, 0.091936015, 0.072293766, 0.2943397]\n",
      "2025-09-10 11:51:08,113 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:08,119 : INFO : topic #0 (0.288): 0.013*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:08,120 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.006*\"work\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"also\" + 0.005*\"company\" + 0.005*\"get\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:08,121 : INFO : topic #2 (0.072): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"human\" + 0.003*\"students\"\n",
      "2025-09-10 11:51:08,121 : INFO : topic #3 (0.294): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:08,122 : INFO : topic diff=0.247908, rho=0.246808\n",
      "2025-09-10 11:51:08,122 : INFO : PROGRESS: pass 8, at document #4000/14833\n",
      "2025-09-10 11:51:08,811 : INFO : optimized alpha [0.3216679, 0.092099406, 0.07047619, 0.29667127]\n",
      "2025-09-10 11:51:08,816 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:08,821 : INFO : topic #0 (0.322): 0.014*\"like\" + 0.013*\"people\" + 0.012*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"really\"\n",
      "2025-09-10 11:51:08,822 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"people\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"language\" + 0.004*\"get\" + 0.004*\"financial\"\n",
      "2025-09-10 11:51:08,823 : INFO : topic #2 (0.070): 0.019*\"would\" + 0.008*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"human\" + 0.003*\"many\" + 0.003*\"students\"\n",
      "2025-09-10 11:51:08,824 : INFO : topic #3 (0.297): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"really\"\n",
      "2025-09-10 11:51:08,824 : INFO : topic diff=0.171354, rho=0.246808\n",
      "2025-09-10 11:51:08,824 : INFO : PROGRESS: pass 8, at document #6000/14833\n",
      "2025-09-10 11:51:09,520 : INFO : optimized alpha [0.339383, 0.09073871, 0.067163065, 0.31415793]\n",
      "2025-09-10 11:51:09,525 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:09,529 : INFO : topic #0 (0.339): 0.016*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:09,531 : INFO : topic #1 (0.091): 0.008*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"work\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"jobs\" + 0.004*\"company\"\n",
      "2025-09-10 11:51:09,532 : INFO : topic #2 (0.067): 0.018*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"students\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:09,533 : INFO : topic #3 (0.314): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:51:09,533 : INFO : topic diff=0.156010, rho=0.246808\n",
      "2025-09-10 11:51:09,533 : INFO : PROGRESS: pass 8, at document #8000/14833\n",
      "2025-09-10 11:51:10,212 : INFO : optimized alpha [0.34717658, 0.09158826, 0.06773645, 0.3154766]\n",
      "2025-09-10 11:51:10,216 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:10,221 : INFO : topic #0 (0.347): 0.015*\"like\" + 0.014*\"think\" + 0.014*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:10,222 : INFO : topic #1 (0.092): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"work\" + 0.005*\"like\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:10,223 : INFO : topic #2 (0.068): 0.018*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.003*\"war\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:10,224 : INFO : topic #3 (0.315): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:10,224 : INFO : topic diff=0.160106, rho=0.246808\n",
      "2025-09-10 11:51:10,225 : INFO : PROGRESS: pass 8, at document #10000/14833\n",
      "2025-09-10 11:51:10,881 : INFO : optimized alpha [0.3406253, 0.09201205, 0.06707356, 0.31482154]\n",
      "2025-09-10 11:51:10,885 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:10,890 : INFO : topic #0 (0.341): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.007*\"life\"\n",
      "2025-09-10 11:51:10,891 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"get\" + 0.005*\"actually\" + 0.005*\"even\" + 0.005*\"sleep\" + 0.004*\"business\"\n",
      "2025-09-10 11:51:10,892 : INFO : topic #2 (0.067): 0.020*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"imagine\" + 0.003*\"human\"\n",
      "2025-09-10 11:51:10,893 : INFO : topic #3 (0.315): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:51:10,893 : INFO : topic diff=0.156156, rho=0.246808\n",
      "2025-09-10 11:51:10,894 : INFO : PROGRESS: pass 8, at document #12000/14833\n",
      "2025-09-10 11:51:11,532 : INFO : optimized alpha [0.32444012, 0.09064024, 0.0654516, 0.30857557]\n",
      "2025-09-10 11:51:11,536 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:11,541 : INFO : topic #0 (0.324): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.010*\"something\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:11,542 : INFO : topic #1 (0.091): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"work\" + 0.005*\"sleep\" + 0.004*\"business\" + 0.004*\"get\" + 0.004*\"jobs\" + 0.004*\"also\"\n",
      "2025-09-10 11:51:11,543 : INFO : topic #2 (0.065): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"travel\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"history\"\n",
      "2025-09-10 11:51:11,544 : INFO : topic #3 (0.309): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\" + 0.005*\"years\"\n",
      "2025-09-10 11:51:11,544 : INFO : topic diff=0.167942, rho=0.246808\n",
      "2025-09-10 11:51:11,545 : INFO : PROGRESS: pass 8, at document #14000/14833\n",
      "2025-09-10 11:51:12,250 : INFO : optimized alpha [0.31412637, 0.09391187, 0.070549875, 0.32914907]\n",
      "2025-09-10 11:51:12,255 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:12,260 : INFO : topic #0 (0.314): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:12,261 : INFO : topic #1 (0.094): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"business\" + 0.004*\"also\" + 0.004*\"language\"\n",
      "2025-09-10 11:51:12,261 : INFO : topic #2 (0.071): 0.018*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"back\" + 0.003*\"years\" + 0.003*\"many\" + 0.003*\"human\"\n",
      "2025-09-10 11:51:12,262 : INFO : topic #3 (0.329): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:12,263 : INFO : topic diff=0.370455, rho=0.246808\n",
      "2025-09-10 11:51:12,782 : INFO : -8.459 per-word bound, 351.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:12,783 : INFO : PROGRESS: pass 8, at document #14833/14833\n",
      "2025-09-10 11:51:13,060 : INFO : optimized alpha [0.26413196, 0.094995946, 0.07302997, 0.34298]\n",
      "2025-09-10 11:51:13,065 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:13,070 : INFO : topic #0 (0.264): 0.015*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:13,071 : INFO : topic #1 (0.095): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"jobs\" + 0.005*\"get\" + 0.005*\"business\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:13,072 : INFO : topic #2 (0.073): 0.016*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"travel\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"many\" + 0.003*\"years\"\n",
      "2025-09-10 11:51:13,073 : INFO : topic #3 (0.343): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:13,074 : INFO : topic diff=0.283410, rho=0.246808\n",
      "2025-09-10 11:51:13,074 : INFO : PROGRESS: pass 9, at document #2000/14833\n",
      "2025-09-10 11:51:13,756 : INFO : optimized alpha [0.29593137, 0.09337855, 0.07194823, 0.29415268]\n",
      "2025-09-10 11:51:13,761 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:13,766 : INFO : topic #0 (0.296): 0.014*\"like\" + 0.013*\"people\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:13,767 : INFO : topic #1 (0.093): 0.007*\"money\" + 0.006*\"work\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"business\" + 0.005*\"also\" + 0.005*\"company\" + 0.005*\"get\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:13,768 : INFO : topic #2 (0.072): 0.016*\"would\" + 0.008*\"one\" + 0.007*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"even\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"human\" + 0.003*\"students\"\n",
      "2025-09-10 11:51:13,769 : INFO : topic #3 (0.294): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:13,769 : INFO : topic diff=0.238407, rho=0.239618\n",
      "2025-09-10 11:51:13,770 : INFO : PROGRESS: pass 9, at document #4000/14833\n",
      "2025-09-10 11:51:14,452 : INFO : optimized alpha [0.32936826, 0.0935075, 0.070254385, 0.29579327]\n",
      "2025-09-10 11:51:14,457 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:14,461 : INFO : topic #0 (0.329): 0.015*\"like\" + 0.013*\"people\" + 0.012*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"someone\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"really\"\n",
      "2025-09-10 11:51:14,462 : INFO : topic #1 (0.094): 0.007*\"money\" + 0.006*\"work\" + 0.005*\"people\" + 0.005*\"like\" + 0.005*\"also\" + 0.005*\"business\" + 0.005*\"company\" + 0.005*\"get\" + 0.004*\"language\" + 0.004*\"financial\"\n",
      "2025-09-10 11:51:14,463 : INFO : topic #2 (0.070): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.003*\"human\" + 0.003*\"many\" + 0.003*\"students\"\n",
      "2025-09-10 11:51:14,464 : INFO : topic #3 (0.296): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\" + 0.004*\"said\"\n",
      "2025-09-10 11:51:14,464 : INFO : topic diff=0.165296, rho=0.239618\n",
      "2025-09-10 11:51:14,465 : INFO : PROGRESS: pass 9, at document #6000/14833\n",
      "2025-09-10 11:51:15,155 : INFO : optimized alpha [0.3472864, 0.0921064, 0.06707818, 0.3126201]\n",
      "2025-09-10 11:51:15,160 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:15,165 : INFO : topic #0 (0.347): 0.016*\"like\" + 0.014*\"think\" + 0.013*\"people\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:15,165 : INFO : topic #1 (0.092): 0.008*\"money\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"also\" + 0.005*\"work\" + 0.005*\"get\" + 0.005*\"business\" + 0.005*\"even\" + 0.005*\"jobs\" + 0.004*\"company\"\n",
      "2025-09-10 11:51:15,167 : INFO : topic #2 (0.067): 0.018*\"would\" + 0.008*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"students\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:15,167 : INFO : topic #3 (0.313): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:15,168 : INFO : topic diff=0.150233, rho=0.239618\n",
      "2025-09-10 11:51:15,168 : INFO : PROGRESS: pass 9, at document #8000/14833\n",
      "2025-09-10 11:51:15,834 : INFO : optimized alpha [0.35494712, 0.092913225, 0.06767183, 0.31399328]\n",
      "2025-09-10 11:51:15,838 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:15,843 : INFO : topic #0 (0.355): 0.015*\"like\" + 0.014*\"think\" + 0.014*\"people\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"really\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:15,844 : INFO : topic #1 (0.093): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.006*\"also\" + 0.005*\"like\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"get\" + 0.005*\"even\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:15,845 : INFO : topic #2 (0.068): 0.018*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.003*\"war\" + 0.003*\"like\"\n",
      "2025-09-10 11:51:15,845 : INFO : topic #3 (0.314): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:15,846 : INFO : topic diff=0.154007, rho=0.239618\n",
      "2025-09-10 11:51:15,846 : INFO : PROGRESS: pass 9, at document #10000/14833\n",
      "2025-09-10 11:51:16,501 : INFO : optimized alpha [0.34841707, 0.09331756, 0.06703204, 0.31305036]\n",
      "2025-09-10 11:51:16,505 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:16,510 : INFO : topic #0 (0.348): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.010*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.007*\"life\"\n",
      "2025-09-10 11:51:16,512 : INFO : topic #1 (0.093): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"also\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"actually\" + 0.004*\"sleep\" + 0.004*\"business\"\n",
      "2025-09-10 11:51:16,512 : INFO : topic #2 (0.067): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"imagine\" + 0.003*\"human\"\n",
      "2025-09-10 11:51:16,513 : INFO : topic #3 (0.313): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.004*\"get\"\n",
      "2025-09-10 11:51:16,514 : INFO : topic diff=0.150609, rho=0.239618\n",
      "2025-09-10 11:51:16,514 : INFO : PROGRESS: pass 9, at document #12000/14833\n",
      "2025-09-10 11:51:17,147 : INFO : optimized alpha [0.33257884, 0.0919719, 0.06530798, 0.30727234]\n",
      "2025-09-10 11:51:17,152 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:17,158 : INFO : topic #0 (0.333): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.010*\"something\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:17,159 : INFO : topic #1 (0.092): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"work\" + 0.005*\"sleep\" + 0.004*\"business\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"jobs\"\n",
      "2025-09-10 11:51:17,160 : INFO : topic #2 (0.065): 0.017*\"would\" + 0.008*\"one\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"travel\" + 0.004*\"could\" + 0.004*\"back\" + 0.004*\"students\" + 0.003*\"like\" + 0.003*\"history\"\n",
      "2025-09-10 11:51:17,161 : INFO : topic #3 (0.307): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"still\" + 0.005*\"made\" + 0.005*\"years\"\n",
      "2025-09-10 11:51:17,161 : INFO : topic diff=0.161915, rho=0.239618\n",
      "2025-09-10 11:51:17,162 : INFO : PROGRESS: pass 9, at document #14000/14833\n",
      "2025-09-10 11:51:17,860 : INFO : optimized alpha [0.3216376, 0.09526992, 0.070278324, 0.3270887]\n",
      "2025-09-10 11:51:17,864 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:17,869 : INFO : topic #0 (0.322): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"something\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:17,871 : INFO : topic #1 (0.095): 0.007*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"get\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.004*\"business\" + 0.004*\"also\" + 0.004*\"language\"\n",
      "2025-09-10 11:51:17,872 : INFO : topic #2 (0.070): 0.018*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"black\" + 0.004*\"back\" + 0.004*\"hole\" + 0.003*\"years\"\n",
      "2025-09-10 11:51:17,872 : INFO : topic #3 (0.327): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:17,873 : INFO : topic diff=0.356622, rho=0.239618\n",
      "2025-09-10 11:51:18,391 : INFO : -8.454 per-word bound, 350.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:18,392 : INFO : PROGRESS: pass 9, at document #14833/14833\n",
      "2025-09-10 11:51:18,670 : INFO : optimized alpha [0.2701828, 0.09630276, 0.07265067, 0.3401604]\n",
      "2025-09-10 11:51:18,675 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:18,680 : INFO : topic #0 (0.270): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:18,681 : INFO : topic #1 (0.096): 0.008*\"money\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"get\" + 0.005*\"jobs\" + 0.005*\"business\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:18,682 : INFO : topic #2 (0.073): 0.016*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"travel\" + 0.003*\"back\" + 0.003*\"black\" + 0.003*\"war\" + 0.003*\"many\"\n",
      "2025-09-10 11:51:18,682 : INFO : topic #3 (0.340): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:18,683 : INFO : topic diff=0.272560, rho=0.239618\n",
      "2025-09-10 11:51:18,683 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=34546, num_topics=4, decay=0.5, chunksize=2000> in 61.89s', 'datetime': '2025-09-10T11:51:18.683786', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-09-10 11:51:18,686 : INFO : using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows\n",
      "2025-09-10 11:51:22,540 : INFO : 1 batches submitted to accumulate stats from 64 documents (2312 virtual)\n",
      "2025-09-10 11:51:22,545 : INFO : 2 batches submitted to accumulate stats from 128 documents (4776 virtual)\n",
      "2025-09-10 11:51:22,549 : INFO : 3 batches submitted to accumulate stats from 192 documents (8552 virtual)\n",
      "2025-09-10 11:51:22,552 : INFO : 4 batches submitted to accumulate stats from 256 documents (12770 virtual)\n",
      "2025-09-10 11:51:22,556 : INFO : 5 batches submitted to accumulate stats from 320 documents (15636 virtual)\n",
      "2025-09-10 11:51:22,560 : INFO : 6 batches submitted to accumulate stats from 384 documents (19247 virtual)\n",
      "2025-09-10 11:51:22,572 : INFO : 7 batches submitted to accumulate stats from 448 documents (20248 virtual)\n",
      "2025-09-10 11:51:22,574 : INFO : 8 batches submitted to accumulate stats from 512 documents (21583 virtual)\n",
      "2025-09-10 11:51:22,577 : INFO : 9 batches submitted to accumulate stats from 576 documents (24028 virtual)\n",
      "2025-09-10 11:51:22,579 : INFO : 10 batches submitted to accumulate stats from 640 documents (24977 virtual)\n",
      "2025-09-10 11:51:22,582 : INFO : 11 batches submitted to accumulate stats from 704 documents (26982 virtual)\n",
      "2025-09-10 11:51:22,584 : INFO : 12 batches submitted to accumulate stats from 768 documents (30059 virtual)\n",
      "2025-09-10 11:51:22,586 : INFO : 13 batches submitted to accumulate stats from 832 documents (30786 virtual)\n",
      "2025-09-10 11:51:22,589 : INFO : 14 batches submitted to accumulate stats from 896 documents (34101 virtual)\n",
      "2025-09-10 11:51:22,591 : INFO : 15 batches submitted to accumulate stats from 960 documents (34664 virtual)\n",
      "2025-09-10 11:51:22,595 : INFO : 16 batches submitted to accumulate stats from 1024 documents (38291 virtual)\n",
      "2025-09-10 11:51:22,598 : INFO : 17 batches submitted to accumulate stats from 1088 documents (40666 virtual)\n",
      "2025-09-10 11:51:22,600 : INFO : 18 batches submitted to accumulate stats from 1152 documents (42061 virtual)\n",
      "2025-09-10 11:51:22,603 : INFO : 19 batches submitted to accumulate stats from 1216 documents (44289 virtual)\n",
      "2025-09-10 11:51:22,605 : INFO : 20 batches submitted to accumulate stats from 1280 documents (47210 virtual)\n",
      "2025-09-10 11:51:22,608 : INFO : 21 batches submitted to accumulate stats from 1344 documents (48243 virtual)\n",
      "2025-09-10 11:51:22,610 : INFO : 22 batches submitted to accumulate stats from 1408 documents (48909 virtual)\n",
      "2025-09-10 11:51:22,612 : INFO : 23 batches submitted to accumulate stats from 1472 documents (49511 virtual)\n",
      "2025-09-10 11:51:22,614 : INFO : 24 batches submitted to accumulate stats from 1536 documents (51165 virtual)\n",
      "2025-09-10 11:51:22,616 : INFO : 25 batches submitted to accumulate stats from 1600 documents (51700 virtual)\n",
      "2025-09-10 11:51:22,621 : INFO : 26 batches submitted to accumulate stats from 1664 documents (54911 virtual)\n",
      "2025-09-10 11:51:22,624 : INFO : 27 batches submitted to accumulate stats from 1728 documents (58811 virtual)\n",
      "2025-09-10 11:51:22,627 : INFO : 28 batches submitted to accumulate stats from 1792 documents (62711 virtual)\n",
      "2025-09-10 11:51:22,630 : INFO : 29 batches submitted to accumulate stats from 1856 documents (64395 virtual)\n",
      "2025-09-10 11:51:22,632 : INFO : 30 batches submitted to accumulate stats from 1920 documents (67132 virtual)\n",
      "2025-09-10 11:51:22,635 : INFO : 31 batches submitted to accumulate stats from 1984 documents (69130 virtual)\n",
      "2025-09-10 11:51:22,637 : INFO : 32 batches submitted to accumulate stats from 2048 documents (71562 virtual)\n",
      "2025-09-10 11:51:22,641 : INFO : 33 batches submitted to accumulate stats from 2112 documents (72821 virtual)\n",
      "2025-09-10 11:51:22,644 : INFO : 34 batches submitted to accumulate stats from 2176 documents (73606 virtual)\n",
      "2025-09-10 11:51:22,646 : INFO : 35 batches submitted to accumulate stats from 2240 documents (73967 virtual)\n",
      "2025-09-10 11:51:22,649 : INFO : 36 batches submitted to accumulate stats from 2304 documents (75970 virtual)\n",
      "2025-09-10 11:51:22,652 : INFO : 37 batches submitted to accumulate stats from 2368 documents (77816 virtual)\n",
      "2025-09-10 11:51:22,655 : INFO : 38 batches submitted to accumulate stats from 2432 documents (78243 virtual)\n",
      "2025-09-10 11:51:22,658 : INFO : 39 batches submitted to accumulate stats from 2496 documents (80113 virtual)\n",
      "2025-09-10 11:51:22,662 : INFO : 40 batches submitted to accumulate stats from 2560 documents (83282 virtual)\n",
      "2025-09-10 11:51:22,666 : INFO : 41 batches submitted to accumulate stats from 2624 documents (84555 virtual)\n",
      "2025-09-10 11:51:22,668 : INFO : 42 batches submitted to accumulate stats from 2688 documents (85006 virtual)\n",
      "2025-09-10 11:51:22,672 : INFO : 43 batches submitted to accumulate stats from 2752 documents (85561 virtual)\n",
      "2025-09-10 11:51:22,679 : INFO : 44 batches submitted to accumulate stats from 2816 documents (87847 virtual)\n",
      "2025-09-10 11:51:22,681 : INFO : 45 batches submitted to accumulate stats from 2880 documents (88294 virtual)\n",
      "2025-09-10 11:51:22,684 : INFO : 46 batches submitted to accumulate stats from 2944 documents (89254 virtual)\n",
      "2025-09-10 11:51:22,694 : INFO : 47 batches submitted to accumulate stats from 3008 documents (90361 virtual)\n",
      "2025-09-10 11:51:22,705 : INFO : 49 batches submitted to accumulate stats from 3136 documents (90494 virtual)\n",
      "2025-09-10 11:51:22,707 : INFO : 50 batches submitted to accumulate stats from 3200 documents (91230 virtual)\n",
      "2025-09-10 11:51:22,711 : INFO : 51 batches submitted to accumulate stats from 3264 documents (92239 virtual)\n",
      "2025-09-10 11:51:22,715 : INFO : 52 batches submitted to accumulate stats from 3328 documents (93051 virtual)\n",
      "2025-09-10 11:51:22,718 : INFO : 53 batches submitted to accumulate stats from 3392 documents (93654 virtual)\n",
      "2025-09-10 11:51:22,732 : INFO : 59 batches submitted to accumulate stats from 3776 documents (90837 virtual)\n",
      "2025-09-10 11:51:22,736 : INFO : 61 batches submitted to accumulate stats from 3904 documents (89441 virtual)\n",
      "2025-09-10 11:51:22,740 : INFO : 63 batches submitted to accumulate stats from 4032 documents (90763 virtual)\n",
      "2025-09-10 11:51:22,746 : INFO : 67 batches submitted to accumulate stats from 4288 documents (89512 virtual)\n",
      "2025-09-10 11:51:22,756 : INFO : 73 batches submitted to accumulate stats from 4672 documents (84248 virtual)\n",
      "2025-09-10 11:51:22,757 : INFO : 74 batches submitted to accumulate stats from 4736 documents (84612 virtual)\n",
      "2025-09-10 11:51:22,759 : INFO : 75 batches submitted to accumulate stats from 4800 documents (85605 virtual)\n",
      "2025-09-10 11:51:22,763 : INFO : 77 batches submitted to accumulate stats from 4928 documents (86303 virtual)\n",
      "2025-09-10 11:51:22,766 : INFO : 78 batches submitted to accumulate stats from 4992 documents (86705 virtual)\n",
      "2025-09-10 11:51:22,771 : INFO : 79 batches submitted to accumulate stats from 5056 documents (87250 virtual)\n",
      "2025-09-10 11:51:22,774 : INFO : 80 batches submitted to accumulate stats from 5120 documents (87252 virtual)\n",
      "2025-09-10 11:51:22,781 : INFO : 83 batches submitted to accumulate stats from 5312 documents (85664 virtual)\n",
      "2025-09-10 11:51:22,784 : INFO : 84 batches submitted to accumulate stats from 5376 documents (85668 virtual)\n",
      "2025-09-10 11:51:22,791 : INFO : 87 batches submitted to accumulate stats from 5568 documents (85126 virtual)\n",
      "2025-09-10 11:51:22,800 : INFO : 91 batches submitted to accumulate stats from 5824 documents (83475 virtual)\n",
      "2025-09-10 11:51:22,805 : INFO : 93 batches submitted to accumulate stats from 5952 documents (83419 virtual)\n",
      "2025-09-10 11:51:22,844 : INFO : 120 batches submitted to accumulate stats from 7680 documents (53159 virtual)\n",
      "2025-09-10 11:51:22,875 : INFO : 144 batches submitted to accumulate stats from 9216 documents (25428 virtual)\n",
      "2025-09-10 11:51:22,888 : INFO : 153 batches submitted to accumulate stats from 9792 documents (18399 virtual)\n",
      "2025-09-10 11:51:22,893 : INFO : 157 batches submitted to accumulate stats from 10048 documents (15860 virtual)\n",
      "2025-09-10 11:51:22,895 : INFO : 158 batches submitted to accumulate stats from 10112 documents (16981 virtual)\n",
      "2025-09-10 11:51:22,899 : INFO : 160 batches submitted to accumulate stats from 10240 documents (14423 virtual)\n",
      "2025-09-10 11:51:22,905 : INFO : 165 batches submitted to accumulate stats from 10560 documents (7614 virtual)\n",
      "2025-09-10 11:51:22,908 : INFO : 167 batches submitted to accumulate stats from 10688 documents (8130 virtual)\n",
      "2025-09-10 11:51:22,913 : INFO : 169 batches submitted to accumulate stats from 10816 documents (9358 virtual)\n",
      "2025-09-10 11:51:22,922 : INFO : 171 batches submitted to accumulate stats from 10944 documents (10141 virtual)\n",
      "2025-09-10 11:51:22,929 : INFO : 177 batches submitted to accumulate stats from 11328 documents (598 virtual)\n",
      "2025-09-10 11:51:22,948 : INFO : 192 batches submitted to accumulate stats from 12288 documents (-20530 virtual)\n",
      "2025-09-10 11:51:22,951 : INFO : 194 batches submitted to accumulate stats from 12416 documents (-21842 virtual)\n",
      "2025-09-10 11:51:22,958 : INFO : 199 batches submitted to accumulate stats from 12736 documents (-21354 virtual)\n",
      "2025-09-10 11:51:22,960 : INFO : 200 batches submitted to accumulate stats from 12800 documents (-18732 virtual)\n",
      "2025-09-10 11:51:22,964 : INFO : 201 batches submitted to accumulate stats from 12864 documents (-16983 virtual)\n",
      "2025-09-10 11:51:22,967 : INFO : 202 batches submitted to accumulate stats from 12928 documents (-10231 virtual)\n",
      "2025-09-10 11:51:22,971 : INFO : 203 batches submitted to accumulate stats from 12992 documents (-2873 virtual)\n",
      "2025-09-10 11:51:22,974 : INFO : 204 batches submitted to accumulate stats from 13056 documents (2095 virtual)\n",
      "2025-09-10 11:51:22,978 : INFO : 206 batches submitted to accumulate stats from 13184 documents (1129 virtual)\n",
      "2025-09-10 11:51:22,985 : INFO : 210 batches submitted to accumulate stats from 13440 documents (33 virtual)\n",
      "2025-09-10 11:51:22,988 : INFO : 211 batches submitted to accumulate stats from 13504 documents (766 virtual)\n",
      "2025-09-10 11:51:22,992 : INFO : 212 batches submitted to accumulate stats from 13568 documents (2126 virtual)\n",
      "2025-09-10 11:51:22,995 : INFO : 213 batches submitted to accumulate stats from 13632 documents (2894 virtual)\n",
      "2025-09-10 11:51:23,001 : INFO : 215 batches submitted to accumulate stats from 13760 documents (4686 virtual)\n",
      "2025-09-10 11:51:23,004 : INFO : 216 batches submitted to accumulate stats from 13824 documents (5912 virtual)\n",
      "2025-09-10 11:51:23,007 : INFO : 217 batches submitted to accumulate stats from 13888 documents (9533 virtual)\n",
      "2025-09-10 11:51:23,016 : INFO : 222 batches submitted to accumulate stats from 14208 documents (9279 virtual)\n",
      "2025-09-10 11:51:23,019 : INFO : 223 batches submitted to accumulate stats from 14272 documents (14537 virtual)\n",
      "2025-09-10 11:51:23,021 : INFO : 224 batches submitted to accumulate stats from 14336 documents (16740 virtual)\n",
      "2025-09-10 11:51:23,023 : INFO : 225 batches submitted to accumulate stats from 14400 documents (17807 virtual)\n",
      "2025-09-10 11:51:23,025 : INFO : 226 batches submitted to accumulate stats from 14464 documents (18030 virtual)\n",
      "2025-09-10 11:51:23,029 : INFO : 228 batches submitted to accumulate stats from 14592 documents (19290 virtual)\n",
      "2025-09-10 11:51:23,033 : INFO : 230 batches submitted to accumulate stats from 14720 documents (21691 virtual)\n",
      "2025-09-10 11:51:23,036 : INFO : 231 batches submitted to accumulate stats from 14784 documents (23450 virtual)\n",
      "2025-09-10 11:51:23,039 : INFO : 232 batches submitted to accumulate stats from 14848 documents (23484 virtual)\n",
      "2025-09-10 11:51:23,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,104 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,104 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,104 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,107 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,107 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,108 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,108 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,109 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,108 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,109 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,107 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,104 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,107 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,108 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,109 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,109 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,109 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,110 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,113 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,115 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,118 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,111 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,118 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,118 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,119 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,123 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,129 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,130 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,133 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,127 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,139 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,136 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,138 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,128 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,139 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,140 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,140 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,143 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,143 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,145 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,146 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,149 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,148 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,152 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,158 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,156 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,157 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,160 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,161 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,163 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,162 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,164 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,172 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,173 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,176 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,178 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,180 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,180 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,184 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,183 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,182 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,184 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,189 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,189 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,188 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,193 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,194 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,198 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,201 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,234 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,243 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,252 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,254 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,257 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,256 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,259 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,263 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,270 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,273 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,278 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,186 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,170 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,284 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,289 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,289 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,290 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,294 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,301 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,112 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,334 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,334 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,115 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,339 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,111 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,384 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,388 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,134 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,233 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,153 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,167 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,176 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,187 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,285 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,294 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,261 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,147 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,140 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,167 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,275 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,238 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,115 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,167 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,243 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,115 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,233 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,131 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,177 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,247 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,138 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,144 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,180 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,126 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,288 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:23,267 : INFO : accumulator serialized\n",
      "2025-09-10 11:51:25,857 : INFO : 127 accumulators retrieved from output queue\n",
      "2025-09-10 11:51:25,901 : INFO : accumulated word occurrence stats for 296776 virtual documents\n",
      "2025-09-10 11:51:26,302 : INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "2025-09-10 11:51:26,303 : INFO : using symmetric eta at 0.2\n",
      "2025-09-10 11:51:26,307 : INFO : using serial LDA version on this node\n",
      "2025-09-10 11:51:26,320 : INFO : running online (multi-pass) LDA training, 5 topics, 10 passes over the supplied corpus of 14833 documents, updating model once every 2000 documents, evaluating perplexity every 14833 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-09-10 11:51:26,321 : INFO : PROGRESS: pass 0, at document #2000/14833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 4 - Coherence: 0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:51:27,528 : INFO : optimized alpha [0.36779004, 0.32840165, 0.3322057, 0.3351854, 0.32000184]\n",
      "2025-09-10 11:51:27,534 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:27,540 : INFO : topic #0 (0.368): 0.012*\"like\" + 0.009*\"one\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"also\" + 0.006*\"really\"\n",
      "2025-09-10 11:51:27,541 : INFO : topic #1 (0.328): 0.010*\"one\" + 0.010*\"like\" + 0.009*\"people\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"might\" + 0.005*\"often\" + 0.005*\"really\"\n",
      "2025-09-10 11:51:27,542 : INFO : topic #2 (0.332): 0.011*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.006*\"would\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"one\" + 0.006*\"also\" + 0.005*\"could\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:27,542 : INFO : topic #3 (0.335): 0.013*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.008*\"might\" + 0.007*\"people\" + 0.007*\"really\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"someone\"\n",
      "2025-09-10 11:51:27,543 : INFO : topic #4 (0.320): 0.015*\"like\" + 0.008*\"people\" + 0.008*\"one\" + 0.008*\"often\" + 0.006*\"might\" + 0.006*\"could\" + 0.005*\"also\" + 0.005*\"time\" + 0.004*\"think\" + 0.004*\"life\"\n",
      "2025-09-10 11:51:27,543 : INFO : topic diff=4.932661, rho=1.000000\n",
      "2025-09-10 11:51:27,544 : INFO : PROGRESS: pass 0, at document #4000/14833\n",
      "2025-09-10 11:51:28,717 : INFO : optimized alpha [0.36545616, 0.26470286, 0.29847637, 0.42676216, 0.18645759]\n",
      "2025-09-10 11:51:28,722 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:28,728 : INFO : topic #0 (0.365): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"one\" + 0.009*\"something\" + 0.009*\"people\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"really\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:51:28,729 : INFO : topic #1 (0.265): 0.010*\"like\" + 0.009*\"one\" + 0.009*\"people\" + 0.007*\"life\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"time\" + 0.005*\"way\" + 0.005*\"might\" + 0.005*\"really\"\n",
      "2025-09-10 11:51:28,730 : INFO : topic #2 (0.298): 0.011*\"think\" + 0.011*\"would\" + 0.011*\"like\" + 0.008*\"something\" + 0.006*\"time\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"people\"\n",
      "2025-09-10 11:51:28,731 : INFO : topic #3 (0.427): 0.015*\"like\" + 0.011*\"one\" + 0.009*\"think\" + 0.008*\"really\" + 0.008*\"time\" + 0.007*\"people\" + 0.006*\"way\" + 0.006*\"even\" + 0.005*\"someone\" + 0.005*\"know\"\n",
      "2025-09-10 11:51:28,731 : INFO : topic #4 (0.186): 0.015*\"like\" + 0.009*\"people\" + 0.009*\"one\" + 0.007*\"often\" + 0.006*\"think\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"might\" + 0.005*\"way\" + 0.004*\"even\"\n",
      "2025-09-10 11:51:28,732 : INFO : topic diff=0.730921, rho=0.707107\n",
      "2025-09-10 11:51:28,733 : INFO : PROGRESS: pass 0, at document #6000/14833\n",
      "2025-09-10 11:51:29,798 : INFO : optimized alpha [0.25346863, 0.15577555, 0.18361144, 0.48862767, 0.12493214]\n",
      "2025-09-10 11:51:29,803 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:29,809 : INFO : topic #0 (0.253): 0.014*\"like\" + 0.013*\"think\" + 0.011*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"also\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"really\" + 0.006*\"someone\"\n",
      "2025-09-10 11:51:29,810 : INFO : topic #1 (0.156): 0.011*\"like\" + 0.009*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"even\" + 0.006*\"life\" + 0.005*\"way\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:51:29,811 : INFO : topic #2 (0.184): 0.013*\"think\" + 0.012*\"like\" + 0.011*\"would\" + 0.008*\"something\" + 0.007*\"even\" + 0.006*\"one\" + 0.005*\"time\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"things\"\n",
      "2025-09-10 11:51:29,812 : INFO : topic #3 (0.489): 0.020*\"like\" + 0.011*\"one\" + 0.010*\"think\" + 0.009*\"really\" + 0.008*\"even\" + 0.008*\"okay\" + 0.008*\"time\" + 0.007*\"know\" + 0.007*\"people\" + 0.006*\"little\"\n",
      "2025-09-10 11:51:29,812 : INFO : topic #4 (0.125): 0.015*\"like\" + 0.011*\"people\" + 0.008*\"one\" + 0.007*\"think\" + 0.006*\"often\" + 0.005*\"know\" + 0.005*\"also\" + 0.005*\"even\" + 0.005*\"way\" + 0.005*\"something\"\n",
      "2025-09-10 11:51:29,813 : INFO : topic diff=0.577047, rho=0.577350\n",
      "2025-09-10 11:51:29,814 : INFO : PROGRESS: pass 0, at document #8000/14833\n",
      "2025-09-10 11:51:30,851 : INFO : optimized alpha [0.2091156, 0.13143942, 0.15786716, 0.43452072, 0.10969676]\n",
      "2025-09-10 11:51:30,856 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:30,862 : INFO : topic #0 (0.209): 0.012*\"think\" + 0.012*\"people\" + 0.012*\"like\" + 0.011*\"something\" + 0.007*\"might\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"even\" + 0.006*\"really\"\n",
      "2025-09-10 11:51:30,863 : INFO : topic #1 (0.131): 0.008*\"people\" + 0.008*\"think\" + 0.008*\"like\" + 0.006*\"one\" + 0.006*\"even\" + 0.006*\"actually\" + 0.006*\"life\" + 0.005*\"way\" + 0.005*\"time\" + 0.004*\"really\"\n",
      "2025-09-10 11:51:30,864 : INFO : topic #2 (0.158): 0.015*\"would\" + 0.011*\"think\" + 0.009*\"like\" + 0.006*\"something\" + 0.006*\"even\" + 0.005*\"people\" + 0.005*\"one\" + 0.005*\"time\" + 0.005*\"might\" + 0.005*\"could\"\n",
      "2025-09-10 11:51:30,865 : INFO : topic #3 (0.435): 0.017*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.007*\"really\" + 0.007*\"people\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"know\" + 0.006*\"something\"\n",
      "2025-09-10 11:51:30,866 : INFO : topic #4 (0.110): 0.011*\"like\" + 0.011*\"people\" + 0.007*\"think\" + 0.006*\"something\" + 0.006*\"one\" + 0.005*\"could\" + 0.005*\"death\" + 0.004*\"actually\" + 0.004*\"way\" + 0.004*\"often\"\n",
      "2025-09-10 11:51:30,866 : INFO : topic diff=0.484519, rho=0.500000\n",
      "2025-09-10 11:51:30,867 : INFO : PROGRESS: pass 0, at document #10000/14833\n",
      "2025-09-10 11:51:31,804 : INFO : optimized alpha [0.18718633, 0.11880828, 0.14102529, 0.42820457, 0.098877095]\n",
      "2025-09-10 11:51:31,809 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:31,815 : INFO : topic #0 (0.187): 0.014*\"people\" + 0.011*\"like\" + 0.011*\"think\" + 0.010*\"something\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"also\"\n",
      "2025-09-10 11:51:31,816 : INFO : topic #1 (0.119): 0.009*\"like\" + 0.008*\"people\" + 0.007*\"think\" + 0.006*\"actually\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"way\" + 0.005*\"language\" + 0.005*\"one\" + 0.004*\"time\"\n",
      "2025-09-10 11:51:31,817 : INFO : topic #2 (0.141): 0.017*\"would\" + 0.009*\"like\" + 0.009*\"think\" + 0.007*\"even\" + 0.006*\"people\" + 0.006*\"something\" + 0.005*\"might\" + 0.005*\"one\" + 0.005*\"time\" + 0.005*\"could\"\n",
      "2025-09-10 11:51:31,817 : INFO : topic #3 (0.428): 0.015*\"like\" + 0.009*\"one\" + 0.007*\"people\" + 0.007*\"even\" + 0.007*\"think\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"way\"\n",
      "2025-09-10 11:51:31,818 : INFO : topic #4 (0.099): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"something\" + 0.007*\"think\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"god\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"way\"\n",
      "2025-09-10 11:51:31,818 : INFO : topic diff=0.400573, rho=0.447214\n",
      "2025-09-10 11:51:31,820 : INFO : PROGRESS: pass 0, at document #12000/14833\n",
      "2025-09-10 11:51:32,702 : INFO : optimized alpha [0.17441852, 0.10818818, 0.12738548, 0.42498624, 0.091346785]\n",
      "2025-09-10 11:51:32,708 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:32,714 : INFO : topic #0 (0.174): 0.015*\"people\" + 0.010*\"like\" + 0.009*\"even\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.006*\"time\" + 0.005*\"often\"\n",
      "2025-09-10 11:51:32,715 : INFO : topic #1 (0.108): 0.008*\"people\" + 0.008*\"like\" + 0.007*\"even\" + 0.005*\"actually\" + 0.005*\"think\" + 0.005*\"life\" + 0.005*\"one\" + 0.005*\"way\" + 0.005*\"success\" + 0.005*\"sleep\"\n",
      "2025-09-10 11:51:32,716 : INFO : topic #2 (0.127): 0.014*\"would\" + 0.008*\"like\" + 0.007*\"even\" + 0.006*\"one\" + 0.006*\"think\" + 0.005*\"people\" + 0.005*\"time\" + 0.004*\"might\" + 0.004*\"could\" + 0.004*\"something\"\n",
      "2025-09-10 11:51:32,717 : INFO : topic #3 (0.425): 0.015*\"like\" + 0.011*\"one\" + 0.008*\"even\" + 0.007*\"people\" + 0.007*\"time\" + 0.006*\"someone\" + 0.005*\"something\" + 0.005*\"way\" + 0.005*\"really\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:32,717 : INFO : topic #4 (0.091): 0.011*\"people\" + 0.009*\"like\" + 0.008*\"one\" + 0.007*\"death\" + 0.007*\"something\" + 0.006*\"even\" + 0.006*\"think\" + 0.005*\"entrepreneurs\" + 0.005*\"often\" + 0.005*\"many\"\n",
      "2025-09-10 11:51:32,718 : INFO : topic diff=0.366387, rho=0.408248\n",
      "2025-09-10 11:51:32,719 : INFO : PROGRESS: pass 0, at document #14000/14833\n",
      "2025-09-10 11:51:33,635 : INFO : optimized alpha [0.17903124, 0.10683768, 0.134745, 0.47220927, 0.094402395]\n",
      "2025-09-10 11:51:33,640 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:33,646 : INFO : topic #0 (0.179): 0.014*\"people\" + 0.009*\"like\" + 0.008*\"time\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"think\" + 0.007*\"something\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"work\"\n",
      "2025-09-10 11:51:33,647 : INFO : topic #1 (0.107): 0.007*\"people\" + 0.007*\"language\" + 0.007*\"like\" + 0.006*\"life\" + 0.005*\"even\" + 0.005*\"sleep\" + 0.005*\"one\" + 0.005*\"way\" + 0.004*\"day\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:33,648 : INFO : topic #2 (0.135): 0.020*\"would\" + 0.007*\"time\" + 0.007*\"like\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"think\" + 0.005*\"people\" + 0.004*\"could\" + 0.004*\"travel\" + 0.003*\"might\"\n",
      "2025-09-10 11:51:33,649 : INFO : topic #3 (0.472): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"back\" + 0.004*\"got\" + 0.004*\"life\"\n",
      "2025-09-10 11:51:33,650 : INFO : topic #4 (0.094): 0.010*\"god\" + 0.010*\"death\" + 0.009*\"people\" + 0.008*\"one\" + 0.007*\"like\" + 0.005*\"many\" + 0.005*\"something\" + 0.005*\"life\" + 0.005*\"even\" + 0.004*\"question\"\n",
      "2025-09-10 11:51:33,650 : INFO : topic diff=0.720163, rho=0.377964\n",
      "2025-09-10 11:51:34,241 : INFO : -8.908 per-word bound, 480.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:34,242 : INFO : PROGRESS: pass 0, at document #14833/14833\n",
      "2025-09-10 11:51:34,593 : INFO : optimized alpha [0.16944829, 0.1053093, 0.13675064, 0.5231362, 0.096649416]\n",
      "2025-09-10 11:51:34,598 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:34,604 : INFO : topic #0 (0.169): 0.015*\"people\" + 0.008*\"like\" + 0.007*\"time\" + 0.007*\"one\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"something\" + 0.005*\"might\" + 0.005*\"work\"\n",
      "2025-09-10 11:51:34,605 : INFO : topic #1 (0.105): 0.009*\"teeth\" + 0.007*\"jobs\" + 0.007*\"people\" + 0.006*\"apple\" + 0.006*\"like\" + 0.006*\"language\" + 0.005*\"even\" + 0.005*\"one\" + 0.004*\"life\" + 0.004*\"company\"\n",
      "2025-09-10 11:51:34,605 : INFO : topic #2 (0.137): 0.020*\"would\" + 0.006*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"could\" + 0.004*\"people\" + 0.004*\"think\" + 0.003*\"students\" + 0.003*\"first\"\n",
      "2025-09-10 11:51:34,606 : INFO : topic #3 (0.523): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"back\"\n",
      "2025-09-10 11:51:34,607 : INFO : topic #4 (0.097): 0.010*\"death\" + 0.009*\"god\" + 0.008*\"people\" + 0.007*\"one\" + 0.006*\"many\" + 0.006*\"like\" + 0.005*\"life\" + 0.004*\"entrepreneurs\" + 0.004*\"even\" + 0.004*\"men\"\n",
      "2025-09-10 11:51:34,607 : INFO : topic diff=0.567807, rho=0.353553\n",
      "2025-09-10 11:51:34,608 : INFO : PROGRESS: pass 1, at document #2000/14833\n",
      "2025-09-10 11:51:35,628 : INFO : optimized alpha [0.19390559, 0.098551065, 0.122489154, 0.36706072, 0.09191441]\n",
      "2025-09-10 11:51:35,634 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:35,640 : INFO : topic #0 (0.194): 0.012*\"people\" + 0.011*\"like\" + 0.009*\"might\" + 0.008*\"think\" + 0.007*\"time\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"someone\" + 0.007*\"even\" + 0.006*\"also\"\n",
      "2025-09-10 11:51:35,641 : INFO : topic #1 (0.099): 0.007*\"language\" + 0.007*\"jobs\" + 0.007*\"like\" + 0.007*\"apple\" + 0.006*\"people\" + 0.006*\"teeth\" + 0.005*\"even\" + 0.005*\"one\" + 0.005*\"life\" + 0.005*\"company\"\n",
      "2025-09-10 11:51:35,642 : INFO : topic #2 (0.122): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"like\" + 0.006*\"could\" + 0.006*\"time\" + 0.006*\"even\" + 0.005*\"think\" + 0.004*\"might\" + 0.004*\"travel\" + 0.003*\"people\"\n",
      "2025-09-10 11:51:35,643 : INFO : topic #3 (0.367): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"people\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:51:35,643 : INFO : topic #4 (0.092): 0.009*\"death\" + 0.008*\"like\" + 0.008*\"people\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"many\" + 0.005*\"life\" + 0.005*\"something\" + 0.005*\"often\" + 0.005*\"idea\"\n",
      "2025-09-10 11:51:35,644 : INFO : topic diff=0.483657, rho=0.325878\n",
      "2025-09-10 11:51:35,644 : INFO : PROGRESS: pass 1, at document #4000/14833\n",
      "2025-09-10 11:51:36,580 : INFO : optimized alpha [0.21316756, 0.09229112, 0.11053234, 0.36313015, 0.08560611]\n",
      "2025-09-10 11:51:36,585 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:36,591 : INFO : topic #0 (0.213): 0.012*\"people\" + 0.012*\"like\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"time\" + 0.007*\"also\" + 0.007*\"even\"\n",
      "2025-09-10 11:51:36,592 : INFO : topic #1 (0.092): 0.008*\"language\" + 0.008*\"like\" + 0.006*\"teeth\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"sleep\" + 0.005*\"jobs\" + 0.005*\"one\" + 0.005*\"life\"\n",
      "2025-09-10 11:51:36,593 : INFO : topic #2 (0.111): 0.023*\"would\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"time\" + 0.004*\"might\" + 0.004*\"travel\" + 0.004*\"people\"\n",
      "2025-09-10 11:51:36,594 : INFO : topic #3 (0.363): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"people\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:36,594 : INFO : topic #4 (0.086): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"one\" + 0.008*\"people\" + 0.007*\"god\" + 0.007*\"think\" + 0.006*\"something\" + 0.005*\"many\" + 0.005*\"life\" + 0.005*\"idea\"\n",
      "2025-09-10 11:51:36,595 : INFO : topic diff=0.289939, rho=0.325878\n",
      "2025-09-10 11:51:36,596 : INFO : PROGRESS: pass 1, at document #6000/14833\n",
      "2025-09-10 11:51:37,423 : INFO : optimized alpha [0.21412179, 0.08586465, 0.09889649, 0.38591558, 0.07796656]\n",
      "2025-09-10 11:51:37,428 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:37,434 : INFO : topic #0 (0.214): 0.014*\"like\" + 0.013*\"people\" + 0.013*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.006*\"really\"\n",
      "2025-09-10 11:51:37,435 : INFO : topic #1 (0.086): 0.008*\"like\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"people\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:37,436 : INFO : topic #2 (0.099): 0.021*\"would\" + 0.008*\"like\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"time\" + 0.004*\"might\" + 0.004*\"people\" + 0.003*\"world\"\n",
      "2025-09-10 11:51:37,437 : INFO : topic #3 (0.386): 0.015*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.007*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"little\" + 0.005*\"back\" + 0.005*\"think\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:37,438 : INFO : topic #4 (0.078): 0.010*\"like\" + 0.008*\"people\" + 0.008*\"death\" + 0.008*\"think\" + 0.008*\"one\" + 0.006*\"god\" + 0.006*\"something\" + 0.006*\"idea\" + 0.005*\"many\" + 0.005*\"life\"\n",
      "2025-09-10 11:51:37,438 : INFO : topic diff=0.250849, rho=0.325878\n",
      "2025-09-10 11:51:37,439 : INFO : PROGRESS: pass 1, at document #8000/14833\n",
      "2025-09-10 11:51:38,256 : INFO : optimized alpha [0.20374559, 0.08258278, 0.09527873, 0.3650585, 0.07537909]\n",
      "2025-09-10 11:51:38,261 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:38,267 : INFO : topic #0 (0.204): 0.014*\"people\" + 0.013*\"think\" + 0.013*\"like\" + 0.009*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"time\"\n",
      "2025-09-10 11:51:38,268 : INFO : topic #1 (0.083): 0.007*\"like\" + 0.007*\"think\" + 0.007*\"jobs\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"people\" + 0.005*\"actually\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:38,269 : INFO : topic #2 (0.095): 0.021*\"would\" + 0.008*\"think\" + 0.007*\"like\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"might\" + 0.004*\"people\" + 0.004*\"travel\"\n",
      "2025-09-10 11:51:38,270 : INFO : topic #3 (0.365): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"know\" + 0.005*\"back\" + 0.005*\"people\" + 0.005*\"think\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:38,271 : INFO : topic #4 (0.075): 0.009*\"like\" + 0.008*\"people\" + 0.008*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"many\" + 0.005*\"idea\" + 0.004*\"life\"\n",
      "2025-09-10 11:51:38,271 : INFO : topic diff=0.275938, rho=0.325878\n",
      "2025-09-10 11:51:38,272 : INFO : PROGRESS: pass 1, at document #10000/14833\n",
      "2025-09-10 11:51:39,053 : INFO : optimized alpha [0.19397146, 0.08075974, 0.091913104, 0.35921064, 0.07352425]\n",
      "2025-09-10 11:51:39,059 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:39,065 : INFO : topic #0 (0.194): 0.015*\"people\" + 0.012*\"like\" + 0.012*\"think\" + 0.009*\"something\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"time\"\n",
      "2025-09-10 11:51:39,066 : INFO : topic #1 (0.081): 0.008*\"like\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"people\" + 0.006*\"actually\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"way\" + 0.005*\"jobs\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:39,067 : INFO : topic #2 (0.092): 0.022*\"would\" + 0.008*\"like\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"one\" + 0.006*\"could\" + 0.006*\"time\" + 0.006*\"might\" + 0.005*\"travel\" + 0.004*\"people\"\n",
      "2025-09-10 11:51:39,067 : INFO : topic #3 (0.359): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"something\" + 0.005*\"people\" + 0.005*\"day\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:39,068 : INFO : topic #4 (0.074): 0.010*\"like\" + 0.008*\"people\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"question\" + 0.005*\"idea\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:39,069 : INFO : topic diff=0.248635, rho=0.325878\n",
      "2025-09-10 11:51:39,070 : INFO : PROGRESS: pass 1, at document #12000/14833\n",
      "2025-09-10 11:51:39,792 : INFO : optimized alpha [0.1879812, 0.07802018, 0.08825634, 0.35229787, 0.07062437]\n",
      "2025-09-10 11:51:39,797 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:39,803 : INFO : topic #0 (0.188): 0.016*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:39,803 : INFO : topic #1 (0.078): 0.008*\"sleep\" + 0.008*\"like\" + 0.006*\"people\" + 0.006*\"even\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.005*\"actually\" + 0.005*\"think\" + 0.005*\"teeth\" + 0.005*\"language\"\n",
      "2025-09-10 11:51:39,804 : INFO : topic #2 (0.088): 0.018*\"would\" + 0.007*\"like\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"think\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"might\" + 0.004*\"people\" + 0.004*\"travel\"\n",
      "2025-09-10 11:51:39,805 : INFO : topic #3 (0.352): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.005*\"people\" + 0.005*\"really\" + 0.005*\"something\" + 0.005*\"way\" + 0.005*\"back\" + 0.004*\"got\"\n",
      "2025-09-10 11:51:39,806 : INFO : topic #4 (0.071): 0.009*\"like\" + 0.009*\"people\" + 0.008*\"death\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"even\" + 0.005*\"idea\" + 0.005*\"many\" + 0.005*\"god\"\n",
      "2025-09-10 11:51:39,806 : INFO : topic diff=0.244895, rho=0.325878\n",
      "2025-09-10 11:51:39,807 : INFO : PROGRESS: pass 1, at document #14000/14833\n",
      "2025-09-10 11:51:40,596 : INFO : optimized alpha [0.18815696, 0.07920585, 0.09435988, 0.38841513, 0.07485628]\n",
      "2025-09-10 11:51:40,601 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:40,607 : INFO : topic #0 (0.188): 0.015*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"something\" + 0.006*\"might\" + 0.006*\"life\"\n",
      "2025-09-10 11:51:40,608 : INFO : topic #1 (0.079): 0.008*\"language\" + 0.008*\"sleep\" + 0.007*\"like\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"jobs\" + 0.004*\"day\" + 0.004*\"business\" + 0.004*\"apple\" + 0.004*\"actually\"\n",
      "2025-09-10 11:51:40,609 : INFO : topic #2 (0.094): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.006*\"travel\" + 0.005*\"could\" + 0.004*\"think\" + 0.004*\"people\" + 0.003*\"might\"\n",
      "2025-09-10 11:51:40,610 : INFO : topic #3 (0.388): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"people\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:51:40,610 : INFO : topic #4 (0.075): 0.010*\"death\" + 0.009*\"god\" + 0.008*\"people\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"many\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:40,611 : INFO : topic diff=0.531333, rho=0.325878\n",
      "2025-09-10 11:51:41,148 : INFO : -8.688 per-word bound, 412.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:41,149 : INFO : PROGRESS: pass 1, at document #14833/14833\n",
      "2025-09-10 11:51:41,447 : INFO : optimized alpha [0.17412908, 0.08016864, 0.098003834, 0.42745572, 0.07716844]\n",
      "2025-09-10 11:51:41,452 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:41,458 : INFO : topic #0 (0.174): 0.016*\"people\" + 0.009*\"like\" + 0.008*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"something\" + 0.006*\"might\" + 0.006*\"life\"\n",
      "2025-09-10 11:51:41,459 : INFO : topic #1 (0.080): 0.009*\"teeth\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"people\" + 0.006*\"money\" + 0.006*\"like\" + 0.006*\"company\" + 0.006*\"business\"\n",
      "2025-09-10 11:51:41,460 : INFO : topic #2 (0.098): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"travel\" + 0.003*\"war\" + 0.003*\"people\"\n",
      "2025-09-10 11:51:41,461 : INFO : topic #3 (0.427): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"people\"\n",
      "2025-09-10 11:51:41,461 : INFO : topic #4 (0.077): 0.010*\"death\" + 0.009*\"god\" + 0.007*\"people\" + 0.007*\"one\" + 0.006*\"many\" + 0.006*\"like\" + 0.006*\"life\" + 0.005*\"question\" + 0.005*\"something\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:41,462 : INFO : topic diff=0.431468, rho=0.325878\n",
      "2025-09-10 11:51:41,463 : INFO : PROGRESS: pass 2, at document #2000/14833\n",
      "2025-09-10 11:51:42,350 : INFO : optimized alpha [0.20287493, 0.076838404, 0.09288592, 0.32883102, 0.0762653]\n",
      "2025-09-10 11:51:42,355 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:42,361 : INFO : topic #0 (0.203): 0.013*\"people\" + 0.012*\"like\" + 0.009*\"might\" + 0.009*\"think\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"often\"\n",
      "2025-09-10 11:51:42,362 : INFO : topic #1 (0.077): 0.008*\"language\" + 0.008*\"jobs\" + 0.008*\"apple\" + 0.006*\"teeth\" + 0.006*\"like\" + 0.006*\"company\" + 0.006*\"business\" + 0.005*\"people\" + 0.005*\"also\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:42,362 : INFO : topic #2 (0.093): 0.021*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"could\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:42,363 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"people\"\n",
      "2025-09-10 11:51:42,364 : INFO : topic #4 (0.076): 0.009*\"death\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"people\" + 0.006*\"life\" + 0.006*\"many\" + 0.006*\"something\" + 0.005*\"idea\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:42,365 : INFO : topic diff=0.378076, rho=0.309841\n",
      "2025-09-10 11:51:42,365 : INFO : PROGRESS: pass 2, at document #4000/14833\n",
      "2025-09-10 11:51:43,202 : INFO : optimized alpha [0.22991541, 0.07377866, 0.08766547, 0.33070615, 0.07382489]\n",
      "2025-09-10 11:51:43,207 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:43,213 : INFO : topic #0 (0.230): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:51:43,214 : INFO : topic #1 (0.074): 0.009*\"language\" + 0.007*\"like\" + 0.007*\"sleep\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"business\" + 0.005*\"apple\" + 0.005*\"also\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:43,214 : INFO : topic #2 (0.088): 0.024*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"could\" + 0.006*\"like\" + 0.006*\"even\" + 0.006*\"travel\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"might\"\n",
      "2025-09-10 11:51:43,215 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:43,216 : INFO : topic #4 (0.074): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"one\" + 0.008*\"think\" + 0.007*\"god\" + 0.007*\"people\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"idea\" + 0.006*\"sound\"\n",
      "2025-09-10 11:51:43,216 : INFO : topic diff=0.234457, rho=0.309841\n",
      "2025-09-10 11:51:43,217 : INFO : PROGRESS: pass 2, at document #6000/14833\n",
      "2025-09-10 11:51:43,995 : INFO : optimized alpha [0.24125269, 0.07114921, 0.08144932, 0.35543028, 0.0690938]\n",
      "2025-09-10 11:51:44,001 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:44,007 : INFO : topic #0 (0.241): 0.014*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:44,008 : INFO : topic #1 (0.071): 0.008*\"like\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"language\" + 0.007*\"apple\" + 0.006*\"teeth\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"people\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:44,008 : INFO : topic #2 (0.081): 0.022*\"would\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"think\" + 0.006*\"time\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"back\"\n",
      "2025-09-10 11:51:44,009 : INFO : topic #3 (0.355): 0.014*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"back\" + 0.005*\"little\" + 0.005*\"day\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:44,010 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.008*\"one\" + 0.007*\"people\" + 0.007*\"something\" + 0.006*\"idea\" + 0.006*\"god\" + 0.005*\"life\" + 0.005*\"many\"\n",
      "2025-09-10 11:51:44,010 : INFO : topic diff=0.212114, rho=0.309841\n",
      "2025-09-10 11:51:44,011 : INFO : PROGRESS: pass 2, at document #8000/14833\n",
      "2025-09-10 11:51:44,773 : INFO : optimized alpha [0.23871423, 0.06983608, 0.08050179, 0.34889638, 0.0684176]\n",
      "2025-09-10 11:51:44,778 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:44,784 : INFO : topic #0 (0.239): 0.014*\"people\" + 0.013*\"like\" + 0.013*\"think\" + 0.009*\"something\" + 0.009*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:44,785 : INFO : topic #1 (0.070): 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"like\" + 0.006*\"think\" + 0.006*\"sleep\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"actually\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:44,786 : INFO : topic #2 (0.081): 0.022*\"would\" + 0.007*\"think\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"could\" + 0.006*\"even\" + 0.005*\"travel\" + 0.005*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:44,787 : INFO : topic #3 (0.349): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:51:44,788 : INFO : topic #4 (0.068): 0.010*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"people\" + 0.006*\"sound\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"many\" + 0.005*\"idea\"\n",
      "2025-09-10 11:51:44,788 : INFO : topic diff=0.232558, rho=0.309841\n",
      "2025-09-10 11:51:44,789 : INFO : PROGRESS: pass 2, at document #10000/14833\n",
      "2025-09-10 11:51:45,532 : INFO : optimized alpha [0.23037761, 0.06904769, 0.07897786, 0.34721097, 0.068238795]\n",
      "2025-09-10 11:51:45,538 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:45,544 : INFO : topic #0 (0.230): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:45,545 : INFO : topic #1 (0.069): 0.008*\"sleep\" + 0.007*\"language\" + 0.007*\"like\" + 0.006*\"actually\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"money\" + 0.005*\"even\" + 0.005*\"think\" + 0.005*\"apple\"\n",
      "2025-09-10 11:51:45,546 : INFO : topic #2 (0.079): 0.023*\"would\" + 0.007*\"like\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"could\" + 0.006*\"think\" + 0.006*\"one\" + 0.006*\"travel\" + 0.005*\"might\" + 0.004*\"back\"\n",
      "2025-09-10 11:51:45,547 : INFO : topic #3 (0.347): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"something\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:45,548 : INFO : topic #4 (0.068): 0.010*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"people\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"sound\" + 0.006*\"question\" + 0.006*\"idea\"\n",
      "2025-09-10 11:51:45,548 : INFO : topic diff=0.209619, rho=0.309841\n",
      "2025-09-10 11:51:45,549 : INFO : PROGRESS: pass 2, at document #12000/14833\n",
      "2025-09-10 11:51:46,245 : INFO : optimized alpha [0.2242864, 0.06797259, 0.07716861, 0.34276122, 0.06606145]\n",
      "2025-09-10 11:51:46,251 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:46,257 : INFO : topic #0 (0.224): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:46,258 : INFO : topic #1 (0.068): 0.008*\"sleep\" + 0.007*\"like\" + 0.006*\"people\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"even\" + 0.005*\"money\" + 0.005*\"actually\" + 0.005*\"business\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:46,259 : INFO : topic #2 (0.077): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"like\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"could\" + 0.005*\"think\" + 0.005*\"might\" + 0.004*\"travel\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:46,259 : INFO : topic #3 (0.343): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"people\" + 0.005*\"something\" + 0.004*\"day\"\n",
      "2025-09-10 11:51:46,260 : INFO : topic #4 (0.066): 0.010*\"like\" + 0.008*\"something\" + 0.008*\"death\" + 0.008*\"people\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"idea\" + 0.006*\"even\" + 0.005*\"love\" + 0.005*\"god\"\n",
      "2025-09-10 11:51:46,261 : INFO : topic diff=0.214527, rho=0.309841\n",
      "2025-09-10 11:51:46,261 : INFO : PROGRESS: pass 2, at document #14000/14833\n",
      "2025-09-10 11:51:47,015 : INFO : optimized alpha [0.22219437, 0.069911234, 0.08293999, 0.3769722, 0.07039661]\n",
      "2025-09-10 11:51:47,021 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:47,027 : INFO : topic #0 (0.222): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"time\" + 0.006*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:51:47,028 : INFO : topic #1 (0.070): 0.008*\"sleep\" + 0.008*\"language\" + 0.006*\"like\" + 0.006*\"money\" + 0.006*\"people\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"even\" + 0.005*\"apple\" + 0.005*\"company\"\n",
      "2025-09-10 11:51:47,029 : INFO : topic #2 (0.083): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.004*\"think\" + 0.004*\"back\" + 0.003*\"speed\"\n",
      "2025-09-10 11:51:47,030 : INFO : topic #3 (0.377): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:51:47,031 : INFO : topic #4 (0.070): 0.010*\"death\" + 0.009*\"god\" + 0.008*\"like\" + 0.007*\"people\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.005*\"question\" + 0.005*\"many\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:47,031 : INFO : topic diff=0.467352, rho=0.309841\n",
      "2025-09-10 11:51:47,563 : INFO : -8.608 per-word bound, 390.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:47,563 : INFO : PROGRESS: pass 2, at document #14833/14833\n",
      "2025-09-10 11:51:47,843 : INFO : optimized alpha [0.20149632, 0.071505986, 0.08667601, 0.41166762, 0.07269351]\n",
      "2025-09-10 11:51:47,849 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:47,855 : INFO : topic #0 (0.201): 0.017*\"people\" + 0.010*\"like\" + 0.008*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"something\" + 0.006*\"time\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:51:47,856 : INFO : topic #1 (0.072): 0.009*\"teeth\" + 0.008*\"jobs\" + 0.007*\"money\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:51:47,857 : INFO : topic #2 (0.087): 0.021*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:51:47,858 : INFO : topic #3 (0.412): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\"\n",
      "2025-09-10 11:51:47,859 : INFO : topic #4 (0.073): 0.010*\"death\" + 0.009*\"god\" + 0.007*\"one\" + 0.007*\"people\" + 0.007*\"like\" + 0.006*\"life\" + 0.006*\"many\" + 0.005*\"something\" + 0.005*\"question\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:47,859 : INFO : topic diff=0.376803, rho=0.309841\n",
      "2025-09-10 11:51:47,860 : INFO : PROGRESS: pass 3, at document #2000/14833\n",
      "2025-09-10 11:51:48,665 : INFO : optimized alpha [0.23334225, 0.0693036, 0.08398169, 0.32630348, 0.07276973]\n",
      "2025-09-10 11:51:48,670 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:48,676 : INFO : topic #0 (0.233): 0.013*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:51:48,677 : INFO : topic #1 (0.069): 0.008*\"jobs\" + 0.008*\"language\" + 0.008*\"apple\" + 0.007*\"teeth\" + 0.006*\"company\" + 0.006*\"business\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"sleep\" + 0.005*\"people\"\n",
      "2025-09-10 11:51:48,677 : INFO : topic #2 (0.084): 0.021*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.005*\"even\" + 0.005*\"like\" + 0.005*\"travel\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"might\"\n",
      "2025-09-10 11:51:48,678 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"got\"\n",
      "2025-09-10 11:51:48,679 : INFO : topic #4 (0.073): 0.009*\"death\" + 0.009*\"like\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"people\" + 0.006*\"many\" + 0.006*\"idea\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:48,679 : INFO : topic diff=0.327830, rho=0.295960\n",
      "2025-09-10 11:51:48,680 : INFO : PROGRESS: pass 3, at document #4000/14833\n",
      "2025-09-10 11:51:49,463 : INFO : optimized alpha [0.26447996, 0.067574956, 0.080614574, 0.32896304, 0.071516424]\n",
      "2025-09-10 11:51:49,468 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:49,474 : INFO : topic #0 (0.264): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.007*\"also\"\n",
      "2025-09-10 11:51:49,475 : INFO : topic #1 (0.068): 0.009*\"language\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"money\" + 0.006*\"apple\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:49,476 : INFO : topic #2 (0.081): 0.024*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"back\" + 0.004*\"might\"\n",
      "2025-09-10 11:51:49,477 : INFO : topic #3 (0.329): 0.012*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"would\"\n",
      "2025-09-10 11:51:49,478 : INFO : topic #4 (0.072): 0.011*\"like\" + 0.008*\"death\" + 0.008*\"one\" + 0.008*\"think\" + 0.007*\"god\" + 0.007*\"something\" + 0.006*\"sound\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"idea\"\n",
      "2025-09-10 11:51:49,478 : INFO : topic diff=0.204991, rho=0.295960\n",
      "2025-09-10 11:51:49,479 : INFO : PROGRESS: pass 3, at document #6000/14833\n",
      "2025-09-10 11:51:50,227 : INFO : optimized alpha [0.2810453, 0.0661176, 0.07584759, 0.35384098, 0.06765831]\n",
      "2025-09-10 11:51:50,232 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:50,238 : INFO : topic #0 (0.281): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:50,239 : INFO : topic #1 (0.066): 0.007*\"sleep\" + 0.007*\"like\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"money\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"even\"\n",
      "2025-09-10 11:51:50,240 : INFO : topic #2 (0.076): 0.023*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"like\" + 0.006*\"even\" + 0.006*\"think\" + 0.005*\"travel\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:51:50,241 : INFO : topic #3 (0.354): 0.014*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"back\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:50,242 : INFO : topic #4 (0.068): 0.011*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"idea\" + 0.006*\"people\" + 0.006*\"god\" + 0.006*\"life\" + 0.005*\"sound\"\n",
      "2025-09-10 11:51:50,242 : INFO : topic diff=0.190666, rho=0.295960\n",
      "2025-09-10 11:51:50,243 : INFO : PROGRESS: pass 3, at document #8000/14833\n",
      "2025-09-10 11:51:50,967 : INFO : optimized alpha [0.28032106, 0.065744884, 0.075513676, 0.35121047, 0.06755929]\n",
      "2025-09-10 11:51:50,973 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:50,979 : INFO : topic #0 (0.280): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"something\" + 0.009*\"someone\" + 0.008*\"might\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:50,980 : INFO : topic #1 (0.066): 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"money\" + 0.006*\"like\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"business\" + 0.005*\"also\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:50,981 : INFO : topic #2 (0.076): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"like\" + 0.006*\"could\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"travel\" + 0.005*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:50,982 : INFO : topic #3 (0.351): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.006*\"really\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:51:50,983 : INFO : topic #4 (0.068): 0.010*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"people\" + 0.006*\"sound\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"idea\" + 0.005*\"question\"\n",
      "2025-09-10 11:51:50,983 : INFO : topic diff=0.206141, rho=0.295960\n",
      "2025-09-10 11:51:50,984 : INFO : PROGRESS: pass 3, at document #10000/14833\n",
      "2025-09-10 11:51:51,688 : INFO : optimized alpha [0.27101928, 0.065472946, 0.074540235, 0.34973842, 0.06782767]\n",
      "2025-09-10 11:51:51,693 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:51,699 : INFO : topic #0 (0.271): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:51:51,700 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.007*\"language\" + 0.007*\"money\" + 0.007*\"like\" + 0.006*\"actually\" + 0.006*\"jobs\" + 0.006*\"people\" + 0.005*\"apple\" + 0.005*\"business\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:51,701 : INFO : topic #2 (0.075): 0.024*\"would\" + 0.009*\"time\" + 0.007*\"like\" + 0.007*\"even\" + 0.007*\"could\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"think\" + 0.005*\"might\" + 0.005*\"back\"\n",
      "2025-09-10 11:51:51,701 : INFO : topic #3 (0.350): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"get\" + 0.004*\"years\"\n",
      "2025-09-10 11:51:51,702 : INFO : topic #4 (0.068): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"people\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"sound\" + 0.006*\"question\" + 0.006*\"idea\"\n",
      "2025-09-10 11:51:51,703 : INFO : topic diff=0.188090, rho=0.295960\n",
      "2025-09-10 11:51:51,703 : INFO : PROGRESS: pass 3, at document #12000/14833\n",
      "2025-09-10 11:51:52,370 : INFO : optimized alpha [0.2642738, 0.06494192, 0.07321192, 0.34634638, 0.06598575]\n",
      "2025-09-10 11:51:52,375 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:52,381 : INFO : topic #0 (0.264): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:52,382 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.007*\"money\" + 0.007*\"like\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"business\" + 0.005*\"actually\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:52,383 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"like\" + 0.006*\"could\" + 0.005*\"travel\" + 0.005*\"think\" + 0.005*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:52,384 : INFO : topic #3 (0.346): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"made\" + 0.004*\"still\"\n",
      "2025-09-10 11:51:52,385 : INFO : topic #4 (0.066): 0.010*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"idea\" + 0.006*\"even\" + 0.006*\"love\" + 0.006*\"sound\"\n",
      "2025-09-10 11:51:52,385 : INFO : topic diff=0.196046, rho=0.295960\n",
      "2025-09-10 11:51:52,386 : INFO : PROGRESS: pass 3, at document #14000/14833\n",
      "2025-09-10 11:51:53,118 : INFO : optimized alpha [0.25930673, 0.06717366, 0.078603, 0.37946746, 0.07029208]\n",
      "2025-09-10 11:51:53,123 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:53,129 : INFO : topic #0 (0.259): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:51:53,130 : INFO : topic #1 (0.067): 0.008*\"sleep\" + 0.008*\"language\" + 0.007*\"money\" + 0.006*\"like\" + 0.005*\"people\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"even\" + 0.005*\"apple\"\n",
      "2025-09-10 11:51:53,131 : INFO : topic #2 (0.079): 0.023*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"like\" + 0.004*\"back\" + 0.004*\"space\" + 0.004*\"think\"\n",
      "2025-09-10 11:51:53,132 : INFO : topic #3 (0.379): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"years\"\n",
      "2025-09-10 11:51:53,133 : INFO : topic #4 (0.070): 0.009*\"death\" + 0.009*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"question\" + 0.005*\"think\" + 0.005*\"sound\"\n",
      "2025-09-10 11:51:53,133 : INFO : topic diff=0.429255, rho=0.295960\n",
      "2025-09-10 11:51:53,649 : INFO : -8.561 per-word bound, 377.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:53,650 : INFO : PROGRESS: pass 3, at document #14833/14833\n",
      "2025-09-10 11:51:53,927 : INFO : optimized alpha [0.23011382, 0.0689318, 0.082297914, 0.41087356, 0.07227609]\n",
      "2025-09-10 11:51:53,933 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:53,940 : INFO : topic #0 (0.230): 0.017*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:51:53,941 : INFO : topic #1 (0.069): 0.009*\"teeth\" + 0.008*\"money\" + 0.008*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:51:53,941 : INFO : topic #2 (0.082): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:51:53,942 : INFO : topic #3 (0.411): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:53,943 : INFO : topic #4 (0.072): 0.010*\"death\" + 0.009*\"god\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"many\" + 0.005*\"something\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:53,943 : INFO : topic diff=0.339122, rho=0.295960\n",
      "2025-09-10 11:51:53,944 : INFO : PROGRESS: pass 4, at document #2000/14833\n",
      "2025-09-10 11:51:54,724 : INFO : optimized alpha [0.26344007, 0.06720693, 0.08043459, 0.32992738, 0.07277891]\n",
      "2025-09-10 11:51:54,729 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:54,735 : INFO : topic #0 (0.263): 0.013*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:51:54,737 : INFO : topic #1 (0.067): 0.008*\"jobs\" + 0.008*\"language\" + 0.008*\"apple\" + 0.007*\"money\" + 0.007*\"teeth\" + 0.007*\"company\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:54,738 : INFO : topic #2 (0.080): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:51:54,738 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:51:54,739 : INFO : topic #4 (0.073): 0.009*\"like\" + 0.009*\"death\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"idea\" + 0.006*\"think\" + 0.005*\"many\"\n",
      "2025-09-10 11:51:54,740 : INFO : topic diff=0.299159, rho=0.283792\n",
      "2025-09-10 11:51:54,741 : INFO : PROGRESS: pass 4, at document #4000/14833\n",
      "2025-09-10 11:51:55,504 : INFO : optimized alpha [0.29719874, 0.06601286, 0.07770894, 0.33270445, 0.07205056]\n",
      "2025-09-10 11:51:55,509 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:55,516 : INFO : topic #0 (0.297): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:51:55,517 : INFO : topic #1 (0.066): 0.009*\"language\" + 0.007*\"sleep\" + 0.007*\"money\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"apple\" + 0.005*\"also\"\n",
      "2025-09-10 11:51:55,518 : INFO : topic #2 (0.078): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"might\"\n",
      "2025-09-10 11:51:55,519 : INFO : topic #3 (0.333): 0.012*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"really\"\n",
      "2025-09-10 11:51:55,520 : INFO : topic #4 (0.072): 0.011*\"like\" + 0.008*\"death\" + 0.008*\"think\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"sound\" + 0.006*\"people\"\n",
      "2025-09-10 11:51:55,520 : INFO : topic diff=0.186586, rho=0.283792\n",
      "2025-09-10 11:51:55,521 : INFO : PROGRESS: pass 4, at document #6000/14833\n",
      "2025-09-10 11:51:56,250 : INFO : optimized alpha [0.3169811, 0.06502596, 0.073584855, 0.3567916, 0.06858124]\n",
      "2025-09-10 11:51:56,255 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:56,261 : INFO : topic #0 (0.317): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:51:56,263 : INFO : topic #1 (0.065): 0.007*\"sleep\" + 0.007*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"like\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:56,263 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"even\" + 0.006*\"like\" + 0.006*\"travel\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:51:56,264 : INFO : topic #3 (0.357): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.006*\"really\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:56,265 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"idea\" + 0.006*\"god\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:51:56,265 : INFO : topic diff=0.176607, rho=0.283792\n",
      "2025-09-10 11:51:56,266 : INFO : PROGRESS: pass 4, at document #8000/14833\n",
      "2025-09-10 11:51:56,972 : INFO : optimized alpha [0.31706256, 0.06502643, 0.07345536, 0.35556373, 0.06873396]\n",
      "2025-09-10 11:51:56,977 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:56,983 : INFO : topic #0 (0.317): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.009*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:51:56,984 : INFO : topic #1 (0.065): 0.008*\"jobs\" + 0.007*\"money\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"like\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"also\" + 0.005*\"actually\"\n",
      "2025-09-10 11:51:56,985 : INFO : topic #2 (0.073): 0.022*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"like\" + 0.006*\"even\" + 0.006*\"think\" + 0.006*\"travel\" + 0.004*\"might\" + 0.004*\"back\"\n",
      "2025-09-10 11:51:56,986 : INFO : topic #3 (0.356): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:56,987 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"sound\" + 0.006*\"people\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"idea\" + 0.005*\"question\"\n",
      "2025-09-10 11:51:56,987 : INFO : topic diff=0.188793, rho=0.283792\n",
      "2025-09-10 11:51:56,988 : INFO : PROGRESS: pass 4, at document #10000/14833\n",
      "2025-09-10 11:51:57,674 : INFO : optimized alpha [0.30592644, 0.06501467, 0.072437406, 0.35396546, 0.06923829]\n",
      "2025-09-10 11:51:57,680 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:57,685 : INFO : topic #0 (0.306): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:51:57,687 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.008*\"money\" + 0.007*\"language\" + 0.006*\"like\" + 0.006*\"actually\" + 0.006*\"jobs\" + 0.005*\"business\" + 0.005*\"people\" + 0.005*\"apple\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:51:57,688 : INFO : topic #2 (0.072): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"could\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"travel\" + 0.006*\"think\" + 0.005*\"might\" + 0.005*\"back\"\n",
      "2025-09-10 11:51:57,689 : INFO : topic #3 (0.354): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:57,689 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"people\" + 0.006*\"sound\" + 0.006*\"idea\" + 0.006*\"question\"\n",
      "2025-09-10 11:51:57,690 : INFO : topic diff=0.174133, rho=0.283792\n",
      "2025-09-10 11:51:57,690 : INFO : PROGRESS: pass 4, at document #12000/14833\n",
      "2025-09-10 11:51:58,340 : INFO : optimized alpha [0.29763746, 0.06468502, 0.07131097, 0.35063097, 0.06741039]\n",
      "2025-09-10 11:51:58,345 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:58,351 : INFO : topic #0 (0.298): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:51:58,352 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.007*\"money\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"even\" + 0.005*\"people\" + 0.005*\"teeth\" + 0.005*\"actually\"\n",
      "2025-09-10 11:51:58,353 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"travel\" + 0.005*\"think\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:51:58,354 : INFO : topic #3 (0.351): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"really\" + 0.005*\"still\"\n",
      "2025-09-10 11:51:58,355 : INFO : topic #4 (0.067): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"idea\" + 0.006*\"even\" + 0.006*\"love\" + 0.006*\"sound\"\n",
      "2025-09-10 11:51:58,355 : INFO : topic diff=0.183402, rho=0.283792\n",
      "2025-09-10 11:51:58,356 : INFO : PROGRESS: pass 4, at document #14000/14833\n",
      "2025-09-10 11:51:59,075 : INFO : optimized alpha [0.29066798, 0.066997506, 0.0763991, 0.38310337, 0.07167771]\n",
      "2025-09-10 11:51:59,080 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:59,086 : INFO : topic #0 (0.291): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:51:59,087 : INFO : topic #1 (0.067): 0.008*\"sleep\" + 0.008*\"money\" + 0.008*\"language\" + 0.006*\"like\" + 0.005*\"business\" + 0.005*\"people\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:59,088 : INFO : topic #2 (0.076): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"space\" + 0.004*\"speed\"\n",
      "2025-09-10 11:51:59,089 : INFO : topic #3 (0.383): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:51:59,090 : INFO : topic #4 (0.072): 0.009*\"death\" + 0.009*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"question\" + 0.005*\"think\" + 0.005*\"sound\"\n",
      "2025-09-10 11:51:59,090 : INFO : topic diff=0.400759, rho=0.283792\n",
      "2025-09-10 11:51:59,604 : INFO : -8.533 per-word bound, 370.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:51:59,604 : INFO : PROGRESS: pass 4, at document #14833/14833\n",
      "2025-09-10 11:51:59,878 : INFO : optimized alpha [0.25397176, 0.06882147, 0.07981633, 0.41183022, 0.07342243]\n",
      "2025-09-10 11:51:59,883 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:51:59,889 : INFO : topic #0 (0.254): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:51:59,890 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:51:59,890 : INFO : topic #2 (0.080): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:51:59,891 : INFO : topic #3 (0.412): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:51:59,892 : INFO : topic #4 (0.073): 0.009*\"death\" + 0.008*\"god\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"something\" + 0.006*\"many\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:51:59,892 : INFO : topic diff=0.311632, rho=0.283792\n",
      "2025-09-10 11:51:59,893 : INFO : PROGRESS: pass 5, at document #2000/14833\n",
      "2025-09-10 11:52:00,646 : INFO : optimized alpha [0.2880056, 0.06726809, 0.07838332, 0.334005, 0.07412417]\n",
      "2025-09-10 11:52:00,652 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:00,657 : INFO : topic #0 (0.288): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:00,658 : INFO : topic #1 (0.067): 0.008*\"jobs\" + 0.008*\"money\" + 0.008*\"apple\" + 0.008*\"language\" + 0.007*\"company\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:00,659 : INFO : topic #2 (0.078): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:00,660 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:00,661 : INFO : topic #4 (0.074): 0.010*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"idea\" + 0.006*\"think\" + 0.006*\"people\" + 0.005*\"many\"\n",
      "2025-09-10 11:52:00,661 : INFO : topic diff=0.279044, rho=0.273011\n",
      "2025-09-10 11:52:00,662 : INFO : PROGRESS: pass 5, at document #4000/14833\n",
      "2025-09-10 11:52:01,406 : INFO : optimized alpha [0.32319972, 0.066220276, 0.07616434, 0.3365073, 0.073610395]\n",
      "2025-09-10 11:52:01,412 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:01,418 : INFO : topic #0 (0.323): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:01,418 : INFO : topic #1 (0.066): 0.008*\"language\" + 0.007*\"money\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:01,419 : INFO : topic #2 (0.076): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:01,420 : INFO : topic #3 (0.337): 0.012*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:01,421 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"sound\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:01,421 : INFO : topic diff=0.174004, rho=0.273011\n",
      "2025-09-10 11:52:01,422 : INFO : PROGRESS: pass 5, at document #6000/14833\n",
      "2025-09-10 11:52:02,149 : INFO : optimized alpha [0.3445014, 0.065402634, 0.072433144, 0.35931608, 0.0703142]\n",
      "2025-09-10 11:52:02,154 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:02,160 : INFO : topic #0 (0.345): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:52:02,161 : INFO : topic #1 (0.065): 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"like\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:02,162 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"even\" + 0.006*\"travel\" + 0.006*\"like\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:02,162 : INFO : topic #3 (0.359): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:52:02,163 : INFO : topic #4 (0.070): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"death\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"idea\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"people\" + 0.005*\"maybe\"\n",
      "2025-09-10 11:52:02,163 : INFO : topic diff=0.166406, rho=0.273011\n",
      "2025-09-10 11:52:02,164 : INFO : PROGRESS: pass 5, at document #8000/14833\n",
      "2025-09-10 11:52:02,868 : INFO : optimized alpha [0.34447286, 0.065435484, 0.07237052, 0.35874906, 0.0706615]\n",
      "2025-09-10 11:52:02,873 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:02,878 : INFO : topic #0 (0.344): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:52:02,879 : INFO : topic #1 (0.065): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"company\"\n",
      "2025-09-10 11:52:02,880 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"think\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:02,881 : INFO : topic #3 (0.359): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:02,882 : INFO : topic #4 (0.071): 0.011*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"sound\" + 0.006*\"one\" + 0.006*\"people\" + 0.006*\"god\" + 0.005*\"idea\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:02,882 : INFO : topic diff=0.175311, rho=0.273011\n",
      "2025-09-10 11:52:02,883 : INFO : PROGRESS: pass 5, at document #10000/14833\n",
      "2025-09-10 11:52:03,576 : INFO : optimized alpha [0.3316537, 0.06535768, 0.07140115, 0.3565209, 0.07129088]\n",
      "2025-09-10 11:52:03,581 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:03,587 : INFO : topic #0 (0.332): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:52:03,587 : INFO : topic #1 (0.065): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.006*\"actually\" + 0.005*\"business\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:03,588 : INFO : topic #2 (0.071): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"could\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"might\" + 0.005*\"back\"\n",
      "2025-09-10 11:52:03,589 : INFO : topic #3 (0.357): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:03,590 : INFO : topic #4 (0.071): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"sound\" + 0.006*\"maybe\" + 0.006*\"question\"\n",
      "2025-09-10 11:52:03,590 : INFO : topic diff=0.163184, rho=0.273011\n",
      "2025-09-10 11:52:03,591 : INFO : PROGRESS: pass 5, at document #12000/14833\n",
      "2025-09-10 11:52:04,248 : INFO : optimized alpha [0.32267728, 0.06510542, 0.070355505, 0.35342687, 0.06942106]\n",
      "2025-09-10 11:52:04,254 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:04,260 : INFO : topic #0 (0.323): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:04,261 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.008*\"money\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"people\" + 0.005*\"actually\"\n",
      "2025-09-10 11:52:04,262 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"travel\" + 0.004*\"think\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:04,263 : INFO : topic #3 (0.353): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\" + 0.005*\"years\"\n",
      "2025-09-10 11:52:04,264 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"people\" + 0.006*\"idea\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"love\"\n",
      "2025-09-10 11:52:04,264 : INFO : topic diff=0.174016, rho=0.273011\n",
      "2025-09-10 11:52:04,265 : INFO : PROGRESS: pass 5, at document #14000/14833\n",
      "2025-09-10 11:52:04,981 : INFO : optimized alpha [0.31412688, 0.067473404, 0.07514426, 0.38523778, 0.073621176]\n",
      "2025-09-10 11:52:04,987 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:04,993 : INFO : topic #0 (0.314): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:52:04,994 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"jobs\" + 0.005*\"people\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:04,995 : INFO : topic #2 (0.075): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"space\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:52:04,996 : INFO : topic #3 (0.385): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:04,996 : INFO : topic #4 (0.074): 0.009*\"death\" + 0.009*\"like\" + 0.008*\"god\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"black\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:04,997 : INFO : topic diff=0.376788, rho=0.273011\n",
      "2025-09-10 11:52:05,519 : INFO : -8.513 per-word bound, 365.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:05,519 : INFO : PROGRESS: pass 5, at document #14833/14833\n",
      "2025-09-10 11:52:05,796 : INFO : optimized alpha [0.27352253, 0.06928526, 0.07842942, 0.41236153, 0.074965924]\n",
      "2025-09-10 11:52:05,800 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:05,806 : INFO : topic #0 (0.274): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:52:05,807 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:52:05,808 : INFO : topic #2 (0.078): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:52:05,809 : INFO : topic #3 (0.412): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:05,809 : INFO : topic #4 (0.075): 0.009*\"death\" + 0.008*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"people\" + 0.006*\"something\" + 0.005*\"many\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:05,810 : INFO : topic diff=0.292157, rho=0.273011\n",
      "2025-09-10 11:52:05,810 : INFO : PROGRESS: pass 6, at document #2000/14833\n",
      "2025-09-10 11:52:06,574 : INFO : optimized alpha [0.30753177, 0.067769475, 0.07732223, 0.3375794, 0.07571033]\n",
      "2025-09-10 11:52:06,581 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:06,588 : INFO : topic #0 (0.308): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:06,589 : INFO : topic #1 (0.068): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"company\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:06,590 : INFO : topic #2 (0.077): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:06,591 : INFO : topic #3 (0.338): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:06,592 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"think\" + 0.006*\"idea\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:06,593 : INFO : topic diff=0.262859, rho=0.263372\n",
      "2025-09-10 11:52:06,593 : INFO : PROGRESS: pass 6, at document #4000/14833\n",
      "2025-09-10 11:52:07,338 : INFO : optimized alpha [0.3434658, 0.06675167, 0.07527453, 0.33972496, 0.07530257]\n",
      "2025-09-10 11:52:07,343 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:07,349 : INFO : topic #0 (0.343): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:07,350 : INFO : topic #1 (0.067): 0.008*\"language\" + 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:07,351 : INFO : topic #2 (0.075): 0.024*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.007*\"could\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:07,352 : INFO : topic #3 (0.340): 0.012*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"know\"\n",
      "2025-09-10 11:52:07,353 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"god\" + 0.006*\"idea\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:07,353 : INFO : topic diff=0.164240, rho=0.263372\n",
      "2025-09-10 11:52:07,354 : INFO : PROGRESS: pass 6, at document #6000/14833\n",
      "2025-09-10 11:52:08,076 : INFO : optimized alpha [0.3663487, 0.066092595, 0.071817115, 0.36166987, 0.07205387]\n",
      "2025-09-10 11:52:08,081 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:08,087 : INFO : topic #0 (0.366): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:52:08,088 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"like\" + 0.006*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:08,089 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:08,089 : INFO : topic #3 (0.362): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:52:08,090 : INFO : topic #4 (0.072): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"idea\" + 0.006*\"god\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.005*\"sound\"\n",
      "2025-09-10 11:52:08,091 : INFO : topic diff=0.158125, rho=0.263372\n",
      "2025-09-10 11:52:08,091 : INFO : PROGRESS: pass 6, at document #8000/14833\n",
      "2025-09-10 11:52:08,799 : INFO : optimized alpha [0.3654638, 0.066150144, 0.07184405, 0.36124134, 0.072422]\n",
      "2025-09-10 11:52:08,804 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:08,810 : INFO : topic #0 (0.365): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:52:08,811 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:08,812 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:52:08,812 : INFO : topic #3 (0.361): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:08,813 : INFO : topic #4 (0.072): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"god\" + 0.006*\"people\" + 0.005*\"idea\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:08,814 : INFO : topic diff=0.164573, rho=0.263372\n",
      "2025-09-10 11:52:08,814 : INFO : PROGRESS: pass 6, at document #10000/14833\n",
      "2025-09-10 11:52:09,505 : INFO : optimized alpha [0.3509968, 0.0661131, 0.070841946, 0.3585045, 0.07300389]\n",
      "2025-09-10 11:52:09,510 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:09,516 : INFO : topic #0 (0.351): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:52:09,516 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"actually\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:52:09,517 : INFO : topic #2 (0.071): 0.024*\"would\" + 0.011*\"time\" + 0.007*\"could\" + 0.007*\"travel\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"might\" + 0.005*\"back\"\n",
      "2025-09-10 11:52:09,518 : INFO : topic #3 (0.359): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:09,519 : INFO : topic #4 (0.073): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"sound\" + 0.006*\"question\"\n",
      "2025-09-10 11:52:09,519 : INFO : topic diff=0.154375, rho=0.263372\n",
      "2025-09-10 11:52:09,520 : INFO : PROGRESS: pass 6, at document #12000/14833\n",
      "2025-09-10 11:52:10,173 : INFO : optimized alpha [0.34170422, 0.065868825, 0.06981419, 0.3555163, 0.07109891]\n",
      "2025-09-10 11:52:10,178 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:10,184 : INFO : topic #0 (0.342): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:10,185 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"like\" + 0.006*\"business\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"people\" + 0.005*\"actually\"\n",
      "2025-09-10 11:52:10,186 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"students\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:10,186 : INFO : topic #3 (0.356): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\" + 0.005*\"years\"\n",
      "2025-09-10 11:52:10,187 : INFO : topic #4 (0.071): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"idea\" + 0.006*\"people\" + 0.006*\"maybe\" + 0.005*\"sound\"\n",
      "2025-09-10 11:52:10,188 : INFO : topic diff=0.166253, rho=0.263372\n",
      "2025-09-10 11:52:10,188 : INFO : PROGRESS: pass 6, at document #14000/14833\n",
      "2025-09-10 11:52:10,906 : INFO : optimized alpha [0.33158645, 0.06827342, 0.0744175, 0.38644773, 0.075170375]\n",
      "2025-09-10 11:52:10,911 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:10,917 : INFO : topic #0 (0.332): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:10,918 : INFO : topic #1 (0.068): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"people\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:10,918 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"space\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:10,919 : INFO : topic #3 (0.386): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:10,920 : INFO : topic #4 (0.075): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:10,920 : INFO : topic diff=0.356951, rho=0.263372\n",
      "2025-09-10 11:52:11,450 : INFO : -8.498 per-word bound, 361.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:11,450 : INFO : PROGRESS: pass 6, at document #14833/14833\n",
      "2025-09-10 11:52:11,728 : INFO : optimized alpha [0.28851798, 0.07009224, 0.07755827, 0.4121349, 0.076304615]\n",
      "2025-09-10 11:52:11,733 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:11,738 : INFO : topic #0 (0.289): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:11,739 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:52:11,740 : INFO : topic #2 (0.078): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:52:11,741 : INFO : topic #3 (0.412): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:11,742 : INFO : topic #4 (0.076): 0.009*\"death\" + 0.008*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"people\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:52:11,742 : INFO : topic diff=0.276287, rho=0.263372\n",
      "2025-09-10 11:52:11,743 : INFO : PROGRESS: pass 7, at document #2000/14833\n",
      "2025-09-10 11:52:12,491 : INFO : optimized alpha [0.3223411, 0.06861589, 0.07662225, 0.340276, 0.07707367]\n",
      "2025-09-10 11:52:12,496 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:12,502 : INFO : topic #0 (0.322): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:12,503 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"company\" + 0.007*\"business\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:12,504 : INFO : topic #2 (0.077): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:12,505 : INFO : topic #3 (0.340): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:12,506 : INFO : topic #4 (0.077): 0.010*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"think\" + 0.006*\"idea\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:12,507 : INFO : topic diff=0.249725, rho=0.254687\n",
      "2025-09-10 11:52:12,507 : INFO : PROGRESS: pass 7, at document #4000/14833\n",
      "2025-09-10 11:52:13,234 : INFO : optimized alpha [0.3580677, 0.06763431, 0.07473594, 0.34168708, 0.07673995]\n",
      "2025-09-10 11:52:13,239 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:13,245 : INFO : topic #0 (0.358): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:13,246 : INFO : topic #1 (0.068): 0.008*\"language\" + 0.008*\"money\" + 0.007*\"sleep\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:13,248 : INFO : topic #2 (0.075): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.007*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:13,248 : INFO : topic #3 (0.342): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:52:13,249 : INFO : topic #4 (0.077): 0.012*\"like\" + 0.008*\"think\" + 0.008*\"death\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:13,250 : INFO : topic diff=0.156156, rho=0.254687\n",
      "2025-09-10 11:52:13,250 : INFO : PROGRESS: pass 7, at document #6000/14833\n",
      "2025-09-10 11:52:13,963 : INFO : optimized alpha [0.38165456, 0.06698612, 0.07149884, 0.36274916, 0.07345985]\n",
      "2025-09-10 11:52:13,968 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:13,974 : INFO : topic #0 (0.382): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:52:13,975 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:13,976 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.004*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:13,977 : INFO : topic #3 (0.363): 0.012*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:13,978 : INFO : topic #4 (0.073): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"one\" + 0.006*\"idea\" + 0.006*\"god\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 11:52:13,978 : INFO : topic diff=0.151174, rho=0.254687\n",
      "2025-09-10 11:52:13,979 : INFO : PROGRESS: pass 7, at document #8000/14833\n",
      "2025-09-10 11:52:14,679 : INFO : optimized alpha [0.38085708, 0.06706625, 0.07153999, 0.3624599, 0.07386083]\n",
      "2025-09-10 11:52:14,684 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:14,690 : INFO : topic #0 (0.381): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:14,691 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:14,692 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:52:14,693 : INFO : topic #3 (0.362): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:14,694 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"god\" + 0.006*\"people\" + 0.005*\"really\" + 0.005*\"idea\"\n",
      "2025-09-10 11:52:14,694 : INFO : topic diff=0.155598, rho=0.254687\n",
      "2025-09-10 11:52:14,695 : INFO : PROGRESS: pass 7, at document #10000/14833\n",
      "2025-09-10 11:52:15,378 : INFO : optimized alpha [0.36634433, 0.06707532, 0.07059032, 0.3594606, 0.07440889]\n",
      "2025-09-10 11:52:15,383 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:15,389 : INFO : topic #0 (0.366): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:52:15,390 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"actually\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:52:15,391 : INFO : topic #2 (0.071): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"travel\" + 0.007*\"could\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"back\" + 0.005*\"might\"\n",
      "2025-09-10 11:52:15,392 : INFO : topic #3 (0.359): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:15,393 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"question\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:15,393 : INFO : topic diff=0.146995, rho=0.254687\n",
      "2025-09-10 11:52:15,394 : INFO : PROGRESS: pass 7, at document #12000/14833\n",
      "2025-09-10 11:52:16,045 : INFO : optimized alpha [0.35689202, 0.06680998, 0.069604315, 0.3566134, 0.07249383]\n",
      "2025-09-10 11:52:16,050 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:16,057 : INFO : topic #0 (0.357): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:16,058 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"language\"\n",
      "2025-09-10 11:52:16,059 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"could\" + 0.006*\"like\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"might\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:16,059 : INFO : topic #3 (0.357): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:16,060 : INFO : topic #4 (0.072): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"people\" + 0.005*\"way\"\n",
      "2025-09-10 11:52:16,061 : INFO : topic diff=0.159818, rho=0.254687\n",
      "2025-09-10 11:52:16,061 : INFO : PROGRESS: pass 7, at document #14000/14833\n",
      "2025-09-10 11:52:16,770 : INFO : optimized alpha [0.3456189, 0.06915918, 0.073996395, 0.3866143, 0.07644659]\n",
      "2025-09-10 11:52:16,775 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:16,782 : INFO : topic #0 (0.346): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:16,782 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:16,783 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"space\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:16,784 : INFO : topic #3 (0.387): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:16,785 : INFO : topic #4 (0.076): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"people\" + 0.006*\"question\"\n",
      "2025-09-10 11:52:16,785 : INFO : topic diff=0.340153, rho=0.254687\n",
      "2025-09-10 11:52:17,312 : INFO : -8.486 per-word bound, 358.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:17,313 : INFO : PROGRESS: pass 7, at document #14833/14833\n",
      "2025-09-10 11:52:17,596 : INFO : optimized alpha [0.30099273, 0.0709419, 0.07704181, 0.41134283, 0.07740291]\n",
      "2025-09-10 11:52:17,601 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:17,607 : INFO : topic #0 (0.301): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:17,608 : INFO : topic #1 (0.071): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:52:17,609 : INFO : topic #2 (0.077): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:52:17,610 : INFO : topic #3 (0.411): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:17,610 : INFO : topic #4 (0.077): 0.009*\"death\" + 0.008*\"like\" + 0.008*\"god\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"people\" + 0.005*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:17,611 : INFO : topic diff=0.262871, rho=0.254687\n",
      "2025-09-10 11:52:17,611 : INFO : PROGRESS: pass 8, at document #2000/14833\n",
      "2025-09-10 11:52:18,344 : INFO : optimized alpha [0.3343922, 0.06946978, 0.076226465, 0.34229675, 0.078165606]\n",
      "2025-09-10 11:52:18,350 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:18,356 : INFO : topic #0 (0.334): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:18,357 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:52:18,358 : INFO : topic #2 (0.076): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:52:18,359 : INFO : topic #3 (0.342): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:18,360 : INFO : topic #4 (0.078): 0.010*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"think\" + 0.006*\"idea\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:18,360 : INFO : topic diff=0.238540, rho=0.246808\n",
      "2025-09-10 11:52:18,361 : INFO : PROGRESS: pass 8, at document #4000/14833\n",
      "2025-09-10 11:52:19,083 : INFO : optimized alpha [0.36988682, 0.06851519, 0.07451348, 0.3432593, 0.077820934]\n",
      "2025-09-10 11:52:19,088 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:19,094 : INFO : topic #0 (0.370): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:19,095 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.008*\"language\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:19,096 : INFO : topic #2 (0.075): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.007*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"students\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:19,097 : INFO : topic #3 (0.343): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:52:19,098 : INFO : topic #4 (0.078): 0.012*\"like\" + 0.008*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.008*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:19,098 : INFO : topic diff=0.149308, rho=0.246808\n",
      "2025-09-10 11:52:19,099 : INFO : PROGRESS: pass 8, at document #6000/14833\n",
      "2025-09-10 11:52:19,808 : INFO : optimized alpha [0.39402837, 0.06789485, 0.07143061, 0.36350363, 0.074579574]\n",
      "2025-09-10 11:52:19,814 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:19,820 : INFO : topic #0 (0.394): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:52:19,821 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:19,822 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:19,823 : INFO : topic #3 (0.364): 0.012*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:19,823 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 11:52:19,824 : INFO : topic diff=0.145313, rho=0.246808\n",
      "2025-09-10 11:52:19,825 : INFO : PROGRESS: pass 8, at document #8000/14833\n",
      "2025-09-10 11:52:20,516 : INFO : optimized alpha [0.39338708, 0.06798184, 0.07143553, 0.363239, 0.07498156]\n",
      "2025-09-10 11:52:20,522 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:20,528 : INFO : topic #0 (0.393): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:20,529 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:20,530 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:52:20,531 : INFO : topic #3 (0.363): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:20,532 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"god\" + 0.005*\"people\" + 0.005*\"really\" + 0.005*\"maybe\"\n",
      "2025-09-10 11:52:20,532 : INFO : topic diff=0.148249, rho=0.246808\n",
      "2025-09-10 11:52:20,533 : INFO : PROGRESS: pass 8, at document #10000/14833\n",
      "2025-09-10 11:52:21,208 : INFO : optimized alpha [0.37888026, 0.068000495, 0.07049417, 0.3600069, 0.0754687]\n",
      "2025-09-10 11:52:21,213 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:21,219 : INFO : topic #0 (0.379): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:52:21,220 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"actually\" + 0.005*\"like\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:52:21,221 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"even\" + 0.006*\"like\" + 0.005*\"back\" + 0.005*\"think\" + 0.005*\"might\"\n",
      "2025-09-10 11:52:21,222 : INFO : topic #3 (0.360): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:21,223 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"question\" + 0.006*\"sound\" + 0.006*\"idea\"\n",
      "2025-09-10 11:52:21,224 : INFO : topic diff=0.140832, rho=0.246808\n",
      "2025-09-10 11:52:21,224 : INFO : PROGRESS: pass 8, at document #12000/14833\n",
      "2025-09-10 11:52:21,865 : INFO : optimized alpha [0.36944544, 0.06770068, 0.06949139, 0.35745847, 0.07352455]\n",
      "2025-09-10 11:52:21,870 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:21,876 : INFO : topic #0 (0.369): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.006*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:21,877 : INFO : topic #1 (0.068): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"actually\"\n",
      "2025-09-10 11:52:21,878 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"might\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:21,878 : INFO : topic #3 (0.357): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:21,879 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.008*\"think\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"people\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:21,880 : INFO : topic diff=0.154072, rho=0.246808\n",
      "2025-09-10 11:52:21,880 : INFO : PROGRESS: pass 8, at document #14000/14833\n",
      "2025-09-10 11:52:22,586 : INFO : optimized alpha [0.35732967, 0.07004683, 0.07372245, 0.38664842, 0.07734966]\n",
      "2025-09-10 11:52:22,592 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:22,599 : INFO : topic #0 (0.357): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:22,600 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:52:22,601 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"speed\"\n",
      "2025-09-10 11:52:22,601 : INFO : topic #3 (0.387): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:22,602 : INFO : topic #4 (0.077): 0.010*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"question\"\n",
      "2025-09-10 11:52:22,602 : INFO : topic diff=0.325337, rho=0.246808\n",
      "2025-09-10 11:52:23,149 : INFO : -8.476 per-word bound, 356.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:23,150 : INFO : PROGRESS: pass 8, at document #14833/14833\n",
      "2025-09-10 11:52:23,430 : INFO : optimized alpha [0.31170982, 0.07188026, 0.07651581, 0.41063067, 0.07808641]\n",
      "2025-09-10 11:52:23,435 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:23,441 : INFO : topic #0 (0.312): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:23,442 : INFO : topic #1 (0.072): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:52:23,442 : INFO : topic #2 (0.077): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:52:23,443 : INFO : topic #3 (0.411): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:23,444 : INFO : topic #4 (0.078): 0.009*\"death\" + 0.009*\"like\" + 0.008*\"god\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"life\" + 0.006*\"something\" + 0.005*\"people\" + 0.005*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:23,444 : INFO : topic diff=0.251093, rho=0.246808\n",
      "2025-09-10 11:52:23,445 : INFO : PROGRESS: pass 9, at document #2000/14833\n",
      "2025-09-10 11:52:24,170 : INFO : optimized alpha [0.34460118, 0.070369326, 0.07582369, 0.3438273, 0.07884539]\n",
      "2025-09-10 11:52:24,175 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:24,181 : INFO : topic #0 (0.345): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:24,182 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:52:24,183 : INFO : topic #2 (0.076): 0.021*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"students\"\n",
      "2025-09-10 11:52:24,183 : INFO : topic #3 (0.344): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:24,184 : INFO : topic #4 (0.079): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"think\" + 0.006*\"idea\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:24,185 : INFO : topic diff=0.228959, rho=0.239618\n",
      "2025-09-10 11:52:24,185 : INFO : PROGRESS: pass 9, at document #4000/14833\n",
      "2025-09-10 11:52:24,900 : INFO : optimized alpha [0.37979704, 0.06940908, 0.07420794, 0.3441958, 0.07852667]\n",
      "2025-09-10 11:52:24,906 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:24,911 : INFO : topic #0 (0.380): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:24,912 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.008*\"language\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:52:24,913 : INFO : topic #2 (0.074): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.007*\"could\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"human\"\n",
      "2025-09-10 11:52:24,914 : INFO : topic #3 (0.344): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:52:24,915 : INFO : topic #4 (0.079): 0.012*\"like\" + 0.008*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"idea\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:24,915 : INFO : topic diff=0.143394, rho=0.239618\n",
      "2025-09-10 11:52:24,916 : INFO : PROGRESS: pass 9, at document #6000/14833\n",
      "2025-09-10 11:52:25,608 : INFO : optimized alpha [0.4041775, 0.0688172, 0.07125014, 0.36363998, 0.07530744]\n",
      "2025-09-10 11:52:25,614 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:25,619 : INFO : topic #0 (0.404): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:52:25,620 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:25,621 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:25,622 : INFO : topic #3 (0.364): 0.012*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:25,623 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 11:52:25,623 : INFO : topic diff=0.140123, rho=0.239618\n",
      "2025-09-10 11:52:25,624 : INFO : PROGRESS: pass 9, at document #8000/14833\n",
      "2025-09-10 11:52:26,303 : INFO : optimized alpha [0.4036191, 0.06888738, 0.07123278, 0.3634821, 0.0757136]\n",
      "2025-09-10 11:52:26,308 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:26,315 : INFO : topic #0 (0.404): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:52:26,316 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:52:26,317 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:52:26,317 : INFO : topic #3 (0.363): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:26,318 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"god\" + 0.005*\"really\" + 0.005*\"maybe\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:26,319 : INFO : topic diff=0.142005, rho=0.239618\n",
      "2025-09-10 11:52:26,319 : INFO : PROGRESS: pass 9, at document #10000/14833\n",
      "2025-09-10 11:52:26,988 : INFO : optimized alpha [0.3892004, 0.06888444, 0.07033783, 0.36020654, 0.07613383]\n",
      "2025-09-10 11:52:26,994 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:26,999 : INFO : topic #0 (0.389): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:52:27,000 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"actually\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:52:27,001 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.010*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.007*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.005*\"might\" + 0.005*\"think\"\n",
      "2025-09-10 11:52:27,002 : INFO : topic #3 (0.360): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:27,003 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"question\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:52:27,003 : INFO : topic diff=0.135615, rho=0.239618\n",
      "2025-09-10 11:52:27,003 : INFO : PROGRESS: pass 9, at document #12000/14833\n",
      "2025-09-10 11:52:27,643 : INFO : optimized alpha [0.37962452, 0.06859991, 0.06935212, 0.35785782, 0.0741887]\n",
      "2025-09-10 11:52:27,648 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:27,654 : INFO : topic #0 (0.380): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"things\" + 0.006*\"might\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:27,654 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"language\" + 0.005*\"actually\"\n",
      "2025-09-10 11:52:27,655 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"might\" + 0.004*\"back\"\n",
      "2025-09-10 11:52:27,656 : INFO : topic #3 (0.358): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:52:27,657 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.008*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"people\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:27,657 : INFO : topic diff=0.148751, rho=0.239618\n",
      "2025-09-10 11:52:27,658 : INFO : PROGRESS: pass 9, at document #14000/14833\n",
      "2025-09-10 11:52:28,359 : INFO : optimized alpha [0.3667664, 0.0708828, 0.0733704, 0.38619456, 0.07789495]\n",
      "2025-09-10 11:52:28,365 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:28,371 : INFO : topic #0 (0.367): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:28,372 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"apple\" + 0.005*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:52:28,373 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.012*\"time\" + 0.007*\"travel\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"speed\"\n",
      "2025-09-10 11:52:28,374 : INFO : topic #3 (0.386): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:28,374 : INFO : topic #4 (0.078): 0.010*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"question\" + 0.006*\"people\"\n",
      "2025-09-10 11:52:28,375 : INFO : topic diff=0.312173, rho=0.239618\n",
      "2025-09-10 11:52:28,897 : INFO : -8.468 per-word bound, 354.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:28,898 : INFO : PROGRESS: pass 9, at document #14833/14833\n",
      "2025-09-10 11:52:29,175 : INFO : optimized alpha [0.3207244, 0.07264623, 0.076071054, 0.4095526, 0.078490615]\n",
      "2025-09-10 11:52:29,181 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:29,187 : INFO : topic #0 (0.321): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:52:29,188 : INFO : topic #1 (0.073): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:52:29,189 : INFO : topic #2 (0.076): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:52:29,190 : INFO : topic #3 (0.410): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:29,191 : INFO : topic #4 (0.078): 0.009*\"death\" + 0.009*\"like\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"think\" + 0.005*\"people\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:29,191 : INFO : topic diff=0.240865, rho=0.239618\n",
      "2025-09-10 11:52:29,192 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=34546, num_topics=5, decay=0.5, chunksize=2000> in 62.87s', 'datetime': '2025-09-10T11:52:29.191977', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-09-10 11:52:29,195 : INFO : using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows\n",
      "2025-09-10 11:52:33,481 : INFO : 1 batches submitted to accumulate stats from 64 documents (2312 virtual)\n",
      "2025-09-10 11:52:33,487 : INFO : 2 batches submitted to accumulate stats from 128 documents (4776 virtual)\n",
      "2025-09-10 11:52:33,490 : INFO : 3 batches submitted to accumulate stats from 192 documents (8552 virtual)\n",
      "2025-09-10 11:52:33,493 : INFO : 4 batches submitted to accumulate stats from 256 documents (12770 virtual)\n",
      "2025-09-10 11:52:33,497 : INFO : 5 batches submitted to accumulate stats from 320 documents (15636 virtual)\n",
      "2025-09-10 11:52:33,500 : INFO : 6 batches submitted to accumulate stats from 384 documents (19247 virtual)\n",
      "2025-09-10 11:52:33,505 : INFO : 7 batches submitted to accumulate stats from 448 documents (20248 virtual)\n",
      "2025-09-10 11:52:33,508 : INFO : 8 batches submitted to accumulate stats from 512 documents (21583 virtual)\n",
      "2025-09-10 11:52:33,511 : INFO : 9 batches submitted to accumulate stats from 576 documents (24028 virtual)\n",
      "2025-09-10 11:52:33,518 : INFO : 10 batches submitted to accumulate stats from 640 documents (24977 virtual)\n",
      "2025-09-10 11:52:33,520 : INFO : 11 batches submitted to accumulate stats from 704 documents (26982 virtual)\n",
      "2025-09-10 11:52:33,523 : INFO : 12 batches submitted to accumulate stats from 768 documents (30059 virtual)\n",
      "2025-09-10 11:52:33,525 : INFO : 13 batches submitted to accumulate stats from 832 documents (30786 virtual)\n",
      "2025-09-10 11:52:33,528 : INFO : 14 batches submitted to accumulate stats from 896 documents (34101 virtual)\n",
      "2025-09-10 11:52:33,531 : INFO : 15 batches submitted to accumulate stats from 960 documents (34664 virtual)\n",
      "2025-09-10 11:52:33,534 : INFO : 16 batches submitted to accumulate stats from 1024 documents (38291 virtual)\n",
      "2025-09-10 11:52:33,536 : INFO : 17 batches submitted to accumulate stats from 1088 documents (40666 virtual)\n",
      "2025-09-10 11:52:33,539 : INFO : 18 batches submitted to accumulate stats from 1152 documents (42061 virtual)\n",
      "2025-09-10 11:52:33,541 : INFO : 19 batches submitted to accumulate stats from 1216 documents (44289 virtual)\n",
      "2025-09-10 11:52:33,544 : INFO : 20 batches submitted to accumulate stats from 1280 documents (47210 virtual)\n",
      "2025-09-10 11:52:33,547 : INFO : 21 batches submitted to accumulate stats from 1344 documents (48243 virtual)\n",
      "2025-09-10 11:52:33,549 : INFO : 22 batches submitted to accumulate stats from 1408 documents (48909 virtual)\n",
      "2025-09-10 11:52:33,551 : INFO : 23 batches submitted to accumulate stats from 1472 documents (49511 virtual)\n",
      "2025-09-10 11:52:33,554 : INFO : 24 batches submitted to accumulate stats from 1536 documents (51165 virtual)\n",
      "2025-09-10 11:52:33,557 : INFO : 25 batches submitted to accumulate stats from 1600 documents (51700 virtual)\n",
      "2025-09-10 11:52:33,560 : INFO : 26 batches submitted to accumulate stats from 1664 documents (54911 virtual)\n",
      "2025-09-10 11:52:33,563 : INFO : 27 batches submitted to accumulate stats from 1728 documents (58811 virtual)\n",
      "2025-09-10 11:52:33,566 : INFO : 28 batches submitted to accumulate stats from 1792 documents (62711 virtual)\n",
      "2025-09-10 11:52:33,569 : INFO : 29 batches submitted to accumulate stats from 1856 documents (64395 virtual)\n",
      "2025-09-10 11:52:33,572 : INFO : 30 batches submitted to accumulate stats from 1920 documents (67132 virtual)\n",
      "2025-09-10 11:52:33,574 : INFO : 31 batches submitted to accumulate stats from 1984 documents (69130 virtual)\n",
      "2025-09-10 11:52:33,576 : INFO : 32 batches submitted to accumulate stats from 2048 documents (71562 virtual)\n",
      "2025-09-10 11:52:33,578 : INFO : 33 batches submitted to accumulate stats from 2112 documents (72821 virtual)\n",
      "2025-09-10 11:52:33,581 : INFO : 34 batches submitted to accumulate stats from 2176 documents (73606 virtual)\n",
      "2025-09-10 11:52:33,584 : INFO : 35 batches submitted to accumulate stats from 2240 documents (73967 virtual)\n",
      "2025-09-10 11:52:33,589 : INFO : 36 batches submitted to accumulate stats from 2304 documents (75970 virtual)\n",
      "2025-09-10 11:52:33,605 : INFO : 37 batches submitted to accumulate stats from 2368 documents (77816 virtual)\n",
      "2025-09-10 11:52:33,610 : INFO : 38 batches submitted to accumulate stats from 2432 documents (78243 virtual)\n",
      "2025-09-10 11:52:33,616 : INFO : 39 batches submitted to accumulate stats from 2496 documents (80113 virtual)\n",
      "2025-09-10 11:52:33,620 : INFO : 40 batches submitted to accumulate stats from 2560 documents (83282 virtual)\n",
      "2025-09-10 11:52:33,624 : INFO : 41 batches submitted to accumulate stats from 2624 documents (84555 virtual)\n",
      "2025-09-10 11:52:33,629 : INFO : 42 batches submitted to accumulate stats from 2688 documents (85006 virtual)\n",
      "2025-09-10 11:52:33,633 : INFO : 43 batches submitted to accumulate stats from 2752 documents (85561 virtual)\n",
      "2025-09-10 11:52:33,636 : INFO : 44 batches submitted to accumulate stats from 2816 documents (87847 virtual)\n",
      "2025-09-10 11:52:33,638 : INFO : 45 batches submitted to accumulate stats from 2880 documents (88294 virtual)\n",
      "2025-09-10 11:52:33,642 : INFO : 46 batches submitted to accumulate stats from 2944 documents (89254 virtual)\n",
      "2025-09-10 11:52:33,644 : INFO : 47 batches submitted to accumulate stats from 3008 documents (90361 virtual)\n",
      "2025-09-10 11:52:33,648 : INFO : 49 batches submitted to accumulate stats from 3136 documents (90494 virtual)\n",
      "2025-09-10 11:52:33,651 : INFO : 50 batches submitted to accumulate stats from 3200 documents (91230 virtual)\n",
      "2025-09-10 11:52:33,654 : INFO : 51 batches submitted to accumulate stats from 3264 documents (92239 virtual)\n",
      "2025-09-10 11:52:33,657 : INFO : 52 batches submitted to accumulate stats from 3328 documents (93051 virtual)\n",
      "2025-09-10 11:52:33,659 : INFO : 53 batches submitted to accumulate stats from 3392 documents (93654 virtual)\n",
      "2025-09-10 11:52:33,670 : INFO : 59 batches submitted to accumulate stats from 3776 documents (90837 virtual)\n",
      "2025-09-10 11:52:33,674 : INFO : 61 batches submitted to accumulate stats from 3904 documents (89441 virtual)\n",
      "2025-09-10 11:52:33,677 : INFO : 63 batches submitted to accumulate stats from 4032 documents (90763 virtual)\n",
      "2025-09-10 11:52:33,686 : INFO : 67 batches submitted to accumulate stats from 4288 documents (89512 virtual)\n",
      "2025-09-10 11:52:33,696 : INFO : 73 batches submitted to accumulate stats from 4672 documents (84248 virtual)\n",
      "2025-09-10 11:52:33,699 : INFO : 74 batches submitted to accumulate stats from 4736 documents (84612 virtual)\n",
      "2025-09-10 11:52:33,701 : INFO : 75 batches submitted to accumulate stats from 4800 documents (85605 virtual)\n",
      "2025-09-10 11:52:33,706 : INFO : 77 batches submitted to accumulate stats from 4928 documents (86303 virtual)\n",
      "2025-09-10 11:52:33,708 : INFO : 78 batches submitted to accumulate stats from 4992 documents (86705 virtual)\n",
      "2025-09-10 11:52:33,710 : INFO : 79 batches submitted to accumulate stats from 5056 documents (87250 virtual)\n",
      "2025-09-10 11:52:33,712 : INFO : 80 batches submitted to accumulate stats from 5120 documents (87252 virtual)\n",
      "2025-09-10 11:52:33,719 : INFO : 83 batches submitted to accumulate stats from 5312 documents (85664 virtual)\n",
      "2025-09-10 11:52:33,722 : INFO : 84 batches submitted to accumulate stats from 5376 documents (85668 virtual)\n",
      "2025-09-10 11:52:33,727 : INFO : 87 batches submitted to accumulate stats from 5568 documents (85126 virtual)\n",
      "2025-09-10 11:52:33,734 : INFO : 91 batches submitted to accumulate stats from 5824 documents (83475 virtual)\n",
      "2025-09-10 11:52:33,738 : INFO : 93 batches submitted to accumulate stats from 5952 documents (83419 virtual)\n",
      "2025-09-10 11:52:33,785 : INFO : 120 batches submitted to accumulate stats from 7680 documents (53159 virtual)\n",
      "2025-09-10 11:52:33,817 : INFO : 144 batches submitted to accumulate stats from 9216 documents (25428 virtual)\n",
      "2025-09-10 11:52:33,830 : INFO : 153 batches submitted to accumulate stats from 9792 documents (18399 virtual)\n",
      "2025-09-10 11:52:33,837 : INFO : 157 batches submitted to accumulate stats from 10048 documents (15860 virtual)\n",
      "2025-09-10 11:52:33,839 : INFO : 158 batches submitted to accumulate stats from 10112 documents (16981 virtual)\n",
      "2025-09-10 11:52:33,842 : INFO : 160 batches submitted to accumulate stats from 10240 documents (14423 virtual)\n",
      "2025-09-10 11:52:33,849 : INFO : 165 batches submitted to accumulate stats from 10560 documents (7614 virtual)\n",
      "2025-09-10 11:52:33,854 : INFO : 167 batches submitted to accumulate stats from 10688 documents (8130 virtual)\n",
      "2025-09-10 11:52:33,860 : INFO : 169 batches submitted to accumulate stats from 10816 documents (9358 virtual)\n",
      "2025-09-10 11:52:33,866 : INFO : 171 batches submitted to accumulate stats from 10944 documents (10141 virtual)\n",
      "2025-09-10 11:52:33,878 : INFO : 177 batches submitted to accumulate stats from 11328 documents (598 virtual)\n",
      "2025-09-10 11:52:33,898 : INFO : 192 batches submitted to accumulate stats from 12288 documents (-20530 virtual)\n",
      "2025-09-10 11:52:33,902 : INFO : 194 batches submitted to accumulate stats from 12416 documents (-21842 virtual)\n",
      "2025-09-10 11:52:33,909 : INFO : 199 batches submitted to accumulate stats from 12736 documents (-21354 virtual)\n",
      "2025-09-10 11:52:33,912 : INFO : 200 batches submitted to accumulate stats from 12800 documents (-18732 virtual)\n",
      "2025-09-10 11:52:33,914 : INFO : 201 batches submitted to accumulate stats from 12864 documents (-16983 virtual)\n",
      "2025-09-10 11:52:33,917 : INFO : 202 batches submitted to accumulate stats from 12928 documents (-10231 virtual)\n",
      "2025-09-10 11:52:33,922 : INFO : 203 batches submitted to accumulate stats from 12992 documents (-2873 virtual)\n",
      "2025-09-10 11:52:33,925 : INFO : 204 batches submitted to accumulate stats from 13056 documents (2095 virtual)\n",
      "2025-09-10 11:52:33,928 : INFO : 206 batches submitted to accumulate stats from 13184 documents (1129 virtual)\n",
      "2025-09-10 11:52:33,934 : INFO : 210 batches submitted to accumulate stats from 13440 documents (33 virtual)\n",
      "2025-09-10 11:52:33,938 : INFO : 211 batches submitted to accumulate stats from 13504 documents (766 virtual)\n",
      "2025-09-10 11:52:33,941 : INFO : 212 batches submitted to accumulate stats from 13568 documents (2126 virtual)\n",
      "2025-09-10 11:52:33,944 : INFO : 213 batches submitted to accumulate stats from 13632 documents (2894 virtual)\n",
      "2025-09-10 11:52:33,947 : INFO : 215 batches submitted to accumulate stats from 13760 documents (4686 virtual)\n",
      "2025-09-10 11:52:33,949 : INFO : 216 batches submitted to accumulate stats from 13824 documents (5912 virtual)\n",
      "2025-09-10 11:52:33,953 : INFO : 217 batches submitted to accumulate stats from 13888 documents (9533 virtual)\n",
      "2025-09-10 11:52:33,967 : INFO : 222 batches submitted to accumulate stats from 14208 documents (9279 virtual)\n",
      "2025-09-10 11:52:33,971 : INFO : 223 batches submitted to accumulate stats from 14272 documents (14537 virtual)\n",
      "2025-09-10 11:52:33,974 : INFO : 224 batches submitted to accumulate stats from 14336 documents (16740 virtual)\n",
      "2025-09-10 11:52:33,977 : INFO : 225 batches submitted to accumulate stats from 14400 documents (17807 virtual)\n",
      "2025-09-10 11:52:33,979 : INFO : 226 batches submitted to accumulate stats from 14464 documents (18030 virtual)\n",
      "2025-09-10 11:52:33,985 : INFO : 228 batches submitted to accumulate stats from 14592 documents (19290 virtual)\n",
      "2025-09-10 11:52:33,991 : INFO : 230 batches submitted to accumulate stats from 14720 documents (21691 virtual)\n",
      "2025-09-10 11:52:33,995 : INFO : 231 batches submitted to accumulate stats from 14784 documents (23450 virtual)\n",
      "2025-09-10 11:52:33,998 : INFO : 232 batches submitted to accumulate stats from 14848 documents (23484 virtual)\n",
      "2025-09-10 11:52:34,092 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,093 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,092 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,093 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,092 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,092 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,095 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,093 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,093 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,094 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,094 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,094 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,095 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,094 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,094 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,097 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,097 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,097 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,096 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,103 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,099 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,101 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,107 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,106 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,102 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,103 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,104 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,105 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,107 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,107 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,107 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,108 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,115 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,111 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,111 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,112 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,111 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,113 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,107 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,112 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,115 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,117 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,118 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,119 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,124 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,120 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,116 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,123 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,124 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,108 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,122 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,123 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,127 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,112 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,130 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,141 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,144 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,132 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,129 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,130 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,139 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,148 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,130 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,100 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,137 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,148 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,144 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,146 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,143 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,146 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,150 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,112 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,154 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,098 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,155 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,155 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,109 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,157 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,157 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,158 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,160 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,125 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,159 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,162 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,150 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,163 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,114 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,167 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,172 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,136 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,173 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,170 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,171 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,121 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,161 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,132 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,178 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,180 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,182 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,140 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,188 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,158 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,188 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,193 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,195 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,198 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,157 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,200 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,111 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,136 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,205 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,222 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,229 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,233 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,235 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,234 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,243 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,247 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,234 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,235 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,252 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,256 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,240 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,253 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,263 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,269 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,273 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,268 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,274 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,259 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,269 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,267 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,267 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,285 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,281 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,288 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,296 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,290 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,299 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,302 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,107 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,223 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,340 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,179 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,242 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,302 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,414 : INFO : serializing accumulator to return to master...\n",
      "2025-09-10 11:52:34,250 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,220 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,110 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,418 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,204 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:34,209 : INFO : accumulator serialized\n",
      "2025-09-10 11:52:37,090 : INFO : 127 accumulators retrieved from output queue\n",
      "2025-09-10 11:52:37,140 : INFO : accumulated word occurrence stats for 296776 virtual documents\n",
      "2025-09-10 11:52:37,500 : INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "2025-09-10 11:52:37,501 : INFO : using symmetric eta at 0.2\n",
      "2025-09-10 11:52:37,507 : INFO : using serial LDA version on this node\n",
      "2025-09-10 11:52:37,522 : INFO : running online (multi-pass) LDA training, 5 topics, 50 passes over the supplied corpus of 14833 documents, updating model once every 2000 documents, evaluating perplexity every 2000 documents, iterating 50x with a convergence threshold of 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 5 - Coherence: 0.3217\n",
      "KneeLocator detected optimal topic number: 5\n",
      "최적 토픽 수: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 11:52:39,515 : INFO : -11.043 per-word bound, 2110.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:52:39,517 : INFO : PROGRESS: pass 0, at document #2000/14833\n",
      "2025-09-10 11:52:40,732 : INFO : optimized alpha [0.36752695, 0.32752627, 0.33193612, 0.33693862, 0.320106]\n",
      "2025-09-10 11:52:40,738 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:40,744 : INFO : topic #0 (0.368): 0.012*\"like\" + 0.009*\"one\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"even\" + 0.007*\"also\" + 0.006*\"time\" + 0.006*\"really\"\n",
      "2025-09-10 11:52:40,745 : INFO : topic #1 (0.328): 0.010*\"one\" + 0.010*\"like\" + 0.009*\"people\" + 0.007*\"even\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"might\" + 0.005*\"often\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:40,746 : INFO : topic #2 (0.332): 0.011*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.006*\"would\" + 0.006*\"time\" + 0.006*\"one\" + 0.006*\"might\" + 0.006*\"also\" + 0.005*\"could\" + 0.005*\"even\"\n",
      "2025-09-10 11:52:40,747 : INFO : topic #3 (0.337): 0.013*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.008*\"might\" + 0.007*\"people\" + 0.007*\"really\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"way\" + 0.005*\"someone\"\n",
      "2025-09-10 11:52:40,748 : INFO : topic #4 (0.320): 0.015*\"like\" + 0.008*\"people\" + 0.008*\"one\" + 0.008*\"often\" + 0.006*\"might\" + 0.006*\"could\" + 0.005*\"also\" + 0.005*\"time\" + 0.004*\"think\" + 0.004*\"even\"\n",
      "2025-09-10 11:52:40,748 : INFO : topic diff=4.931192, rho=1.000000\n",
      "2025-09-10 11:52:42,559 : INFO : -8.342 per-word bound, 324.5 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:52:42,559 : INFO : PROGRESS: pass 0, at document #4000/14833\n",
      "2025-09-10 11:52:43,730 : INFO : optimized alpha [0.3636253, 0.2625306, 0.29722476, 0.43324876, 0.19272184]\n",
      "2025-09-10 11:52:43,735 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:43,741 : INFO : topic #0 (0.364): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"one\" + 0.009*\"something\" + 0.009*\"people\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"really\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:52:43,742 : INFO : topic #1 (0.263): 0.010*\"like\" + 0.009*\"one\" + 0.009*\"people\" + 0.007*\"life\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"time\" + 0.005*\"way\" + 0.005*\"might\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:43,743 : INFO : topic #2 (0.297): 0.011*\"like\" + 0.011*\"would\" + 0.011*\"think\" + 0.008*\"something\" + 0.006*\"time\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"people\"\n",
      "2025-09-10 11:52:43,744 : INFO : topic #3 (0.433): 0.015*\"like\" + 0.011*\"one\" + 0.010*\"think\" + 0.008*\"really\" + 0.008*\"time\" + 0.007*\"people\" + 0.006*\"way\" + 0.006*\"even\" + 0.005*\"someone\" + 0.005*\"know\"\n",
      "2025-09-10 11:52:43,744 : INFO : topic #4 (0.193): 0.015*\"like\" + 0.009*\"people\" + 0.009*\"one\" + 0.007*\"often\" + 0.006*\"think\" + 0.005*\"could\" + 0.005*\"also\" + 0.005*\"know\" + 0.005*\"might\" + 0.005*\"way\"\n",
      "2025-09-10 11:52:43,745 : INFO : topic diff=0.733505, rho=0.707107\n",
      "2025-09-10 11:52:45,392 : INFO : -8.094 per-word bound, 273.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:52:45,392 : INFO : PROGRESS: pass 0, at document #6000/14833\n",
      "2025-09-10 11:52:46,467 : INFO : optimized alpha [0.2550937, 0.15738288, 0.18449694, 0.49790174, 0.12509975]\n",
      "2025-09-10 11:52:46,473 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:46,479 : INFO : topic #0 (0.255): 0.014*\"like\" + 0.013*\"think\" + 0.011*\"people\" + 0.009*\"something\" + 0.008*\"even\" + 0.008*\"also\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"really\" + 0.006*\"someone\"\n",
      "2025-09-10 11:52:46,480 : INFO : topic #1 (0.157): 0.011*\"like\" + 0.009*\"people\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"even\" + 0.006*\"life\" + 0.005*\"way\" + 0.005*\"time\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:52:46,481 : INFO : topic #2 (0.184): 0.012*\"think\" + 0.012*\"like\" + 0.011*\"would\" + 0.008*\"something\" + 0.007*\"even\" + 0.006*\"one\" + 0.006*\"time\" + 0.005*\"also\" + 0.005*\"could\" + 0.005*\"things\"\n",
      "2025-09-10 11:52:46,481 : INFO : topic #3 (0.498): 0.020*\"like\" + 0.011*\"one\" + 0.010*\"think\" + 0.009*\"really\" + 0.008*\"even\" + 0.008*\"okay\" + 0.008*\"time\" + 0.007*\"know\" + 0.007*\"people\" + 0.006*\"little\"\n",
      "2025-09-10 11:52:46,482 : INFO : topic #4 (0.125): 0.015*\"like\" + 0.011*\"people\" + 0.008*\"one\" + 0.007*\"think\" + 0.006*\"often\" + 0.006*\"know\" + 0.005*\"also\" + 0.005*\"even\" + 0.005*\"way\" + 0.005*\"really\"\n",
      "2025-09-10 11:52:46,483 : INFO : topic diff=0.578102, rho=0.577350\n",
      "2025-09-10 11:52:48,042 : INFO : -8.383 per-word bound, 333.9 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:52:48,043 : INFO : PROGRESS: pass 0, at document #8000/14833\n",
      "2025-09-10 11:52:49,086 : INFO : optimized alpha [0.21215609, 0.13132252, 0.15705052, 0.44165868, 0.10982029]\n",
      "2025-09-10 11:52:49,091 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:49,097 : INFO : topic #0 (0.212): 0.012*\"think\" + 0.012*\"people\" + 0.011*\"like\" + 0.011*\"something\" + 0.007*\"might\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"even\" + 0.006*\"really\"\n",
      "2025-09-10 11:52:49,098 : INFO : topic #1 (0.131): 0.008*\"think\" + 0.008*\"like\" + 0.008*\"people\" + 0.006*\"one\" + 0.006*\"even\" + 0.006*\"actually\" + 0.005*\"life\" + 0.005*\"way\" + 0.005*\"time\" + 0.004*\"really\"\n",
      "2025-09-10 11:52:49,098 : INFO : topic #2 (0.157): 0.015*\"would\" + 0.010*\"think\" + 0.009*\"like\" + 0.006*\"something\" + 0.006*\"even\" + 0.005*\"one\" + 0.005*\"time\" + 0.005*\"people\" + 0.005*\"might\" + 0.005*\"could\"\n",
      "2025-09-10 11:52:49,099 : INFO : topic #3 (0.442): 0.017*\"like\" + 0.009*\"one\" + 0.008*\"think\" + 0.007*\"really\" + 0.007*\"people\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"someone\" + 0.006*\"know\" + 0.006*\"something\"\n",
      "2025-09-10 11:52:49,100 : INFO : topic #4 (0.110): 0.011*\"like\" + 0.011*\"people\" + 0.007*\"think\" + 0.006*\"something\" + 0.006*\"one\" + 0.005*\"could\" + 0.005*\"death\" + 0.004*\"actually\" + 0.004*\"know\" + 0.004*\"way\"\n",
      "2025-09-10 11:52:49,101 : INFO : topic diff=0.488205, rho=0.500000\n",
      "2025-09-10 11:52:50,570 : INFO : -8.358 per-word bound, 328.1 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:52:50,570 : INFO : PROGRESS: pass 0, at document #10000/14833\n",
      "2025-09-10 11:52:51,516 : INFO : optimized alpha [0.1888293, 0.11861409, 0.13984941, 0.4336456, 0.09969424]\n",
      "2025-09-10 11:52:51,522 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:51,527 : INFO : topic #0 (0.189): 0.014*\"people\" + 0.011*\"like\" + 0.011*\"think\" + 0.010*\"something\" + 0.007*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"time\" + 0.006*\"also\"\n",
      "2025-09-10 11:52:51,528 : INFO : topic #1 (0.119): 0.009*\"like\" + 0.008*\"people\" + 0.007*\"think\" + 0.006*\"actually\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\" + 0.005*\"language\" + 0.005*\"one\" + 0.004*\"sleep\"\n",
      "2025-09-10 11:52:51,529 : INFO : topic #2 (0.140): 0.017*\"would\" + 0.009*\"like\" + 0.009*\"think\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"people\" + 0.005*\"something\" + 0.005*\"might\" + 0.005*\"one\" + 0.005*\"could\"\n",
      "2025-09-10 11:52:51,529 : INFO : topic #3 (0.434): 0.015*\"like\" + 0.009*\"one\" + 0.007*\"people\" + 0.007*\"think\" + 0.007*\"even\" + 0.006*\"time\" + 0.006*\"really\" + 0.006*\"something\" + 0.005*\"someone\" + 0.005*\"way\"\n",
      "2025-09-10 11:52:51,530 : INFO : topic #4 (0.100): 0.011*\"like\" + 0.010*\"people\" + 0.007*\"something\" + 0.007*\"think\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"god\" + 0.005*\"way\" + 0.005*\"even\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:51,530 : INFO : topic diff=0.400366, rho=0.447214\n",
      "2025-09-10 11:52:52,927 : INFO : -8.397 per-word bound, 337.1 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:52:52,928 : INFO : PROGRESS: pass 0, at document #12000/14833\n",
      "2025-09-10 11:52:53,813 : INFO : optimized alpha [0.17526276, 0.10797839, 0.12763911, 0.42780912, 0.09115969]\n",
      "2025-09-10 11:52:53,819 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:53,825 : INFO : topic #0 (0.175): 0.015*\"people\" + 0.010*\"like\" + 0.009*\"even\" + 0.008*\"something\" + 0.008*\"think\" + 0.008*\"one\" + 0.007*\"someone\" + 0.007*\"might\" + 0.005*\"time\" + 0.005*\"often\"\n",
      "2025-09-10 11:52:53,826 : INFO : topic #1 (0.108): 0.008*\"people\" + 0.008*\"like\" + 0.007*\"even\" + 0.006*\"sleep\" + 0.005*\"actually\" + 0.005*\"think\" + 0.005*\"one\" + 0.005*\"life\" + 0.005*\"way\" + 0.005*\"apple\"\n",
      "2025-09-10 11:52:53,827 : INFO : topic #2 (0.128): 0.014*\"would\" + 0.008*\"like\" + 0.007*\"even\" + 0.006*\"one\" + 0.006*\"think\" + 0.005*\"time\" + 0.005*\"people\" + 0.004*\"might\" + 0.004*\"could\" + 0.004*\"something\"\n",
      "2025-09-10 11:52:53,827 : INFO : topic #3 (0.428): 0.015*\"like\" + 0.011*\"one\" + 0.008*\"even\" + 0.007*\"people\" + 0.007*\"time\" + 0.006*\"someone\" + 0.005*\"something\" + 0.005*\"way\" + 0.005*\"really\" + 0.005*\"think\"\n",
      "2025-09-10 11:52:53,828 : INFO : topic #4 (0.091): 0.011*\"people\" + 0.010*\"like\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"death\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"entrepreneurs\" + 0.005*\"love\" + 0.005*\"idea\"\n",
      "2025-09-10 11:52:53,829 : INFO : topic diff=0.366290, rho=0.408248\n",
      "2025-09-10 11:52:55,305 : INFO : -9.087 per-word bound, 543.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:52:55,306 : INFO : PROGRESS: pass 0, at document #14000/14833\n",
      "2025-09-10 11:52:56,217 : INFO : optimized alpha [0.17980123, 0.106069505, 0.13613564, 0.474418, 0.09398817]\n",
      "2025-09-10 11:52:56,223 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:56,229 : INFO : topic #0 (0.180): 0.014*\"people\" + 0.009*\"like\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"something\" + 0.007*\"think\" + 0.006*\"someone\" + 0.006*\"might\" + 0.005*\"work\"\n",
      "2025-09-10 11:52:56,230 : INFO : topic #1 (0.106): 0.007*\"people\" + 0.007*\"language\" + 0.007*\"like\" + 0.007*\"sleep\" + 0.005*\"even\" + 0.005*\"life\" + 0.005*\"one\" + 0.005*\"way\" + 0.004*\"day\" + 0.004*\"think\"\n",
      "2025-09-10 11:52:56,231 : INFO : topic #2 (0.136): 0.020*\"would\" + 0.008*\"time\" + 0.006*\"like\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"travel\" + 0.004*\"think\" + 0.004*\"people\" + 0.004*\"could\" + 0.003*\"might\"\n",
      "2025-09-10 11:52:56,231 : INFO : topic #3 (0.474): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"back\" + 0.004*\"life\" + 0.004*\"got\"\n",
      "2025-09-10 11:52:56,232 : INFO : topic #4 (0.094): 0.010*\"god\" + 0.010*\"death\" + 0.009*\"people\" + 0.008*\"one\" + 0.007*\"like\" + 0.005*\"many\" + 0.005*\"something\" + 0.005*\"life\" + 0.005*\"even\" + 0.005*\"question\"\n",
      "2025-09-10 11:52:56,233 : INFO : topic diff=0.724113, rho=0.377964\n",
      "2025-09-10 11:52:56,838 : INFO : -8.906 per-word bound, 479.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:52:56,838 : INFO : PROGRESS: pass 0, at document #14833/14833\n",
      "2025-09-10 11:52:57,197 : INFO : optimized alpha [0.17177303, 0.10370065, 0.13935204, 0.52535117, 0.095775746]\n",
      "2025-09-10 11:52:57,203 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:57,209 : INFO : topic #0 (0.172): 0.015*\"people\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"think\" + 0.006*\"someone\" + 0.006*\"something\" + 0.005*\"might\" + 0.005*\"work\"\n",
      "2025-09-10 11:52:57,210 : INFO : topic #1 (0.104): 0.009*\"teeth\" + 0.007*\"jobs\" + 0.007*\"people\" + 0.006*\"apple\" + 0.006*\"like\" + 0.005*\"language\" + 0.005*\"sleep\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"one\"\n",
      "2025-09-10 11:52:57,211 : INFO : topic #2 (0.139): 0.020*\"would\" + 0.007*\"time\" + 0.006*\"one\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"could\" + 0.004*\"people\" + 0.003*\"students\" + 0.003*\"think\" + 0.003*\"travel\"\n",
      "2025-09-10 11:52:57,211 : INFO : topic #3 (0.525): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"people\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"back\"\n",
      "2025-09-10 11:52:57,212 : INFO : topic #4 (0.096): 0.010*\"death\" + 0.009*\"god\" + 0.008*\"people\" + 0.007*\"one\" + 0.006*\"like\" + 0.006*\"many\" + 0.005*\"life\" + 0.004*\"question\" + 0.004*\"even\" + 0.004*\"entrepreneurs\"\n",
      "2025-09-10 11:52:57,213 : INFO : topic diff=0.566304, rho=0.353553\n",
      "2025-09-10 11:52:58,962 : INFO : -8.178 per-word bound, 289.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:52:58,963 : INFO : PROGRESS: pass 1, at document #2000/14833\n",
      "2025-09-10 11:52:59,988 : INFO : optimized alpha [0.19571868, 0.09668745, 0.1229262, 0.36732596, 0.09161344]\n",
      "2025-09-10 11:52:59,993 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:52:59,999 : INFO : topic #0 (0.196): 0.012*\"people\" + 0.011*\"like\" + 0.010*\"might\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"someone\" + 0.007*\"time\" + 0.007*\"even\" + 0.006*\"also\"\n",
      "2025-09-10 11:53:00,000 : INFO : topic #1 (0.097): 0.007*\"language\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"like\" + 0.006*\"teeth\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"one\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:00,001 : INFO : topic #2 (0.123): 0.019*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"think\" + 0.004*\"travel\" + 0.004*\"might\" + 0.003*\"students\"\n",
      "2025-09-10 11:53:00,002 : INFO : topic #3 (0.367): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"people\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:53:00,003 : INFO : topic #4 (0.092): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"one\" + 0.007*\"people\" + 0.007*\"god\" + 0.006*\"many\" + 0.005*\"something\" + 0.005*\"life\" + 0.005*\"think\" + 0.005*\"idea\"\n",
      "2025-09-10 11:53:00,003 : INFO : topic diff=0.484119, rho=0.325878\n",
      "2025-09-10 11:53:01,563 : INFO : -7.963 per-word bound, 249.4 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:53:01,563 : INFO : PROGRESS: pass 1, at document #4000/14833\n",
      "2025-09-10 11:53:02,506 : INFO : optimized alpha [0.2133846, 0.090248406, 0.11028651, 0.3649473, 0.086106196]\n",
      "2025-09-10 11:53:02,512 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:02,518 : INFO : topic #0 (0.213): 0.012*\"people\" + 0.012*\"like\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"someone\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"time\"\n",
      "2025-09-10 11:53:02,519 : INFO : topic #1 (0.090): 0.008*\"language\" + 0.008*\"like\" + 0.006*\"teeth\" + 0.006*\"sleep\" + 0.005*\"people\" + 0.005*\"even\" + 0.005*\"also\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\"\n",
      "2025-09-10 11:53:02,519 : INFO : topic #2 (0.110): 0.022*\"would\" + 0.007*\"like\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"think\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"people\"\n",
      "2025-09-10 11:53:02,520 : INFO : topic #3 (0.365): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"back\" + 0.005*\"people\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:02,521 : INFO : topic #4 (0.086): 0.011*\"like\" + 0.008*\"one\" + 0.008*\"death\" + 0.008*\"think\" + 0.007*\"people\" + 0.007*\"god\" + 0.006*\"something\" + 0.005*\"idea\" + 0.005*\"life\" + 0.005*\"many\"\n",
      "2025-09-10 11:53:02,522 : INFO : topic diff=0.289933, rho=0.325878\n",
      "2025-09-10 11:53:03,914 : INFO : -7.827 per-word bound, 227.1 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:53:03,914 : INFO : PROGRESS: pass 1, at document #6000/14833\n",
      "2025-09-10 11:53:04,746 : INFO : optimized alpha [0.21395616, 0.0843447, 0.09782847, 0.38837746, 0.07831285]\n",
      "2025-09-10 11:53:04,751 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:04,757 : INFO : topic #0 (0.214): 0.014*\"like\" + 0.013*\"people\" + 0.013*\"think\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"also\" + 0.007*\"one\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:04,758 : INFO : topic #1 (0.084): 0.008*\"like\" + 0.007*\"think\" + 0.006*\"sleep\" + 0.006*\"even\" + 0.006*\"people\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:04,759 : INFO : topic #2 (0.098): 0.020*\"would\" + 0.007*\"like\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"time\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"might\" + 0.004*\"people\"\n",
      "2025-09-10 11:53:04,760 : INFO : topic #3 (0.388): 0.015*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.007*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"little\" + 0.005*\"think\" + 0.005*\"back\" + 0.005*\"people\"\n",
      "2025-09-10 11:53:04,761 : INFO : topic #4 (0.078): 0.011*\"like\" + 0.009*\"think\" + 0.008*\"people\" + 0.008*\"one\" + 0.008*\"death\" + 0.007*\"something\" + 0.006*\"idea\" + 0.006*\"god\" + 0.005*\"many\" + 0.005*\"life\"\n",
      "2025-09-10 11:53:04,761 : INFO : topic diff=0.250883, rho=0.325878\n",
      "2025-09-10 11:53:06,090 : INFO : -8.081 per-word bound, 270.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:53:06,090 : INFO : PROGRESS: pass 1, at document #8000/14833\n",
      "2025-09-10 11:53:06,902 : INFO : optimized alpha [0.2032058, 0.08075429, 0.09394167, 0.3668159, 0.07583363]\n",
      "2025-09-10 11:53:06,908 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:06,914 : INFO : topic #0 (0.203): 0.014*\"people\" + 0.013*\"think\" + 0.012*\"like\" + 0.009*\"something\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:06,915 : INFO : topic #1 (0.081): 0.007*\"like\" + 0.007*\"jobs\" + 0.007*\"think\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"actually\" + 0.005*\"sleep\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:06,916 : INFO : topic #2 (0.094): 0.020*\"would\" + 0.008*\"think\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"might\" + 0.004*\"travel\" + 0.004*\"people\"\n",
      "2025-09-10 11:53:06,917 : INFO : topic #3 (0.367): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"know\" + 0.005*\"people\" + 0.005*\"back\" + 0.005*\"think\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:06,918 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.009*\"think\" + 0.008*\"people\" + 0.008*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"sound\" + 0.005*\"many\" + 0.005*\"idea\"\n",
      "2025-09-10 11:53:06,918 : INFO : topic diff=0.276977, rho=0.325878\n",
      "2025-09-10 11:53:08,220 : INFO : -8.161 per-word bound, 286.3 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:53:08,220 : INFO : PROGRESS: pass 1, at document #10000/14833\n",
      "2025-09-10 11:53:09,002 : INFO : optimized alpha [0.19313978, 0.07900793, 0.09033562, 0.3600623, 0.07439571]\n",
      "2025-09-10 11:53:09,007 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:09,013 : INFO : topic #0 (0.193): 0.015*\"people\" + 0.012*\"like\" + 0.012*\"think\" + 0.009*\"something\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:09,014 : INFO : topic #1 (0.079): 0.008*\"like\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"people\" + 0.006*\"actually\" + 0.006*\"think\" + 0.005*\"even\" + 0.005*\"way\" + 0.005*\"jobs\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:09,015 : INFO : topic #2 (0.090): 0.022*\"would\" + 0.007*\"like\" + 0.007*\"time\" + 0.007*\"even\" + 0.007*\"think\" + 0.006*\"one\" + 0.005*\"travel\" + 0.005*\"might\" + 0.005*\"could\" + 0.004*\"people\"\n",
      "2025-09-10 11:53:09,016 : INFO : topic #3 (0.360): 0.014*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"really\" + 0.005*\"back\" + 0.005*\"people\" + 0.005*\"something\" + 0.005*\"day\" + 0.005*\"think\"\n",
      "2025-09-10 11:53:09,017 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.008*\"something\" + 0.008*\"think\" + 0.008*\"people\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"question\" + 0.005*\"sound\" + 0.005*\"idea\"\n",
      "2025-09-10 11:53:09,017 : INFO : topic diff=0.249505, rho=0.325878\n",
      "2025-09-10 11:53:10,256 : INFO : -8.203 per-word bound, 294.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:53:10,257 : INFO : PROGRESS: pass 1, at document #12000/14833\n",
      "2025-09-10 11:53:10,986 : INFO : optimized alpha [0.1867738, 0.076561406, 0.08688325, 0.3527521, 0.07106587]\n",
      "2025-09-10 11:53:10,992 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:10,997 : INFO : topic #0 (0.187): 0.016*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"often\"\n",
      "2025-09-10 11:53:10,998 : INFO : topic #1 (0.077): 0.008*\"sleep\" + 0.007*\"like\" + 0.006*\"people\" + 0.006*\"even\" + 0.006*\"apple\" + 0.006*\"jobs\" + 0.005*\"actually\" + 0.005*\"think\" + 0.005*\"teeth\" + 0.005*\"business\"\n",
      "2025-09-10 11:53:10,999 : INFO : topic #2 (0.087): 0.018*\"would\" + 0.007*\"even\" + 0.007*\"like\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"think\" + 0.004*\"could\" + 0.004*\"might\" + 0.004*\"people\" + 0.004*\"travel\"\n",
      "2025-09-10 11:53:11,000 : INFO : topic #3 (0.353): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.005*\"people\" + 0.005*\"really\" + 0.005*\"something\" + 0.005*\"way\" + 0.004*\"back\" + 0.004*\"got\"\n",
      "2025-09-10 11:53:11,001 : INFO : topic #4 (0.071): 0.010*\"like\" + 0.008*\"something\" + 0.008*\"people\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"even\" + 0.006*\"idea\" + 0.006*\"love\" + 0.005*\"god\"\n",
      "2025-09-10 11:53:11,001 : INFO : topic diff=0.248466, rho=0.325878\n",
      "2025-09-10 11:53:12,356 : INFO : -8.813 per-word bound, 449.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:53:12,356 : INFO : PROGRESS: pass 1, at document #14000/14833\n",
      "2025-09-10 11:53:13,148 : INFO : optimized alpha [0.18737233, 0.07731389, 0.09397611, 0.38834825, 0.07498731]\n",
      "2025-09-10 11:53:13,153 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:13,159 : INFO : topic #0 (0.187): 0.015*\"people\" + 0.010*\"like\" + 0.008*\"think\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"life\"\n",
      "2025-09-10 11:53:13,160 : INFO : topic #1 (0.077): 0.008*\"sleep\" + 0.008*\"language\" + 0.007*\"like\" + 0.006*\"people\" + 0.005*\"even\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.004*\"day\" + 0.004*\"apple\" + 0.004*\"actually\"\n",
      "2025-09-10 11:53:13,160 : INFO : topic #2 (0.094): 0.022*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"travel\" + 0.006*\"like\" + 0.005*\"could\" + 0.004*\"think\" + 0.004*\"people\" + 0.003*\"speed\"\n",
      "2025-09-10 11:53:13,161 : INFO : topic #3 (0.388): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"people\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.004*\"said\"\n",
      "2025-09-10 11:53:13,162 : INFO : topic #4 (0.075): 0.010*\"death\" + 0.009*\"god\" + 0.008*\"like\" + 0.008*\"people\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.005*\"question\" + 0.005*\"think\" + 0.005*\"many\"\n",
      "2025-09-10 11:53:13,162 : INFO : topic diff=0.535871, rho=0.325878\n",
      "2025-09-10 11:53:13,713 : INFO : -8.683 per-word bound, 411.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:53:13,714 : INFO : PROGRESS: pass 1, at document #14833/14833\n",
      "2025-09-10 11:53:14,014 : INFO : optimized alpha [0.17405458, 0.07785961, 0.098307215, 0.42693615, 0.07672058]\n",
      "2025-09-10 11:53:14,019 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:14,025 : INFO : topic #0 (0.174): 0.016*\"people\" + 0.009*\"like\" + 0.008*\"think\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"something\" + 0.006*\"time\" + 0.006*\"might\" + 0.006*\"life\"\n",
      "2025-09-10 11:53:14,025 : INFO : topic #1 (0.078): 0.009*\"teeth\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"business\" + 0.006*\"people\" + 0.006*\"money\" + 0.006*\"like\"\n",
      "2025-09-10 11:53:14,026 : INFO : topic #2 (0.098): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"could\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"people\"\n",
      "2025-09-10 11:53:14,027 : INFO : topic #3 (0.427): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"people\"\n",
      "2025-09-10 11:53:14,028 : INFO : topic #4 (0.077): 0.010*\"death\" + 0.009*\"god\" + 0.007*\"people\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"many\" + 0.006*\"life\" + 0.005*\"something\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:53:14,028 : INFO : topic diff=0.430974, rho=0.325878\n",
      "2025-09-10 11:53:15,640 : INFO : -8.099 per-word bound, 274.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:53:15,640 : INFO : PROGRESS: pass 2, at document #2000/14833\n",
      "2025-09-10 11:53:16,526 : INFO : optimized alpha [0.2024262, 0.0746259, 0.09219392, 0.32787994, 0.076180935]\n",
      "2025-09-10 11:53:16,531 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:16,537 : INFO : topic #0 (0.202): 0.013*\"people\" + 0.011*\"like\" + 0.009*\"might\" + 0.009*\"think\" + 0.007*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.006*\"also\" + 0.006*\"often\"\n",
      "2025-09-10 11:53:16,537 : INFO : topic #1 (0.075): 0.008*\"language\" + 0.008*\"jobs\" + 0.008*\"apple\" + 0.007*\"teeth\" + 0.006*\"company\" + 0.006*\"like\" + 0.006*\"business\" + 0.005*\"people\" + 0.005*\"also\" + 0.005*\"sleep\"\n",
      "2025-09-10 11:53:16,538 : INFO : topic #2 (0.092): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"think\" + 0.003*\"war\"\n",
      "2025-09-10 11:53:16,539 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"really\"\n",
      "2025-09-10 11:53:16,540 : INFO : topic #4 (0.076): 0.009*\"like\" + 0.009*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"something\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"idea\" + 0.006*\"think\" + 0.006*\"many\"\n",
      "2025-09-10 11:53:16,540 : INFO : topic diff=0.379078, rho=0.309841\n",
      "2025-09-10 11:53:17,998 : INFO : -7.931 per-word bound, 244.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:53:17,999 : INFO : PROGRESS: pass 2, at document #4000/14833\n",
      "2025-09-10 11:53:18,855 : INFO : optimized alpha [0.22750653, 0.07170829, 0.08650126, 0.3303451, 0.07456684]\n",
      "2025-09-10 11:53:18,860 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:18,866 : INFO : topic #0 (0.228): 0.013*\"people\" + 0.012*\"like\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"life\"\n",
      "2025-09-10 11:53:18,867 : INFO : topic #1 (0.072): 0.009*\"language\" + 0.007*\"sleep\" + 0.007*\"like\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"also\" + 0.005*\"people\"\n",
      "2025-09-10 11:53:18,868 : INFO : topic #2 (0.087): 0.023*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"travel\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:18,868 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"really\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"would\"\n",
      "2025-09-10 11:53:18,869 : INFO : topic #4 (0.075): 0.011*\"like\" + 0.008*\"think\" + 0.008*\"one\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"people\" + 0.006*\"sound\" + 0.006*\"idea\" + 0.006*\"life\"\n",
      "2025-09-10 11:53:18,870 : INFO : topic diff=0.233333, rho=0.309841\n",
      "2025-09-10 11:53:20,197 : INFO : -7.808 per-word bound, 224.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:53:20,198 : INFO : PROGRESS: pass 2, at document #6000/14833\n",
      "2025-09-10 11:53:20,973 : INFO : optimized alpha [0.23806192, 0.069311924, 0.07983375, 0.35556182, 0.06958985]\n",
      "2025-09-10 11:53:20,978 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:20,984 : INFO : topic #0 (0.238): 0.014*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"things\"\n",
      "2025-09-10 11:53:20,985 : INFO : topic #1 (0.069): 0.008*\"like\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"think\" + 0.006*\"teeth\" + 0.006*\"even\" + 0.006*\"business\" + 0.006*\"people\"\n",
      "2025-09-10 11:53:20,986 : INFO : topic #2 (0.080): 0.021*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.006*\"think\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:20,987 : INFO : topic #3 (0.356): 0.014*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"little\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:20,988 : INFO : topic #4 (0.070): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"death\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"people\" + 0.007*\"idea\" + 0.006*\"god\" + 0.006*\"life\" + 0.005*\"sound\"\n",
      "2025-09-10 11:53:20,988 : INFO : topic diff=0.212312, rho=0.309841\n",
      "2025-09-10 11:53:22,258 : INFO : -8.055 per-word bound, 266.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:53:22,259 : INFO : PROGRESS: pass 2, at document #8000/14833\n",
      "2025-09-10 11:53:23,009 : INFO : optimized alpha [0.23530307, 0.06797753, 0.07846798, 0.3480798, 0.068796344]\n",
      "2025-09-10 11:53:23,015 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:23,021 : INFO : topic #0 (0.235): 0.014*\"people\" + 0.013*\"like\" + 0.013*\"think\" + 0.009*\"something\" + 0.009*\"someone\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:23,022 : INFO : topic #1 (0.068): 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"like\" + 0.006*\"think\" + 0.006*\"sleep\" + 0.006*\"people\" + 0.005*\"also\" + 0.005*\"teeth\" + 0.005*\"actually\"\n",
      "2025-09-10 11:53:23,023 : INFO : topic #2 (0.078): 0.021*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"think\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:23,024 : INFO : topic #3 (0.348): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"get\" + 0.005*\"little\"\n",
      "2025-09-10 11:53:23,024 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.007*\"people\" + 0.006*\"sound\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"idea\" + 0.005*\"really\"\n",
      "2025-09-10 11:53:23,025 : INFO : topic diff=0.234055, rho=0.309841\n",
      "2025-09-10 11:53:24,279 : INFO : -8.127 per-word bound, 279.5 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:53:24,280 : INFO : PROGRESS: pass 2, at document #10000/14833\n",
      "2025-09-10 11:53:25,020 : INFO : optimized alpha [0.22684145, 0.06723891, 0.07714919, 0.34524563, 0.06882217]\n",
      "2025-09-10 11:53:25,025 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:25,031 : INFO : topic #0 (0.227): 0.015*\"people\" + 0.012*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"also\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:25,031 : INFO : topic #1 (0.067): 0.008*\"sleep\" + 0.007*\"language\" + 0.007*\"like\" + 0.006*\"actually\" + 0.006*\"people\" + 0.005*\"jobs\" + 0.005*\"money\" + 0.005*\"think\" + 0.005*\"even\" + 0.005*\"apple\"\n",
      "2025-09-10 11:53:25,032 : INFO : topic #2 (0.077): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"even\" + 0.007*\"like\" + 0.006*\"one\" + 0.006*\"think\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"might\" + 0.004*\"back\"\n",
      "2025-09-10 11:53:25,033 : INFO : topic #3 (0.345): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"something\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:25,034 : INFO : topic #4 (0.069): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"people\" + 0.006*\"one\" + 0.006*\"god\" + 0.006*\"sound\" + 0.006*\"idea\" + 0.006*\"question\"\n",
      "2025-09-10 11:53:25,034 : INFO : topic diff=0.212178, rho=0.309841\n",
      "2025-09-10 11:53:26,230 : INFO : -8.168 per-word bound, 287.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:53:26,230 : INFO : PROGRESS: pass 2, at document #12000/14833\n",
      "2025-09-10 11:53:26,914 : INFO : optimized alpha [0.2202653, 0.06634204, 0.075496785, 0.3406635, 0.06643014]\n",
      "2025-09-10 11:53:26,919 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:26,925 : INFO : topic #0 (0.220): 0.016*\"people\" + 0.011*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:53:26,925 : INFO : topic #1 (0.066): 0.008*\"sleep\" + 0.007*\"like\" + 0.006*\"people\" + 0.006*\"jobs\" + 0.006*\"even\" + 0.006*\"apple\" + 0.005*\"money\" + 0.005*\"actually\" + 0.005*\"business\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:53:26,926 : INFO : topic #2 (0.075): 0.019*\"would\" + 0.007*\"even\" + 0.007*\"time\" + 0.007*\"one\" + 0.007*\"like\" + 0.005*\"could\" + 0.005*\"think\" + 0.005*\"travel\" + 0.004*\"might\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:26,927 : INFO : topic #3 (0.341): 0.014*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"really\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"people\" + 0.005*\"something\" + 0.004*\"day\"\n",
      "2025-09-10 11:53:26,928 : INFO : topic #4 (0.066): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.008*\"people\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"idea\" + 0.006*\"love\" + 0.006*\"even\" + 0.005*\"sound\"\n",
      "2025-09-10 11:53:26,928 : INFO : topic diff=0.217705, rho=0.309841\n",
      "2025-09-10 11:53:28,232 : INFO : -8.742 per-word bound, 428.2 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:53:28,233 : INFO : PROGRESS: pass 2, at document #14000/14833\n",
      "2025-09-10 11:53:28,987 : INFO : optimized alpha [0.21882604, 0.06798727, 0.08203007, 0.3745031, 0.070576005]\n",
      "2025-09-10 11:53:28,993 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:28,999 : INFO : topic #0 (0.219): 0.016*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"might\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:29,000 : INFO : topic #1 (0.068): 0.008*\"sleep\" + 0.008*\"language\" + 0.006*\"like\" + 0.006*\"people\" + 0.006*\"money\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"apple\"\n",
      "2025-09-10 11:53:29,000 : INFO : topic #2 (0.082): 0.022*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"think\" + 0.003*\"speed\"\n",
      "2025-09-10 11:53:29,001 : INFO : topic #3 (0.375): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"said\" + 0.004*\"years\"\n",
      "2025-09-10 11:53:29,002 : INFO : topic #4 (0.071): 0.009*\"death\" + 0.009*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.007*\"people\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"question\" + 0.005*\"think\" + 0.005*\"sound\"\n",
      "2025-09-10 11:53:29,003 : INFO : topic diff=0.471139, rho=0.309841\n",
      "2025-09-10 11:53:29,531 : INFO : -8.603 per-word bound, 388.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:53:29,531 : INFO : PROGRESS: pass 2, at document #14833/14833\n",
      "2025-09-10 11:53:29,814 : INFO : optimized alpha [0.19904508, 0.069309644, 0.086244024, 0.40844113, 0.072324626]\n",
      "2025-09-10 11:53:29,819 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:29,825 : INFO : topic #0 (0.199): 0.017*\"people\" + 0.010*\"like\" + 0.008*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"might\" + 0.006*\"things\"\n",
      "2025-09-10 11:53:29,826 : INFO : topic #1 (0.069): 0.009*\"teeth\" + 0.008*\"jobs\" + 0.007*\"money\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.006*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:53:29,827 : INFO : topic #2 (0.086): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:53:29,828 : INFO : topic #3 (0.408): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"got\"\n",
      "2025-09-10 11:53:29,829 : INFO : topic #4 (0.072): 0.010*\"death\" + 0.009*\"god\" + 0.007*\"like\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"many\" + 0.005*\"question\" + 0.005*\"think\"\n",
      "2025-09-10 11:53:29,829 : INFO : topic diff=0.376239, rho=0.309841\n",
      "2025-09-10 11:53:31,398 : INFO : -8.071 per-word bound, 268.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:53:31,399 : INFO : PROGRESS: pass 3, at document #2000/14833\n",
      "2025-09-10 11:53:32,227 : INFO : optimized alpha [0.22990422, 0.06735732, 0.082681954, 0.3237604, 0.07270466]\n",
      "2025-09-10 11:53:32,232 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:32,238 : INFO : topic #0 (0.230): 0.013*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:53:32,239 : INFO : topic #1 (0.067): 0.008*\"jobs\" + 0.008*\"language\" + 0.008*\"apple\" + 0.007*\"teeth\" + 0.007*\"company\" + 0.006*\"business\" + 0.006*\"money\" + 0.006*\"like\" + 0.005*\"sleep\" + 0.005*\"people\"\n",
      "2025-09-10 11:53:32,239 : INFO : topic #2 (0.083): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.005*\"even\" + 0.005*\"travel\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:53:32,240 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:53:32,241 : INFO : topic #4 (0.073): 0.010*\"like\" + 0.009*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"idea\" + 0.006*\"people\" + 0.006*\"think\" + 0.005*\"many\"\n",
      "2025-09-10 11:53:32,241 : INFO : topic diff=0.331399, rho=0.295960\n",
      "2025-09-10 11:53:33,656 : INFO : -7.916 per-word bound, 241.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:53:33,657 : INFO : PROGRESS: pass 3, at document #4000/14833\n",
      "2025-09-10 11:53:34,441 : INFO : optimized alpha [0.2588826, 0.06565124, 0.07915539, 0.32710177, 0.0722516]\n",
      "2025-09-10 11:53:34,446 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:34,452 : INFO : topic #0 (0.259): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"also\" + 0.007*\"life\" + 0.007*\"even\"\n",
      "2025-09-10 11:53:34,453 : INFO : topic #1 (0.066): 0.009*\"language\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"like\" + 0.006*\"company\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.005*\"money\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:34,454 : INFO : topic #2 (0.079): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"could\" + 0.006*\"even\" + 0.006*\"like\" + 0.004*\"back\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:34,455 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"know\"\n",
      "2025-09-10 11:53:34,456 : INFO : topic #4 (0.072): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"death\" + 0.008*\"something\" + 0.007*\"god\" + 0.006*\"idea\" + 0.006*\"sound\" + 0.006*\"life\" + 0.006*\"people\"\n",
      "2025-09-10 11:53:34,456 : INFO : topic diff=0.204056, rho=0.295960\n",
      "2025-09-10 11:53:35,767 : INFO : -7.801 per-word bound, 223.1 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:53:35,768 : INFO : PROGRESS: pass 3, at document #6000/14833\n",
      "2025-09-10 11:53:36,521 : INFO : optimized alpha [0.27416793, 0.0642817, 0.0742089, 0.35196388, 0.068301424]\n",
      "2025-09-10 11:53:36,526 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:36,532 : INFO : topic #0 (0.274): 0.014*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:53:36,532 : INFO : topic #1 (0.064): 0.007*\"sleep\" + 0.007*\"like\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"money\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"even\"\n",
      "2025-09-10 11:53:36,533 : INFO : topic #2 (0.074): 0.022*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"travel\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:53:36,534 : INFO : topic #3 (0.352): 0.014*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"back\" + 0.006*\"know\" + 0.005*\"day\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:36,535 : INFO : topic #4 (0.068): 0.012*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"death\" + 0.008*\"one\" + 0.007*\"idea\" + 0.006*\"people\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"maybe\"\n",
      "2025-09-10 11:53:36,535 : INFO : topic diff=0.191129, rho=0.295960\n",
      "2025-09-10 11:53:37,778 : INFO : -8.043 per-word bound, 263.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:53:37,778 : INFO : PROGRESS: pass 3, at document #8000/14833\n",
      "2025-09-10 11:53:38,507 : INFO : optimized alpha [0.27277604, 0.063802704, 0.07361275, 0.3487674, 0.0682395]\n",
      "2025-09-10 11:53:38,512 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:38,518 : INFO : topic #0 (0.273): 0.014*\"people\" + 0.013*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.009*\"something\" + 0.008*\"might\" + 0.007*\"also\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"things\"\n",
      "2025-09-10 11:53:38,519 : INFO : topic #1 (0.064): 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"money\" + 0.006*\"sleep\" + 0.006*\"like\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"think\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:38,520 : INFO : topic #2 (0.074): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"like\" + 0.006*\"even\" + 0.006*\"think\" + 0.006*\"could\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:53:38,521 : INFO : topic #3 (0.349): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"really\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:53:38,522 : INFO : topic #4 (0.068): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"people\" + 0.006*\"sound\" + 0.006*\"one\" + 0.006*\"god\" + 0.005*\"really\" + 0.005*\"idea\"\n",
      "2025-09-10 11:53:38,522 : INFO : topic diff=0.208319, rho=0.295960\n",
      "2025-09-10 11:53:39,751 : INFO : -8.108 per-word bound, 275.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:53:39,751 : INFO : PROGRESS: pass 3, at document #10000/14833\n",
      "2025-09-10 11:53:40,465 : INFO : optimized alpha [0.2626685, 0.0635183, 0.072817154, 0.34648964, 0.06879604]\n",
      "2025-09-10 11:53:40,470 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:40,476 : INFO : topic #0 (0.263): 0.016*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"one\" + 0.006*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:53:40,478 : INFO : topic #1 (0.064): 0.008*\"sleep\" + 0.007*\"language\" + 0.007*\"like\" + 0.007*\"money\" + 0.006*\"actually\" + 0.006*\"jobs\" + 0.005*\"people\" + 0.005*\"apple\" + 0.005*\"business\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:53:40,479 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"like\" + 0.006*\"travel\" + 0.006*\"could\" + 0.006*\"think\" + 0.005*\"might\" + 0.005*\"back\"\n",
      "2025-09-10 11:53:40,479 : INFO : topic #3 (0.346): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"get\" + 0.004*\"know\"\n",
      "2025-09-10 11:53:40,480 : INFO : topic #4 (0.069): 0.012*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"people\" + 0.006*\"idea\" + 0.006*\"god\" + 0.006*\"maybe\" + 0.006*\"sound\"\n",
      "2025-09-10 11:53:40,481 : INFO : topic diff=0.191039, rho=0.295960\n",
      "2025-09-10 11:53:41,688 : INFO : -8.153 per-word bound, 284.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:53:41,689 : INFO : PROGRESS: pass 3, at document #12000/14833\n",
      "2025-09-10 11:53:42,350 : INFO : optimized alpha [0.25561395, 0.0631667, 0.07161564, 0.3424514, 0.0666066]\n",
      "2025-09-10 11:53:42,354 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:42,360 : INFO : topic #0 (0.256): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:53:42,361 : INFO : topic #1 (0.063): 0.008*\"sleep\" + 0.007*\"like\" + 0.006*\"money\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"even\" + 0.006*\"people\" + 0.005*\"business\" + 0.005*\"actually\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:53:42,362 : INFO : topic #2 (0.072): 0.019*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"think\" + 0.004*\"students\" + 0.004*\"might\"\n",
      "2025-09-10 11:53:42,363 : INFO : topic #3 (0.342): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\"\n",
      "2025-09-10 11:53:42,363 : INFO : topic #4 (0.067): 0.011*\"like\" + 0.009*\"something\" + 0.008*\"death\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"people\" + 0.006*\"idea\" + 0.006*\"even\" + 0.006*\"love\" + 0.006*\"maybe\"\n",
      "2025-09-10 11:53:42,364 : INFO : topic diff=0.199213, rho=0.295960\n",
      "2025-09-10 11:53:43,648 : INFO : -8.701 per-word bound, 416.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:53:43,649 : INFO : PROGRESS: pass 3, at document #14000/14833\n",
      "2025-09-10 11:53:44,382 : INFO : optimized alpha [0.25137356, 0.06521019, 0.07777787, 0.3748806, 0.07080447]\n",
      "2025-09-10 11:53:44,387 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:44,393 : INFO : topic #0 (0.251): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:53:44,394 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.008*\"language\" + 0.007*\"money\" + 0.006*\"like\" + 0.005*\"people\" + 0.005*\"business\" + 0.005*\"jobs\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"apple\"\n",
      "2025-09-10 11:53:44,395 : INFO : topic #2 (0.078): 0.022*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"speed\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:44,396 : INFO : topic #3 (0.375): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"years\"\n",
      "2025-09-10 11:53:44,396 : INFO : topic #4 (0.071): 0.009*\"death\" + 0.009*\"like\" + 0.009*\"god\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"black\" + 0.006*\"people\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:53:44,397 : INFO : topic diff=0.431692, rho=0.295960\n",
      "2025-09-10 11:53:44,919 : INFO : -8.556 per-word bound, 376.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:53:44,920 : INFO : PROGRESS: pass 3, at document #14833/14833\n",
      "2025-09-10 11:53:45,195 : INFO : optimized alpha [0.22473685, 0.06694332, 0.08168234, 0.406025, 0.07226839]\n",
      "2025-09-10 11:53:45,200 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:45,206 : INFO : topic #0 (0.225): 0.017*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:53:45,207 : INFO : topic #1 (0.067): 0.009*\"teeth\" + 0.008*\"money\" + 0.008*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:53:45,207 : INFO : topic #2 (0.082): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:53:45,208 : INFO : topic #3 (0.406): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"get\" + 0.005*\"got\" + 0.005*\"even\"\n",
      "2025-09-10 11:53:45,209 : INFO : topic #4 (0.072): 0.009*\"death\" + 0.008*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"something\" + 0.006*\"black\" + 0.005*\"question\" + 0.005*\"many\"\n",
      "2025-09-10 11:53:45,210 : INFO : topic diff=0.340199, rho=0.295960\n",
      "2025-09-10 11:53:46,721 : INFO : -8.053 per-word bound, 265.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:53:46,722 : INFO : PROGRESS: pass 4, at document #2000/14833\n",
      "2025-09-10 11:53:47,518 : INFO : optimized alpha [0.25666118, 0.06541258, 0.07921272, 0.32663494, 0.07289765]\n",
      "2025-09-10 11:53:47,525 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:47,533 : INFO : topic #0 (0.257): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.009*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:53:47,533 : INFO : topic #1 (0.065): 0.008*\"jobs\" + 0.008*\"language\" + 0.008*\"apple\" + 0.007*\"money\" + 0.007*\"teeth\" + 0.007*\"company\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:47,534 : INFO : topic #2 (0.079): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:53:47,535 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"got\"\n",
      "2025-09-10 11:53:47,536 : INFO : topic #4 (0.073): 0.010*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"idea\" + 0.006*\"black\" + 0.005*\"people\"\n",
      "2025-09-10 11:53:47,537 : INFO : topic diff=0.302405, rho=0.283792\n",
      "2025-09-10 11:53:48,948 : INFO : -7.907 per-word bound, 240.0 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:53:48,949 : INFO : PROGRESS: pass 4, at document #4000/14833\n",
      "2025-09-10 11:53:49,711 : INFO : optimized alpha [0.28747568, 0.06417181, 0.076518916, 0.3298798, 0.0726616]\n",
      "2025-09-10 11:53:49,717 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:49,723 : INFO : topic #0 (0.287): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"also\" + 0.007*\"even\"\n",
      "2025-09-10 11:53:49,724 : INFO : topic #1 (0.064): 0.008*\"language\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"money\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"apple\" + 0.005*\"also\"\n",
      "2025-09-10 11:53:49,725 : INFO : topic #2 (0.077): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"students\"\n",
      "2025-09-10 11:53:49,726 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:53:49,727 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"god\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"sound\" + 0.006*\"black\"\n",
      "2025-09-10 11:53:49,727 : INFO : topic diff=0.186137, rho=0.283792\n",
      "2025-09-10 11:53:51,018 : INFO : -7.798 per-word bound, 222.6 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:53:51,018 : INFO : PROGRESS: pass 4, at document #6000/14833\n",
      "2025-09-10 11:53:51,754 : INFO : optimized alpha [0.30610347, 0.063278496, 0.07230753, 0.3537219, 0.0691297]\n",
      "2025-09-10 11:53:51,759 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:51,766 : INFO : topic #0 (0.306): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"might\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:53:51,767 : INFO : topic #1 (0.063): 0.008*\"sleep\" + 0.007*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"like\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"even\"\n",
      "2025-09-10 11:53:51,768 : INFO : topic #2 (0.072): 0.022*\"would\" + 0.009*\"time\" + 0.008*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"like\" + 0.006*\"travel\" + 0.005*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:53:51,769 : INFO : topic #3 (0.354): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"know\" + 0.005*\"little\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:51,770 : INFO : topic #4 (0.069): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.007*\"idea\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"people\"\n",
      "2025-09-10 11:53:51,770 : INFO : topic diff=0.177447, rho=0.283792\n",
      "2025-09-10 11:53:53,030 : INFO : -8.037 per-word bound, 262.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:53:53,031 : INFO : PROGRESS: pass 4, at document #8000/14833\n",
      "2025-09-10 11:53:53,742 : INFO : optimized alpha [0.30496886, 0.06316766, 0.07196213, 0.35201558, 0.06945736]\n",
      "2025-09-10 11:53:53,748 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:53,756 : INFO : topic #0 (0.305): 0.014*\"people\" + 0.013*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"one\" + 0.007*\"things\"\n",
      "2025-09-10 11:53:53,757 : INFO : topic #1 (0.063): 0.008*\"jobs\" + 0.007*\"money\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"like\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"also\" + 0.005*\"actually\"\n",
      "2025-09-10 11:53:53,758 : INFO : topic #2 (0.072): 0.021*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"like\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:53:53,759 : INFO : topic #3 (0.352): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"really\" + 0.005*\"day\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:53:53,760 : INFO : topic #4 (0.069): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"people\" + 0.006*\"really\" + 0.006*\"god\" + 0.005*\"idea\"\n",
      "2025-09-10 11:53:53,761 : INFO : topic diff=0.189088, rho=0.283792\n",
      "2025-09-10 11:53:54,992 : INFO : -8.097 per-word bound, 273.9 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:53:54,992 : INFO : PROGRESS: pass 4, at document #10000/14833\n",
      "2025-09-10 11:53:55,686 : INFO : optimized alpha [0.29213768, 0.06302032, 0.07123809, 0.34886387, 0.0701618]\n",
      "2025-09-10 11:53:55,691 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:55,697 : INFO : topic #0 (0.292): 0.016*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:53:55,697 : INFO : topic #1 (0.063): 0.008*\"sleep\" + 0.007*\"money\" + 0.007*\"language\" + 0.006*\"like\" + 0.006*\"actually\" + 0.006*\"jobs\" + 0.005*\"business\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:53:55,698 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.010*\"time\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"like\" + 0.006*\"could\" + 0.005*\"think\" + 0.005*\"back\" + 0.005*\"might\"\n",
      "2025-09-10 11:53:55,699 : INFO : topic #3 (0.349): 0.013*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"years\"\n",
      "2025-09-10 11:53:55,700 : INFO : topic #4 (0.070): 0.012*\"like\" + 0.010*\"something\" + 0.009*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"idea\" + 0.006*\"god\" + 0.006*\"question\"\n",
      "2025-09-10 11:53:55,700 : INFO : topic diff=0.175547, rho=0.283792\n",
      "2025-09-10 11:53:56,867 : INFO : -8.146 per-word bound, 283.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:53:56,868 : INFO : PROGRESS: pass 4, at document #12000/14833\n",
      "2025-09-10 11:53:57,521 : INFO : optimized alpha [0.28436995, 0.06284391, 0.070275635, 0.3454125, 0.06803359]\n",
      "2025-09-10 11:53:57,526 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:57,532 : INFO : topic #0 (0.284): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:53:57,533 : INFO : topic #1 (0.063): 0.008*\"sleep\" + 0.007*\"money\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"even\" + 0.006*\"business\" + 0.005*\"people\" + 0.005*\"actually\" + 0.005*\"teeth\"\n",
      "2025-09-10 11:53:57,534 : INFO : topic #2 (0.070): 0.019*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"think\" + 0.004*\"students\" + 0.004*\"might\"\n",
      "2025-09-10 11:53:57,535 : INFO : topic #3 (0.345): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"made\" + 0.005*\"still\"\n",
      "2025-09-10 11:53:57,536 : INFO : topic #4 (0.068): 0.012*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"people\" + 0.006*\"idea\" + 0.006*\"maybe\" + 0.006*\"way\"\n",
      "2025-09-10 11:53:57,537 : INFO : topic diff=0.186731, rho=0.283792\n",
      "2025-09-10 11:53:58,811 : INFO : -8.674 per-word bound, 408.4 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:53:58,812 : INFO : PROGRESS: pass 4, at document #14000/14833\n",
      "2025-09-10 11:53:59,537 : INFO : optimized alpha [0.2776921, 0.06503681, 0.07615558, 0.37643808, 0.07208996]\n",
      "2025-09-10 11:53:59,543 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:53:59,549 : INFO : topic #0 (0.278): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:53:59,550 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.008*\"money\" + 0.008*\"language\" + 0.006*\"like\" + 0.005*\"business\" + 0.005*\"people\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:53:59,551 : INFO : topic #2 (0.076): 0.022*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"speed\"\n",
      "2025-09-10 11:53:59,552 : INFO : topic #3 (0.376): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"years\"\n",
      "2025-09-10 11:53:59,553 : INFO : topic #4 (0.072): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"people\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"question\"\n",
      "2025-09-10 11:53:59,553 : INFO : topic diff=0.401257, rho=0.283792\n",
      "2025-09-10 11:54:00,072 : INFO : -8.527 per-word bound, 369.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:54:00,073 : INFO : PROGRESS: pass 4, at document #14833/14833\n",
      "2025-09-10 11:54:00,349 : INFO : optimized alpha [0.24506931, 0.06676163, 0.07983786, 0.40483123, 0.0731436]\n",
      "2025-09-10 11:54:00,354 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:00,360 : INFO : topic #0 (0.245): 0.017*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:00,361 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:54:00,362 : INFO : topic #2 (0.080): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"war\" + 0.003*\"back\"\n",
      "2025-09-10 11:54:00,363 : INFO : topic #3 (0.405): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:00,364 : INFO : topic #4 (0.073): 0.009*\"death\" + 0.008*\"god\" + 0.008*\"like\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"something\" + 0.006*\"people\" + 0.005*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:54:00,364 : INFO : topic diff=0.314409, rho=0.283792\n",
      "2025-09-10 11:54:01,855 : INFO : -8.041 per-word bound, 263.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:54:01,856 : INFO : PROGRESS: pass 5, at document #2000/14833\n",
      "2025-09-10 11:54:02,625 : INFO : optimized alpha [0.27733546, 0.065320924, 0.07785321, 0.32899842, 0.073977314]\n",
      "2025-09-10 11:54:02,630 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:02,637 : INFO : topic #0 (0.277): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:54:02,638 : INFO : topic #1 (0.065): 0.008*\"jobs\" + 0.008*\"apple\" + 0.008*\"money\" + 0.007*\"language\" + 0.007*\"company\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:02,639 : INFO : topic #2 (0.078): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:54:02,639 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:02,640 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"god\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\" + 0.005*\"people\"\n",
      "2025-09-10 11:54:02,641 : INFO : topic diff=0.281140, rho=0.273011\n",
      "2025-09-10 11:54:04,020 : INFO : -7.902 per-word bound, 239.2 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:54:04,020 : INFO : PROGRESS: pass 5, at document #4000/14833\n",
      "2025-09-10 11:54:04,772 : INFO : optimized alpha [0.30876982, 0.064204015, 0.07549174, 0.3317793, 0.07395874]\n",
      "2025-09-10 11:54:04,778 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:04,784 : INFO : topic #0 (0.309): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:04,785 : INFO : topic #1 (0.064): 0.008*\"language\" + 0.007*\"money\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:04,786 : INFO : topic #2 (0.075): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"students\" + 0.004*\"think\"\n",
      "2025-09-10 11:54:04,787 : INFO : topic #3 (0.332): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:54:04,788 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:54:04,788 : INFO : topic diff=0.173001, rho=0.273011\n",
      "2025-09-10 11:54:06,072 : INFO : -7.796 per-word bound, 222.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:54:06,072 : INFO : PROGRESS: pass 5, at document #6000/14833\n",
      "2025-09-10 11:54:06,799 : INFO : optimized alpha [0.32891682, 0.06344366, 0.071699396, 0.35472387, 0.07053962]\n",
      "2025-09-10 11:54:06,804 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:06,811 : INFO : topic #0 (0.329): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:06,812 : INFO : topic #1 (0.063): 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"like\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:06,813 : INFO : topic #2 (0.072): 0.022*\"would\" + 0.009*\"time\" + 0.008*\"one\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"like\" + 0.004*\"think\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:54:06,813 : INFO : topic #3 (0.355): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"really\" + 0.006*\"day\" + 0.005*\"know\" + 0.005*\"little\" + 0.005*\"got\"\n",
      "2025-09-10 11:54:06,814 : INFO : topic #4 (0.071): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"idea\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:54:06,815 : INFO : topic diff=0.167075, rho=0.273011\n",
      "2025-09-10 11:54:08,034 : INFO : -8.033 per-word bound, 262.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:54:08,035 : INFO : PROGRESS: pass 5, at document #8000/14833\n",
      "2025-09-10 11:54:08,739 : INFO : optimized alpha [0.32744926, 0.06343265, 0.07134311, 0.35345173, 0.07091671]\n",
      "2025-09-10 11:54:08,745 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:08,751 : INFO : topic #0 (0.327): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:54:08,752 : INFO : topic #1 (0.063): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"like\" + 0.005*\"also\" + 0.005*\"company\"\n",
      "2025-09-10 11:54:08,753 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:08,754 : INFO : topic #3 (0.353): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:08,755 : INFO : topic #4 (0.071): 0.012*\"like\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"really\" + 0.006*\"people\" + 0.006*\"god\" + 0.005*\"maybe\"\n",
      "2025-09-10 11:54:08,755 : INFO : topic diff=0.175246, rho=0.273011\n",
      "2025-09-10 11:54:09,958 : INFO : -8.092 per-word bound, 272.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:54:09,958 : INFO : PROGRESS: pass 5, at document #10000/14833\n",
      "2025-09-10 11:54:10,646 : INFO : optimized alpha [0.31363285, 0.06332269, 0.07054932, 0.349692, 0.07166862]\n",
      "2025-09-10 11:54:10,651 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:10,658 : INFO : topic #0 (0.314): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:54:10,659 : INFO : topic #1 (0.063): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"like\" + 0.006*\"jobs\" + 0.006*\"actually\" + 0.005*\"business\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"people\"\n",
      "2025-09-10 11:54:10,660 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"travel\" + 0.006*\"could\" + 0.006*\"like\" + 0.005*\"think\" + 0.005*\"back\" + 0.004*\"might\"\n",
      "2025-09-10 11:54:10,661 : INFO : topic #3 (0.350): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:10,661 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"something\" + 0.009*\"think\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"idea\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:54:10,662 : INFO : topic diff=0.164316, rho=0.273011\n",
      "2025-09-10 11:54:11,843 : INFO : -8.141 per-word bound, 282.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:54:11,844 : INFO : PROGRESS: pass 5, at document #12000/14833\n",
      "2025-09-10 11:54:12,492 : INFO : optimized alpha [0.30538785, 0.06315816, 0.06961555, 0.3463307, 0.069479756]\n",
      "2025-09-10 11:54:12,497 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:12,503 : INFO : topic #0 (0.305): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:12,504 : INFO : topic #1 (0.063): 0.008*\"sleep\" + 0.008*\"money\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"people\" + 0.005*\"actually\"\n",
      "2025-09-10 11:54:12,506 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"like\" + 0.005*\"could\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"think\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:12,506 : INFO : topic #3 (0.346): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"really\" + 0.005*\"still\"\n",
      "2025-09-10 11:54:12,507 : INFO : topic #4 (0.069): 0.012*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"people\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:12,508 : INFO : topic diff=0.177313, rho=0.273011\n",
      "2025-09-10 11:54:13,789 : INFO : -8.655 per-word bound, 403.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:54:13,789 : INFO : PROGRESS: pass 5, at document #14000/14833\n",
      "2025-09-10 11:54:14,512 : INFO : optimized alpha [0.2980388, 0.06540275, 0.07526092, 0.37646013, 0.073344536]\n",
      "2025-09-10 11:54:14,517 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:14,522 : INFO : topic #0 (0.298): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:14,523 : INFO : topic #1 (0.065): 0.008*\"sleep\" + 0.008*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"jobs\" + 0.005*\"people\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:14,524 : INFO : topic #2 (0.075): 0.022*\"would\" + 0.011*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"speed\"\n",
      "2025-09-10 11:54:14,525 : INFO : topic #3 (0.376): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:14,526 : INFO : topic #4 (0.073): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"people\" + 0.006*\"question\"\n",
      "2025-09-10 11:54:14,526 : INFO : topic diff=0.377616, rho=0.273011\n",
      "2025-09-10 11:54:15,050 : INFO : -8.507 per-word bound, 363.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:54:15,050 : INFO : PROGRESS: pass 5, at document #14833/14833\n",
      "2025-09-10 11:54:15,327 : INFO : optimized alpha [0.26216447, 0.06712712, 0.078588836, 0.40323254, 0.074109934]\n",
      "2025-09-10 11:54:15,331 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:15,337 : INFO : topic #0 (0.262): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:15,338 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:54:15,339 : INFO : topic #2 (0.079): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:54:15,340 : INFO : topic #3 (0.403): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:15,340 : INFO : topic #4 (0.074): 0.009*\"death\" + 0.009*\"like\" + 0.008*\"god\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"people\" + 0.005*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:54:15,341 : INFO : topic diff=0.294814, rho=0.273011\n",
      "2025-09-10 11:54:16,815 : INFO : -8.032 per-word bound, 261.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:54:16,816 : INFO : PROGRESS: pass 6, at document #2000/14833\n",
      "2025-09-10 11:54:17,565 : INFO : optimized alpha [0.2945098, 0.06570008, 0.07692428, 0.3309884, 0.075025946]\n",
      "2025-09-10 11:54:17,570 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:17,576 : INFO : topic #0 (0.295): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:54:17,577 : INFO : topic #1 (0.066): 0.008*\"jobs\" + 0.008*\"money\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"company\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:17,578 : INFO : topic #2 (0.077): 0.020*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.006*\"could\" + 0.006*\"travel\" + 0.005*\"even\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:54:17,579 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:17,580 : INFO : topic #4 (0.075): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:17,580 : INFO : topic diff=0.264853, rho=0.263372\n",
      "2025-09-10 11:54:18,936 : INFO : -7.899 per-word bound, 238.7 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:54:18,937 : INFO : PROGRESS: pass 6, at document #4000/14833\n",
      "2025-09-10 11:54:19,687 : INFO : optimized alpha [0.32664323, 0.06471903, 0.07481568, 0.3332403, 0.07513526]\n",
      "2025-09-10 11:54:19,693 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:19,699 : INFO : topic #0 (0.327): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:19,700 : INFO : topic #1 (0.065): 0.008*\"language\" + 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:19,701 : INFO : topic #2 (0.075): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"could\" + 0.006*\"even\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"students\" + 0.003*\"think\"\n",
      "2025-09-10 11:54:19,702 : INFO : topic #3 (0.333): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:54:19,703 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"idea\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"sound\"\n",
      "2025-09-10 11:54:19,703 : INFO : topic diff=0.163111, rho=0.263372\n",
      "2025-09-10 11:54:20,971 : INFO : -7.794 per-word bound, 221.9 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:54:20,972 : INFO : PROGRESS: pass 6, at document #6000/14833\n",
      "2025-09-10 11:54:21,696 : INFO : optimized alpha [0.34786156, 0.06405277, 0.07127032, 0.35526717, 0.071764864]\n",
      "2025-09-10 11:54:21,702 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:21,708 : INFO : topic #0 (0.348): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:21,709 : INFO : topic #1 (0.064): 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"like\" + 0.006*\"company\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:21,710 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.008*\"time\" + 0.008*\"one\" + 0.006*\"even\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"students\"\n",
      "2025-09-10 11:54:21,711 : INFO : topic #3 (0.355): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:54:21,712 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"idea\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:54:21,712 : INFO : topic diff=0.158709, rho=0.263372\n",
      "2025-09-10 11:54:22,926 : INFO : -8.031 per-word bound, 261.5 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:54:22,927 : INFO : PROGRESS: pass 6, at document #8000/14833\n",
      "2025-09-10 11:54:23,619 : INFO : optimized alpha [0.34648496, 0.06408992, 0.07100704, 0.35442162, 0.07218642]\n",
      "2025-09-10 11:54:23,625 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:23,631 : INFO : topic #0 (0.346): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:54:23,632 : INFO : topic #1 (0.064): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:23,633 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"think\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:23,634 : INFO : topic #3 (0.354): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"really\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:23,635 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"really\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"people\"\n",
      "2025-09-10 11:54:23,635 : INFO : topic diff=0.164540, rho=0.263372\n",
      "2025-09-10 11:54:24,849 : INFO : -8.088 per-word bound, 272.1 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:54:24,850 : INFO : PROGRESS: pass 6, at document #10000/14833\n",
      "2025-09-10 11:54:25,528 : INFO : optimized alpha [0.33222705, 0.06401274, 0.07020396, 0.35054308, 0.072870724]\n",
      "2025-09-10 11:54:25,533 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:25,540 : INFO : topic #0 (0.332): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:54:25,541 : INFO : topic #1 (0.064): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"like\" + 0.006*\"actually\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:54:25,542 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.006*\"like\" + 0.005*\"back\" + 0.005*\"think\" + 0.004*\"might\"\n",
      "2025-09-10 11:54:25,543 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"made\"\n",
      "2025-09-10 11:54:25,544 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"idea\" + 0.006*\"question\"\n",
      "2025-09-10 11:54:25,544 : INFO : topic diff=0.155504, rho=0.263372\n",
      "2025-09-10 11:54:26,693 : INFO : -8.137 per-word bound, 281.4 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:54:26,693 : INFO : PROGRESS: pass 6, at document #12000/14833\n",
      "2025-09-10 11:54:27,341 : INFO : optimized alpha [0.32334366, 0.06390032, 0.06928987, 0.34716392, 0.07059971]\n",
      "2025-09-10 11:54:27,347 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:27,353 : INFO : topic #0 (0.323): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:27,354 : INFO : topic #1 (0.064): 0.008*\"sleep\" + 0.008*\"money\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"like\" + 0.006*\"business\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"people\"\n",
      "2025-09-10 11:54:27,355 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"even\" + 0.005*\"like\" + 0.005*\"travel\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"think\"\n",
      "2025-09-10 11:54:27,356 : INFO : topic #3 (0.347): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\" + 0.005*\"years\"\n",
      "2025-09-10 11:54:27,357 : INFO : topic #4 (0.071): 0.012*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"idea\" + 0.006*\"people\"\n",
      "2025-09-10 11:54:27,357 : INFO : topic diff=0.169393, rho=0.263372\n",
      "2025-09-10 11:54:28,634 : INFO : -8.640 per-word bound, 399.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:54:28,634 : INFO : PROGRESS: pass 6, at document #14000/14833\n",
      "2025-09-10 11:54:29,354 : INFO : optimized alpha [0.31502295, 0.06619697, 0.074720785, 0.37658074, 0.074373394]\n",
      "2025-09-10 11:54:29,359 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:29,365 : INFO : topic #0 (0.315): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:29,366 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"people\" + 0.005*\"apple\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:29,367 : INFO : topic #2 (0.075): 0.022*\"would\" + 0.010*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"speed\"\n",
      "2025-09-10 11:54:29,368 : INFO : topic #3 (0.377): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:29,368 : INFO : topic #4 (0.074): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.007*\"something\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"people\"\n",
      "2025-09-10 11:54:29,369 : INFO : topic diff=0.357916, rho=0.263372\n",
      "2025-09-10 11:54:29,892 : INFO : -8.492 per-word bound, 360.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:54:29,892 : INFO : PROGRESS: pass 6, at document #14833/14833\n",
      "2025-09-10 11:54:30,169 : INFO : optimized alpha [0.27635294, 0.06796334, 0.077844895, 0.40197822, 0.07494406]\n",
      "2025-09-10 11:54:30,174 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:30,180 : INFO : topic #0 (0.276): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:30,181 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"teeth\" + 0.008*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:54:30,182 : INFO : topic #2 (0.078): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:54:30,183 : INFO : topic #3 (0.402): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"would\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:30,184 : INFO : topic #4 (0.075): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"people\" + 0.005*\"question\"\n",
      "2025-09-10 11:54:30,184 : INFO : topic diff=0.278832, rho=0.263372\n",
      "2025-09-10 11:54:31,643 : INFO : -8.025 per-word bound, 260.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:54:31,644 : INFO : PROGRESS: pass 7, at document #2000/14833\n",
      "2025-09-10 11:54:32,388 : INFO : optimized alpha [0.3084454, 0.06655398, 0.0763522, 0.33278054, 0.07583617]\n",
      "2025-09-10 11:54:32,393 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:32,399 : INFO : topic #0 (0.308): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:54:32,400 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"company\" + 0.007*\"business\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:32,401 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:54:32,401 : INFO : topic #3 (0.333): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:32,402 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\" + 0.006*\"time\"\n",
      "2025-09-10 11:54:32,403 : INFO : topic diff=0.251544, rho=0.254687\n",
      "2025-09-10 11:54:33,748 : INFO : -7.896 per-word bound, 238.2 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:54:33,749 : INFO : PROGRESS: pass 7, at document #4000/14833\n",
      "2025-09-10 11:54:34,480 : INFO : optimized alpha [0.34083834, 0.06558136, 0.07433344, 0.33464938, 0.07600777]\n",
      "2025-09-10 11:54:34,485 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:34,491 : INFO : topic #0 (0.341): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:34,492 : INFO : topic #1 (0.066): 0.008*\"language\" + 0.008*\"money\" + 0.007*\"sleep\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:34,493 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"students\" + 0.003*\"war\"\n",
      "2025-09-10 11:54:34,494 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:54:34,495 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"maybe\"\n",
      "2025-09-10 11:54:34,495 : INFO : topic diff=0.155020, rho=0.254687\n",
      "2025-09-10 11:54:35,767 : INFO : -7.792 per-word bound, 221.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:54:35,768 : INFO : PROGRESS: pass 7, at document #6000/14833\n",
      "2025-09-10 11:54:36,477 : INFO : optimized alpha [0.36271992, 0.06497285, 0.0709915, 0.35584027, 0.07267542]\n",
      "2025-09-10 11:54:36,482 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:36,488 : INFO : topic #0 (0.363): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:36,489 : INFO : topic #1 (0.065): 0.008*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:36,490 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.008*\"time\" + 0.008*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:36,491 : INFO : topic #3 (0.356): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:54:36,491 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:54:36,492 : INFO : topic diff=0.151658, rho=0.254687\n",
      "2025-09-10 11:54:37,694 : INFO : -8.028 per-word bound, 261.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:54:37,695 : INFO : PROGRESS: pass 7, at document #8000/14833\n",
      "2025-09-10 11:54:38,415 : INFO : optimized alpha [0.36135024, 0.06498948, 0.07075545, 0.35508513, 0.07306618]\n",
      "2025-09-10 11:54:38,420 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:38,426 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:54:38,427 : INFO : topic #1 (0.065): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:38,428 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"students\" + 0.004*\"think\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:38,429 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:38,430 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"maybe\" + 0.006*\"really\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:38,430 : INFO : topic diff=0.155982, rho=0.254687\n",
      "2025-09-10 11:54:39,620 : INFO : -8.086 per-word bound, 271.6 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:54:39,621 : INFO : PROGRESS: pass 7, at document #10000/14833\n",
      "2025-09-10 11:54:40,300 : INFO : optimized alpha [0.34697047, 0.06488692, 0.069981754, 0.3509592, 0.073667966]\n",
      "2025-09-10 11:54:40,305 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:40,311 : INFO : topic #0 (0.347): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:54:40,312 : INFO : topic #1 (0.065): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"actually\" + 0.005*\"like\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:54:40,313 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"might\"\n",
      "2025-09-10 11:54:40,314 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"made\"\n",
      "2025-09-10 11:54:40,315 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.006*\"death\" + 0.006*\"one\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"question\" + 0.006*\"idea\"\n",
      "2025-09-10 11:54:40,315 : INFO : topic diff=0.148385, rho=0.254687\n",
      "2025-09-10 11:54:41,462 : INFO : -8.133 per-word bound, 280.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:54:41,462 : INFO : PROGRESS: pass 7, at document #12000/14833\n",
      "2025-09-10 11:54:42,100 : INFO : optimized alpha [0.33800587, 0.064793855, 0.06910311, 0.34798113, 0.07135573]\n",
      "2025-09-10 11:54:42,105 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:42,111 : INFO : topic #0 (0.338): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:42,112 : INFO : topic #1 (0.065): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"language\"\n",
      "2025-09-10 11:54:42,113 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"even\" + 0.005*\"travel\" + 0.005*\"like\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"math\"\n",
      "2025-09-10 11:54:42,113 : INFO : topic #3 (0.348): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"made\" + 0.005*\"still\" + 0.005*\"years\"\n",
      "2025-09-10 11:54:42,114 : INFO : topic #4 (0.071): 0.012*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\" + 0.006*\"people\"\n",
      "2025-09-10 11:54:42,115 : INFO : topic diff=0.162419, rho=0.254687\n",
      "2025-09-10 11:54:43,376 : INFO : -8.628 per-word bound, 395.5 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:54:43,377 : INFO : PROGRESS: pass 7, at document #14000/14833\n",
      "2025-09-10 11:54:44,095 : INFO : optimized alpha [0.32843173, 0.06705995, 0.07428176, 0.37649095, 0.07493901]\n",
      "2025-09-10 11:54:44,101 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:44,106 : INFO : topic #0 (0.328): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:44,107 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:44,108 : INFO : topic #2 (0.074): 0.022*\"would\" + 0.009*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:54:44,109 : INFO : topic #3 (0.376): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:44,109 : INFO : topic #4 (0.075): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.007*\"something\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"time\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:54:44,110 : INFO : topic diff=0.340825, rho=0.254687\n",
      "2025-09-10 11:54:44,632 : INFO : -8.480 per-word bound, 357.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:54:44,633 : INFO : PROGRESS: pass 7, at document #14833/14833\n",
      "2025-09-10 11:54:44,909 : INFO : optimized alpha [0.28766474, 0.06884455, 0.077239335, 0.40079907, 0.07533474]\n",
      "2025-09-10 11:54:44,914 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:44,920 : INFO : topic #0 (0.288): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:44,920 : INFO : topic #1 (0.069): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.005*\"like\"\n",
      "2025-09-10 11:54:44,921 : INFO : topic #2 (0.077): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"travel\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"like\" + 0.003*\"back\"\n",
      "2025-09-10 11:54:44,922 : INFO : topic #3 (0.401): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:44,923 : INFO : topic #4 (0.075): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"something\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"time\" + 0.005*\"people\"\n",
      "2025-09-10 11:54:44,924 : INFO : topic diff=0.265329, rho=0.254687\n",
      "2025-09-10 11:54:46,374 : INFO : -8.019 per-word bound, 259.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:54:46,375 : INFO : PROGRESS: pass 8, at document #2000/14833\n",
      "2025-09-10 11:54:47,106 : INFO : optimized alpha [0.31947345, 0.06743622, 0.07587227, 0.33437213, 0.07626056]\n",
      "2025-09-10 11:54:47,111 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:47,117 : INFO : topic #0 (0.319): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:54:47,117 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.007*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:54:47,118 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:54:47,119 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:47,120 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"god\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"time\" + 0.006*\"black\" + 0.006*\"idea\"\n",
      "2025-09-10 11:54:47,120 : INFO : topic diff=0.240309, rho=0.246808\n",
      "2025-09-10 11:54:48,464 : INFO : -7.894 per-word bound, 237.8 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:54:48,464 : INFO : PROGRESS: pass 8, at document #4000/14833\n",
      "2025-09-10 11:54:49,228 : INFO : optimized alpha [0.35176688, 0.066469185, 0.07403033, 0.33595663, 0.07648657]\n",
      "2025-09-10 11:54:49,235 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:49,241 : INFO : topic #0 (0.352): 0.013*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"might\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:49,242 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"language\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:49,242 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"war\"\n",
      "2025-09-10 11:54:49,243 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:54:49,244 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"time\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"idea\"\n",
      "2025-09-10 11:54:49,245 : INFO : topic diff=0.148248, rho=0.246808\n",
      "2025-09-10 11:54:50,516 : INFO : -7.790 per-word bound, 221.4 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:54:50,517 : INFO : PROGRESS: pass 8, at document #6000/14833\n",
      "2025-09-10 11:54:51,228 : INFO : optimized alpha [0.3738808, 0.065880746, 0.070837215, 0.35606453, 0.073175974]\n",
      "2025-09-10 11:54:51,234 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:51,240 : INFO : topic #0 (0.374): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:54:51,241 : INFO : topic #1 (0.066): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:51,242 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.008*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:51,243 : INFO : topic #3 (0.356): 0.013*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.006*\"day\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"got\" + 0.005*\"little\"\n",
      "2025-09-10 11:54:51,244 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"god\" + 0.006*\"time\" + 0.006*\"life\"\n",
      "2025-09-10 11:54:51,244 : INFO : topic diff=0.145664, rho=0.246808\n",
      "2025-09-10 11:54:52,455 : INFO : -8.026 per-word bound, 260.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:54:52,456 : INFO : PROGRESS: pass 8, at document #8000/14833\n",
      "2025-09-10 11:54:53,145 : INFO : optimized alpha [0.37271425, 0.06589877, 0.07065977, 0.3554057, 0.07359361]\n",
      "2025-09-10 11:54:53,150 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:53,156 : INFO : topic #0 (0.373): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:54:53,156 : INFO : topic #1 (0.066): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:54:53,157 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"students\" + 0.004*\"think\" + 0.004*\"back\"\n",
      "2025-09-10 11:54:53,158 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:53,159 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"time\" + 0.006*\"maybe\" + 0.006*\"really\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:53,159 : INFO : topic diff=0.148605, rho=0.246808\n",
      "2025-09-10 11:54:54,345 : INFO : -8.084 per-word bound, 271.3 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:54:54,346 : INFO : PROGRESS: pass 8, at document #10000/14833\n",
      "2025-09-10 11:54:55,019 : INFO : optimized alpha [0.35845032, 0.065794684, 0.06990081, 0.3512597, 0.07419497]\n",
      "2025-09-10 11:54:55,024 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:55,030 : INFO : topic #0 (0.358): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:54:55,031 : INFO : topic #1 (0.066): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"actually\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:54:55,032 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.007*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"think\" + 0.004*\"might\"\n",
      "2025-09-10 11:54:55,033 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"years\" + 0.005*\"made\"\n",
      "2025-09-10 11:54:55,034 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"question\"\n",
      "2025-09-10 11:54:55,034 : INFO : topic diff=0.142361, rho=0.246808\n",
      "2025-09-10 11:54:56,174 : INFO : -8.130 per-word bound, 280.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:54:56,175 : INFO : PROGRESS: pass 8, at document #12000/14833\n",
      "2025-09-10 11:54:56,808 : INFO : optimized alpha [0.34933376, 0.06570086, 0.06900519, 0.3485235, 0.07182441]\n",
      "2025-09-10 11:54:56,813 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:56,819 : INFO : topic #0 (0.349): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:54:56,820 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"like\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"language\"\n",
      "2025-09-10 11:54:56,820 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"math\"\n",
      "2025-09-10 11:54:56,821 : INFO : topic #3 (0.349): 0.013*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"still\" + 0.005*\"years\" + 0.005*\"made\"\n",
      "2025-09-10 11:54:56,822 : INFO : topic #4 (0.072): 0.012*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\" + 0.006*\"people\"\n",
      "2025-09-10 11:54:56,823 : INFO : topic diff=0.156422, rho=0.246808\n",
      "2025-09-10 11:54:58,087 : INFO : -8.617 per-word bound, 392.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:54:58,089 : INFO : PROGRESS: pass 8, at document #14000/14833\n",
      "2025-09-10 11:54:58,800 : INFO : optimized alpha [0.33867756, 0.06794546, 0.07399737, 0.37615877, 0.075332]\n",
      "2025-09-10 11:54:58,805 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:58,810 : INFO : topic #0 (0.339): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:58,811 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:58,812 : INFO : topic #2 (0.074): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:54:58,813 : INFO : topic #3 (0.376): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:54:58,814 : INFO : topic #4 (0.075): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.007*\"something\" + 0.007*\"black\" + 0.007*\"time\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:54:58,814 : INFO : topic diff=0.325779, rho=0.246808\n",
      "2025-09-10 11:54:59,336 : INFO : -8.469 per-word bound, 354.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:54:59,337 : INFO : PROGRESS: pass 8, at document #14833/14833\n",
      "2025-09-10 11:54:59,615 : INFO : optimized alpha [0.29681113, 0.069695115, 0.07686743, 0.39958274, 0.07562429]\n",
      "2025-09-10 11:54:59,620 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:54:59,626 : INFO : topic #0 (0.297): 0.017*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:54:59,627 : INFO : topic #1 (0.070): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:54:59,628 : INFO : topic #2 (0.077): 0.020*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.005*\"even\" + 0.004*\"travel\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\"\n",
      "2025-09-10 11:54:59,629 : INFO : topic #3 (0.400): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:54:59,630 : INFO : topic #4 (0.076): 0.009*\"like\" + 0.009*\"death\" + 0.008*\"god\" + 0.007*\"black\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"time\" + 0.006*\"think\" + 0.005*\"question\"\n",
      "2025-09-10 11:54:59,630 : INFO : topic diff=0.253676, rho=0.246808\n",
      "2025-09-10 11:55:01,080 : INFO : -8.014 per-word bound, 258.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:55:01,080 : INFO : PROGRESS: pass 9, at document #2000/14833\n",
      "2025-09-10 11:55:01,806 : INFO : optimized alpha [0.32802877, 0.0682468, 0.07554818, 0.33536935, 0.07653873]\n",
      "2025-09-10 11:55:01,811 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:01,817 : INFO : topic #0 (0.328): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:01,818 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.007*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:55:01,819 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:01,820 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:01,820 : INFO : topic #4 (0.077): 0.011*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"god\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:01,821 : INFO : topic diff=0.230565, rho=0.239618\n",
      "2025-09-10 11:55:03,160 : INFO : -7.892 per-word bound, 237.5 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:55:03,160 : INFO : PROGRESS: pass 9, at document #4000/14833\n",
      "2025-09-10 11:55:03,880 : INFO : optimized alpha [0.36008242, 0.06734191, 0.073793165, 0.3366496, 0.07685068]\n",
      "2025-09-10 11:55:03,885 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:03,891 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:03,892 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"language\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:03,893 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"war\"\n",
      "2025-09-10 11:55:03,894 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:55:03,895 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.007*\"time\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"maybe\"\n",
      "2025-09-10 11:55:03,895 : INFO : topic diff=0.142423, rho=0.239618\n",
      "2025-09-10 11:55:05,153 : INFO : -7.789 per-word bound, 221.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:55:05,153 : INFO : PROGRESS: pass 9, at document #6000/14833\n",
      "2025-09-10 11:55:05,858 : INFO : optimized alpha [0.3823577, 0.066726044, 0.07071523, 0.35583925, 0.07366477]\n",
      "2025-09-10 11:55:05,863 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:05,870 : INFO : topic #0 (0.382): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"even\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:05,871 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:05,871 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"war\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\"\n",
      "2025-09-10 11:55:05,872 : INFO : topic #3 (0.356): 0.012*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:05,873 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"time\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"god\"\n",
      "2025-09-10 11:55:05,874 : INFO : topic diff=0.140367, rho=0.239618\n",
      "2025-09-10 11:55:07,072 : INFO : -8.025 per-word bound, 260.4 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:55:07,073 : INFO : PROGRESS: pass 9, at document #8000/14833\n",
      "2025-09-10 11:55:07,756 : INFO : optimized alpha [0.38133922, 0.06669359, 0.070564404, 0.35519844, 0.074068874]\n",
      "2025-09-10 11:55:07,761 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:07,767 : INFO : topic #0 (0.381): 0.014*\"people\" + 0.014*\"like\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"also\" + 0.007*\"things\"\n",
      "2025-09-10 11:55:07,768 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:07,769 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"think\"\n",
      "2025-09-10 11:55:07,770 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:07,771 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"death\" + 0.007*\"time\" + 0.006*\"one\" + 0.006*\"sound\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"really\"\n",
      "2025-09-10 11:55:07,772 : INFO : topic diff=0.142334, rho=0.239618\n",
      "2025-09-10 11:55:08,955 : INFO : -8.082 per-word bound, 271.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:55:08,956 : INFO : PROGRESS: pass 9, at document #10000/14833\n",
      "2025-09-10 11:55:09,622 : INFO : optimized alpha [0.36729306, 0.066597186, 0.06981185, 0.3510086, 0.07470053]\n",
      "2025-09-10 11:55:09,627 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:09,633 : INFO : topic #0 (0.367): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:55:09,634 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"actually\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"company\"\n",
      "2025-09-10 11:55:09,635 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.005*\"back\" + 0.004*\"students\" + 0.004*\"think\"\n",
      "2025-09-10 11:55:09,636 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"even\" + 0.005*\"day\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:09,637 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"one\" + 0.006*\"time\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:55:09,637 : INFO : topic diff=0.137101, rho=0.239618\n",
      "2025-09-10 11:55:10,774 : INFO : -8.128 per-word bound, 279.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:55:10,775 : INFO : PROGRESS: pass 9, at document #12000/14833\n",
      "2025-09-10 11:55:11,416 : INFO : optimized alpha [0.35826102, 0.06649076, 0.06891998, 0.34852242, 0.072363734]\n",
      "2025-09-10 11:55:11,421 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:11,427 : INFO : topic #0 (0.358): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:55:11,429 : INFO : topic #1 (0.066): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"even\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"actually\"\n",
      "2025-09-10 11:55:11,429 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"math\"\n",
      "2025-09-10 11:55:11,430 : INFO : topic #3 (0.349): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:11,431 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"time\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:11,432 : INFO : topic diff=0.150999, rho=0.239618\n",
      "2025-09-10 11:55:12,697 : INFO : -8.607 per-word bound, 390.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:55:12,698 : INFO : PROGRESS: pass 9, at document #14000/14833\n",
      "2025-09-10 11:55:13,408 : INFO : optimized alpha [0.34696195, 0.068699986, 0.07376679, 0.3753147, 0.07572963]\n",
      "2025-09-10 11:55:13,414 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:13,420 : INFO : topic #0 (0.347): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:13,421 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:13,422 : INFO : topic #2 (0.074): 0.022*\"would\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:55:13,423 : INFO : topic #3 (0.375): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:13,424 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.008*\"time\" + 0.007*\"something\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:13,424 : INFO : topic diff=0.312098, rho=0.239618\n",
      "2025-09-10 11:55:13,975 : INFO : -8.461 per-word bound, 352.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:55:13,976 : INFO : PROGRESS: pass 9, at document #14833/14833\n",
      "2025-09-10 11:55:14,255 : INFO : optimized alpha [0.30448398, 0.0704367, 0.076489, 0.39793602, 0.07577356]\n",
      "2025-09-10 11:55:14,261 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:14,267 : INFO : topic #0 (0.304): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:14,268 : INFO : topic #1 (0.070): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:14,269 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.004*\"travel\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\"\n",
      "2025-09-10 11:55:14,270 : INFO : topic #3 (0.398): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:14,270 : INFO : topic #4 (0.076): 0.009*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.007*\"time\" + 0.007*\"black\" + 0.007*\"one\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:14,271 : INFO : topic diff=0.243790, rho=0.239618\n",
      "2025-09-10 11:55:15,756 : INFO : -8.009 per-word bound, 257.6 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:55:15,757 : INFO : PROGRESS: pass 10, at document #2000/14833\n",
      "2025-09-10 11:55:16,497 : INFO : optimized alpha [0.33517355, 0.06900096, 0.07526764, 0.33586124, 0.07669955]\n",
      "2025-09-10 11:55:16,502 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:16,508 : INFO : topic #0 (0.335): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:16,509 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.007*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:55:16,510 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:16,511 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:16,512 : INFO : topic #4 (0.077): 0.011*\"like\" + 0.008*\"death\" + 0.008*\"time\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"think\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:16,512 : INFO : topic diff=0.221948, rho=0.233022\n",
      "2025-09-10 11:55:17,874 : INFO : -7.890 per-word bound, 237.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:55:17,874 : INFO : PROGRESS: pass 10, at document #4000/14833\n",
      "2025-09-10 11:55:18,592 : INFO : optimized alpha [0.36689186, 0.06813113, 0.073556766, 0.33690608, 0.07703768]\n",
      "2025-09-10 11:55:18,597 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:18,603 : INFO : topic #0 (0.367): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"life\" + 0.007*\"even\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:18,604 : INFO : topic #1 (0.068): 0.008*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:55:18,605 : INFO : topic #2 (0.074): 0.023*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"travel\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"war\"\n",
      "2025-09-10 11:55:18,606 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:55:18,607 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"time\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"maybe\"\n",
      "2025-09-10 11:55:18,608 : INFO : topic diff=0.137403, rho=0.233022\n",
      "2025-09-10 11:55:19,867 : INFO : -7.788 per-word bound, 220.9 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:55:19,868 : INFO : PROGRESS: pass 10, at document #6000/14833\n",
      "2025-09-10 11:55:20,566 : INFO : optimized alpha [0.38921434, 0.067490384, 0.07061097, 0.3553413, 0.07390059]\n",
      "2025-09-10 11:55:20,572 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:20,577 : INFO : topic #0 (0.389): 0.015*\"like\" + 0.014*\"people\" + 0.013*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:20,578 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:20,579 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"back\"\n",
      "2025-09-10 11:55:20,580 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.012*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:20,581 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"time\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"idea\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:20,581 : INFO : topic diff=0.135652, rho=0.233022\n",
      "2025-09-10 11:55:21,771 : INFO : -8.023 per-word bound, 260.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:55:21,772 : INFO : PROGRESS: pass 10, at document #8000/14833\n",
      "2025-09-10 11:55:22,456 : INFO : optimized alpha [0.3882964, 0.06742813, 0.07046297, 0.3547193, 0.074292414]\n",
      "2025-09-10 11:55:22,461 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:22,467 : INFO : topic #0 (0.388): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:22,468 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:22,469 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:55:22,470 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:22,471 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"time\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"sound\" + 0.006*\"way\" + 0.006*\"really\"\n",
      "2025-09-10 11:55:22,471 : INFO : topic diff=0.136863, rho=0.233022\n",
      "2025-09-10 11:55:23,653 : INFO : -8.081 per-word bound, 270.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:55:23,654 : INFO : PROGRESS: pass 10, at document #10000/14833\n",
      "2025-09-10 11:55:24,316 : INFO : optimized alpha [0.3745813, 0.06734422, 0.06970266, 0.35072833, 0.074872255]\n",
      "2025-09-10 11:55:24,321 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:24,327 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:55:24,328 : INFO : topic #1 (0.067): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"actually\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\"\n",
      "2025-09-10 11:55:24,328 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"time\" + 0.006*\"even\" + 0.006*\"could\" + 0.005*\"like\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"think\"\n",
      "2025-09-10 11:55:24,329 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:24,330 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"time\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:55:24,330 : INFO : topic diff=0.132550, rho=0.233022\n",
      "2025-09-10 11:55:25,467 : INFO : -8.126 per-word bound, 279.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:55:25,468 : INFO : PROGRESS: pass 10, at document #12000/14833\n",
      "2025-09-10 11:55:26,094 : INFO : optimized alpha [0.36559188, 0.06722067, 0.06882163, 0.3484115, 0.07252927]\n",
      "2025-09-10 11:55:26,099 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:26,105 : INFO : topic #0 (0.366): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:55:26,106 : INFO : topic #1 (0.067): 0.008*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"language\" + 0.005*\"company\"\n",
      "2025-09-10 11:55:26,107 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.005*\"like\" + 0.005*\"students\" + 0.004*\"back\" + 0.004*\"math\"\n",
      "2025-09-10 11:55:26,108 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:26,108 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"way\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:26,109 : INFO : topic diff=0.146278, rho=0.233022\n",
      "2025-09-10 11:55:27,362 : INFO : -8.599 per-word bound, 387.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:55:27,363 : INFO : PROGRESS: pass 10, at document #14000/14833\n",
      "2025-09-10 11:55:28,090 : INFO : optimized alpha [0.3537479, 0.06938363, 0.07350524, 0.3743226, 0.075785585]\n",
      "2025-09-10 11:55:28,095 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:28,101 : INFO : topic #0 (0.354): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:28,102 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"apple\" + 0.005*\"people\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:28,103 : INFO : topic #2 (0.074): 0.022*\"would\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:55:28,104 : INFO : topic #3 (0.374): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:28,105 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.008*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"black\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:28,105 : INFO : topic diff=0.300134, rho=0.233022\n",
      "2025-09-10 11:55:28,626 : INFO : -8.455 per-word bound, 350.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:55:28,627 : INFO : PROGRESS: pass 10, at document #14833/14833\n",
      "2025-09-10 11:55:28,904 : INFO : optimized alpha [0.3109072, 0.07105237, 0.076132, 0.3962728, 0.07571756]\n",
      "2025-09-10 11:55:28,909 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:28,914 : INFO : topic #0 (0.311): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:28,915 : INFO : topic #1 (0.071): 0.010*\"money\" + 0.008*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:28,916 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\"\n",
      "2025-09-10 11:55:28,917 : INFO : topic #3 (0.396): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:28,918 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"god\" + 0.008*\"time\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:28,918 : INFO : topic diff=0.235190, rho=0.233022\n",
      "2025-09-10 11:55:30,373 : INFO : -8.004 per-word bound, 256.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:55:30,374 : INFO : PROGRESS: pass 11, at document #2000/14833\n",
      "2025-09-10 11:55:31,103 : INFO : optimized alpha [0.3409906, 0.06959356, 0.07496295, 0.33613896, 0.07665103]\n",
      "2025-09-10 11:55:31,108 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:31,114 : INFO : topic #0 (0.341): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"might\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:31,115 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:55:31,116 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"human\"\n",
      "2025-09-10 11:55:31,116 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:31,117 : INFO : topic #4 (0.077): 0.012*\"like\" + 0.008*\"time\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:31,118 : INFO : topic diff=0.214296, rho=0.226942\n",
      "2025-09-10 11:55:32,460 : INFO : -7.888 per-word bound, 236.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:55:32,461 : INFO : PROGRESS: pass 11, at document #4000/14833\n",
      "2025-09-10 11:55:33,180 : INFO : optimized alpha [0.37227088, 0.06874921, 0.07333423, 0.33697432, 0.0769843]\n",
      "2025-09-10 11:55:33,185 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:33,191 : INFO : topic #0 (0.372): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:33,192 : INFO : topic #1 (0.069): 0.008*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"teeth\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:55:33,193 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"time\" + 0.006*\"could\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"war\"\n",
      "2025-09-10 11:55:33,194 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 11:55:33,195 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"time\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"maybe\" + 0.006*\"black\"\n",
      "2025-09-10 11:55:33,195 : INFO : topic diff=0.132933, rho=0.226942\n",
      "2025-09-10 11:55:34,449 : INFO : -7.786 per-word bound, 220.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:55:34,450 : INFO : PROGRESS: pass 11, at document #6000/14833\n",
      "2025-09-10 11:55:35,156 : INFO : optimized alpha [0.39455187, 0.068139404, 0.07047216, 0.3548252, 0.07389778]\n",
      "2025-09-10 11:55:35,163 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:35,170 : INFO : topic #0 (0.395): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:55:35,171 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:35,171 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"travel\" + 0.004*\"war\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:35,172 : INFO : topic #3 (0.355): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:35,173 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:35,174 : INFO : topic diff=0.131533, rho=0.226942\n",
      "2025-09-10 11:55:36,377 : INFO : -8.021 per-word bound, 259.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:55:36,377 : INFO : PROGRESS: pass 11, at document #8000/14833\n",
      "2025-09-10 11:55:37,054 : INFO : optimized alpha [0.39363524, 0.06808011, 0.070341125, 0.35410744, 0.074284375]\n",
      "2025-09-10 11:55:37,059 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:37,065 : INFO : topic #0 (0.394): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:37,065 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:55:37,066 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"war\"\n",
      "2025-09-10 11:55:37,067 : INFO : topic #3 (0.354): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:37,068 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.008*\"time\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"sound\" + 0.006*\"way\" + 0.006*\"really\"\n",
      "2025-09-10 11:55:37,069 : INFO : topic diff=0.131970, rho=0.226942\n",
      "2025-09-10 11:55:38,248 : INFO : -8.079 per-word bound, 270.4 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:55:38,249 : INFO : PROGRESS: pass 11, at document #10000/14833\n",
      "2025-09-10 11:55:38,943 : INFO : optimized alpha [0.3802492, 0.06799625, 0.06955574, 0.350277, 0.074837625]\n",
      "2025-09-10 11:55:38,948 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:38,955 : INFO : topic #0 (0.380): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:55:38,956 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"actually\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\"\n",
      "2025-09-10 11:55:38,957 : INFO : topic #2 (0.070): 0.024*\"would\" + 0.007*\"one\" + 0.006*\"travel\" + 0.006*\"even\" + 0.006*\"time\" + 0.006*\"could\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"might\"\n",
      "2025-09-10 11:55:38,958 : INFO : topic #3 (0.350): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:38,959 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"time\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:55:38,959 : INFO : topic diff=0.128499, rho=0.226942\n",
      "2025-09-10 11:55:40,105 : INFO : -8.124 per-word bound, 278.9 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:55:40,106 : INFO : PROGRESS: pass 11, at document #12000/14833\n",
      "2025-09-10 11:55:40,738 : INFO : optimized alpha [0.3712968, 0.06783792, 0.06866523, 0.34811687, 0.07256615]\n",
      "2025-09-10 11:55:40,743 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:40,749 : INFO : topic #0 (0.371): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:55:40,750 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"language\" + 0.005*\"company\"\n",
      "2025-09-10 11:55:40,750 : INFO : topic #2 (0.069): 0.020*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"math\"\n",
      "2025-09-10 11:55:40,751 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:40,752 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.007*\"time\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:40,752 : INFO : topic diff=0.141896, rho=0.226942\n",
      "2025-09-10 11:55:42,017 : INFO : -8.592 per-word bound, 385.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:55:42,018 : INFO : PROGRESS: pass 11, at document #14000/14833\n",
      "2025-09-10 11:55:42,725 : INFO : optimized alpha [0.35906765, 0.06995026, 0.07321177, 0.373272, 0.075728446]\n",
      "2025-09-10 11:55:42,731 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:42,737 : INFO : topic #0 (0.359): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:42,738 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.005*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:55:42,739 : INFO : topic #2 (0.073): 0.022*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.006*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"back\" + 0.004*\"speed\"\n",
      "2025-09-10 11:55:42,740 : INFO : topic #3 (0.373): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:42,741 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.009*\"time\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"black\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:42,741 : INFO : topic diff=0.289348, rho=0.226942\n",
      "2025-09-10 11:55:43,269 : INFO : -8.449 per-word bound, 349.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:55:43,270 : INFO : PROGRESS: pass 11, at document #14833/14833\n",
      "2025-09-10 11:55:43,550 : INFO : optimized alpha [0.31633586, 0.071610324, 0.07577441, 0.39468086, 0.07567253]\n",
      "2025-09-10 11:55:43,555 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:43,561 : INFO : topic #0 (0.316): 0.016*\"people\" + 0.011*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:43,562 : INFO : topic #1 (0.072): 0.010*\"money\" + 0.007*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.005*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:43,563 : INFO : topic #2 (0.076): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"travel\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\"\n",
      "2025-09-10 11:55:43,564 : INFO : topic #3 (0.395): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:43,565 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.008*\"death\" + 0.008*\"time\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"life\" + 0.006*\"think\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:43,565 : INFO : topic diff=0.227592, rho=0.226942\n",
      "2025-09-10 11:55:45,014 : INFO : -8.001 per-word bound, 256.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:55:45,015 : INFO : PROGRESS: pass 12, at document #2000/14833\n",
      "2025-09-10 11:55:45,738 : INFO : optimized alpha [0.34583098, 0.07017527, 0.07465497, 0.33645108, 0.07663825]\n",
      "2025-09-10 11:55:45,743 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:45,749 : INFO : topic #0 (0.346): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:45,750 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"language\" + 0.007*\"business\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:55:45,751 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"human\"\n",
      "2025-09-10 11:55:45,752 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:45,753 : INFO : topic #4 (0.077): 0.012*\"like\" + 0.009*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:45,753 : INFO : topic diff=0.207381, rho=0.221314\n",
      "2025-09-10 11:55:47,097 : INFO : -7.886 per-word bound, 236.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:55:47,098 : INFO : PROGRESS: pass 12, at document #4000/14833\n",
      "2025-09-10 11:55:47,819 : INFO : optimized alpha [0.37663862, 0.06935972, 0.07308826, 0.33711877, 0.07700147]\n",
      "2025-09-10 11:55:47,825 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:47,831 : INFO : topic #0 (0.377): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:55:47,832 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:55:47,833 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:47,834 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:47,835 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"time\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"death\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"maybe\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:47,835 : INFO : topic diff=0.128880, rho=0.221314\n",
      "2025-09-10 11:55:49,092 : INFO : -7.785 per-word bound, 220.5 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:55:49,093 : INFO : PROGRESS: pass 12, at document #6000/14833\n",
      "2025-09-10 11:55:49,795 : INFO : optimized alpha [0.3987297, 0.06876409, 0.070333496, 0.35441825, 0.074014686]\n",
      "2025-09-10 11:55:49,801 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:49,807 : INFO : topic #0 (0.399): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:55:49,808 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:55:49,809 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"time\" + 0.005*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:49,809 : INFO : topic #3 (0.354): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:49,810 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.008*\"something\" + 0.008*\"time\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:55:49,811 : INFO : topic diff=0.127798, rho=0.221314\n",
      "2025-09-10 11:55:51,011 : INFO : -8.020 per-word bound, 259.6 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:55:51,012 : INFO : PROGRESS: pass 12, at document #8000/14833\n",
      "2025-09-10 11:55:51,694 : INFO : optimized alpha [0.39778244, 0.06867795, 0.07019452, 0.35360897, 0.07440362]\n",
      "2025-09-10 11:55:51,699 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:51,705 : INFO : topic #0 (0.398): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:55:51,706 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:55:51,707 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"time\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"war\" + 0.004*\"back\"\n",
      "2025-09-10 11:55:51,708 : INFO : topic #3 (0.354): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:51,709 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.008*\"time\" + 0.007*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"sound\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:51,709 : INFO : topic diff=0.127549, rho=0.221314\n",
      "2025-09-10 11:55:52,888 : INFO : -8.078 per-word bound, 270.2 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:55:52,888 : INFO : PROGRESS: pass 12, at document #10000/14833\n",
      "2025-09-10 11:55:53,548 : INFO : optimized alpha [0.384655, 0.06859492, 0.069417335, 0.34983885, 0.07493705]\n",
      "2025-09-10 11:55:53,553 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:53,559 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:55:53,560 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"actually\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\"\n",
      "2025-09-10 11:55:53,561 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"time\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"students\" + 0.004*\"might\"\n",
      "2025-09-10 11:55:53,562 : INFO : topic #3 (0.350): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.005*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"said\"\n",
      "2025-09-10 11:55:53,563 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.008*\"time\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:55:53,564 : INFO : topic diff=0.124742, rho=0.221314\n",
      "2025-09-10 11:55:54,698 : INFO : -8.122 per-word bound, 278.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:55:54,699 : INFO : PROGRESS: pass 12, at document #12000/14833\n",
      "2025-09-10 11:55:55,326 : INFO : optimized alpha [0.37576765, 0.06843402, 0.06856676, 0.3477457, 0.07270581]\n",
      "2025-09-10 11:55:55,331 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:55,337 : INFO : topic #0 (0.376): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:55:55,338 : INFO : topic #1 (0.068): 0.009*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"language\"\n",
      "2025-09-10 11:55:55,339 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"students\" + 0.004*\"travel\" + 0.004*\"like\" + 0.004*\"math\" + 0.004*\"back\"\n",
      "2025-09-10 11:55:55,340 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:55:55,341 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.008*\"think\" + 0.008*\"time\" + 0.007*\"maybe\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\"\n",
      "2025-09-10 11:55:55,341 : INFO : topic diff=0.137814, rho=0.221314\n",
      "2025-09-10 11:55:56,599 : INFO : -8.585 per-word bound, 384.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:55:56,600 : INFO : PROGRESS: pass 12, at document #14000/14833\n",
      "2025-09-10 11:55:57,297 : INFO : optimized alpha [0.36328703, 0.07053852, 0.07299751, 0.37222195, 0.0757874]\n",
      "2025-09-10 11:55:57,302 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:57,308 : INFO : topic #0 (0.363): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:57,309 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:55:57,310 : INFO : topic #2 (0.073): 0.022*\"would\" + 0.007*\"one\" + 0.006*\"time\" + 0.005*\"travel\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"speed\"\n",
      "2025-09-10 11:55:57,311 : INFO : topic #3 (0.372): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"back\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:55:57,311 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.009*\"time\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"black\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:57,312 : INFO : topic diff=0.279659, rho=0.221314\n",
      "2025-09-10 11:55:57,830 : INFO : -8.445 per-word bound, 348.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:55:57,831 : INFO : PROGRESS: pass 12, at document #14833/14833\n",
      "2025-09-10 11:55:58,105 : INFO : optimized alpha [0.32086152, 0.07215766, 0.07547744, 0.39310795, 0.07565869]\n",
      "2025-09-10 11:55:58,110 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:55:58,116 : INFO : topic #0 (0.321): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"life\" + 0.006*\"things\" + 0.006*\"might\"\n",
      "2025-09-10 11:55:58,117 : INFO : topic #1 (0.072): 0.010*\"money\" + 0.007*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:55:58,117 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"travel\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 11:55:58,118 : INFO : topic #3 (0.393): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:55:58,119 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.009*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"even\"\n",
      "2025-09-10 11:55:58,120 : INFO : topic diff=0.220874, rho=0.221314\n",
      "2025-09-10 11:55:59,546 : INFO : -7.997 per-word bound, 255.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:55:59,546 : INFO : PROGRESS: pass 13, at document #2000/14833\n",
      "2025-09-10 11:56:00,264 : INFO : optimized alpha [0.34973788, 0.070731625, 0.0743581, 0.33660218, 0.0766254]\n",
      "2025-09-10 11:56:00,270 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:00,276 : INFO : topic #0 (0.350): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:00,277 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.007*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:56:00,278 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"time\" + 0.005*\"even\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"human\"\n",
      "2025-09-10 11:56:00,278 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:00,279 : INFO : topic #4 (0.077): 0.012*\"like\" + 0.009*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:00,280 : INFO : topic diff=0.201067, rho=0.216086\n",
      "2025-09-10 11:56:01,600 : INFO : -7.885 per-word bound, 236.3 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:56:01,600 : INFO : PROGRESS: pass 13, at document #4000/14833\n",
      "2025-09-10 11:56:02,307 : INFO : optimized alpha [0.38000444, 0.06991268, 0.07284969, 0.33712593, 0.07700973]\n",
      "2025-09-10 11:56:02,312 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:02,318 : INFO : topic #0 (0.380): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:02,320 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:56:02,320 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"travel\" + 0.005*\"time\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"like\"\n",
      "2025-09-10 11:56:02,321 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:02,322 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"think\" + 0.009*\"time\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"maybe\" + 0.006*\"god\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:02,323 : INFO : topic diff=0.125212, rho=0.216086\n",
      "2025-09-10 11:56:03,558 : INFO : -7.784 per-word bound, 220.4 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:56:03,558 : INFO : PROGRESS: pass 13, at document #6000/14833\n",
      "2025-09-10 11:56:04,249 : INFO : optimized alpha [0.40187395, 0.06932165, 0.07018173, 0.3538233, 0.0740901]\n",
      "2025-09-10 11:56:04,254 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:04,260 : INFO : topic #0 (0.402): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:56:04,261 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:56:04,262 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"time\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"travel\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:04,263 : INFO : topic #3 (0.354): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.006*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:04,263 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.009*\"time\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:04,264 : INFO : topic diff=0.124358, rho=0.216086\n",
      "2025-09-10 11:56:05,450 : INFO : -8.019 per-word bound, 259.4 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:56:05,450 : INFO : PROGRESS: pass 13, at document #8000/14833\n",
      "2025-09-10 11:56:06,120 : INFO : optimized alpha [0.40092215, 0.06921448, 0.07004923, 0.3528965, 0.0744908]\n",
      "2025-09-10 11:56:06,125 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:06,131 : INFO : topic #0 (0.401): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:56:06,133 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:06,134 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"time\" + 0.004*\"travel\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:06,135 : INFO : topic #3 (0.353): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:06,135 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.009*\"time\" + 0.006*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"sound\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:06,136 : INFO : topic diff=0.123534, rho=0.216086\n",
      "2025-09-10 11:56:07,302 : INFO : -8.077 per-word bound, 270.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:56:07,303 : INFO : PROGRESS: pass 13, at document #10000/14833\n",
      "2025-09-10 11:56:07,967 : INFO : optimized alpha [0.38805577, 0.06912684, 0.06928819, 0.34924665, 0.075033695]\n",
      "2025-09-10 11:56:07,972 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:07,978 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:07,979 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:07,980 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"travel\" + 0.005*\"time\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"like\" + 0.004*\"internet\"\n",
      "2025-09-10 11:56:07,981 : INFO : topic #3 (0.349): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"said\"\n",
      "2025-09-10 11:56:07,982 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.008*\"time\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"question\"\n",
      "2025-09-10 11:56:07,982 : INFO : topic diff=0.121407, rho=0.216086\n",
      "2025-09-10 11:56:09,108 : INFO : -8.120 per-word bound, 278.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:56:09,108 : INFO : PROGRESS: pass 13, at document #12000/14833\n",
      "2025-09-10 11:56:09,729 : INFO : optimized alpha [0.37929603, 0.06894045, 0.06845129, 0.34721017, 0.072802715]\n",
      "2025-09-10 11:56:09,734 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:09,740 : INFO : topic #0 (0.379): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:09,741 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.008*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"language\"\n",
      "2025-09-10 11:56:09,742 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"time\" + 0.004*\"like\" + 0.004*\"travel\" + 0.004*\"math\" + 0.004*\"back\"\n",
      "2025-09-10 11:56:09,743 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"made\"\n",
      "2025-09-10 11:56:09,744 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.008*\"time\" + 0.007*\"maybe\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\"\n",
      "2025-09-10 11:56:09,744 : INFO : topic diff=0.134110, rho=0.216086\n",
      "2025-09-10 11:56:10,995 : INFO : -8.579 per-word bound, 382.4 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:56:10,996 : INFO : PROGRESS: pass 13, at document #14000/14833\n",
      "2025-09-10 11:56:11,695 : INFO : optimized alpha [0.3666475, 0.07096829, 0.072771944, 0.37103865, 0.07576012]\n",
      "2025-09-10 11:56:11,701 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:11,707 : INFO : topic #0 (0.367): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:11,708 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:56:11,709 : INFO : topic #2 (0.073): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"time\" + 0.005*\"travel\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"speed\"\n",
      "2025-09-10 11:56:11,709 : INFO : topic #3 (0.371): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:11,710 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:11,710 : INFO : topic diff=0.270861, rho=0.216086\n",
      "2025-09-10 11:56:12,229 : INFO : -8.440 per-word bound, 347.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:56:12,230 : INFO : PROGRESS: pass 13, at document #14833/14833\n",
      "2025-09-10 11:56:12,503 : INFO : optimized alpha [0.32458794, 0.072519846, 0.07512244, 0.39138892, 0.0755757]\n",
      "2025-09-10 11:56:12,508 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:12,514 : INFO : topic #0 (0.325): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:12,515 : INFO : topic #1 (0.073): 0.010*\"money\" + 0.007*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:12,516 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"time\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"travel\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 11:56:12,517 : INFO : topic #3 (0.391): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:56:12,517 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.009*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"think\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:12,518 : INFO : topic diff=0.214633, rho=0.216086\n",
      "2025-09-10 11:56:13,939 : INFO : -7.994 per-word bound, 254.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:56:13,939 : INFO : PROGRESS: pass 14, at document #2000/14833\n",
      "2025-09-10 11:56:14,655 : INFO : optimized alpha [0.35283142, 0.07109338, 0.07402225, 0.3364782, 0.0765363]\n",
      "2025-09-10 11:56:14,660 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:14,666 : INFO : topic #0 (0.353): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"might\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:14,667 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.007*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:56:14,667 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"time\" + 0.004*\"war\" + 0.004*\"travel\" + 0.004*\"back\" + 0.004*\"human\"\n",
      "2025-09-10 11:56:14,668 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:14,669 : INFO : topic #4 (0.077): 0.012*\"like\" + 0.010*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:14,669 : INFO : topic diff=0.195450, rho=0.211211\n",
      "2025-09-10 11:56:15,976 : INFO : -7.883 per-word bound, 236.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:56:15,977 : INFO : PROGRESS: pass 14, at document #4000/14833\n",
      "2025-09-10 11:56:16,673 : INFO : optimized alpha [0.38253212, 0.07029883, 0.07252208, 0.33692786, 0.07695441]\n",
      "2025-09-10 11:56:16,678 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:16,684 : INFO : topic #0 (0.383): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:16,685 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:56:16,686 : INFO : topic #2 (0.073): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"time\" + 0.004*\"travel\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:56:16,687 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:16,687 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\"\n",
      "2025-09-10 11:56:16,688 : INFO : topic diff=0.121944, rho=0.211211\n",
      "2025-09-10 11:56:17,908 : INFO : -7.783 per-word bound, 220.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:56:17,908 : INFO : PROGRESS: pass 14, at document #6000/14833\n",
      "2025-09-10 11:56:18,583 : INFO : optimized alpha [0.40410858, 0.06971782, 0.06993628, 0.35308552, 0.07411722]\n",
      "2025-09-10 11:56:18,588 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:18,594 : INFO : topic #0 (0.404): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:56:18,595 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:56:18,595 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"time\" + 0.004*\"back\" + 0.004*\"travel\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:18,596 : INFO : topic #3 (0.353): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:18,597 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.009*\"time\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:18,598 : INFO : topic diff=0.121210, rho=0.211211\n",
      "2025-09-10 11:56:19,767 : INFO : -8.018 per-word bound, 259.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:56:19,767 : INFO : PROGRESS: pass 14, at document #8000/14833\n",
      "2025-09-10 11:56:20,431 : INFO : optimized alpha [0.40318367, 0.06964237, 0.06982964, 0.35214525, 0.07449539]\n",
      "2025-09-10 11:56:20,436 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:20,442 : INFO : topic #0 (0.403): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:56:20,443 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:20,444 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"time\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"travel\"\n",
      "2025-09-10 11:56:20,444 : INFO : topic #3 (0.352): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"really\" + 0.005*\"know\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:20,445 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"something\" + 0.009*\"time\" + 0.006*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"sound\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:20,446 : INFO : topic diff=0.119902, rho=0.211211\n",
      "2025-09-10 11:56:21,596 : INFO : -8.076 per-word bound, 269.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:56:21,596 : INFO : PROGRESS: pass 14, at document #10000/14833\n",
      "2025-09-10 11:56:22,241 : INFO : optimized alpha [0.390602, 0.06953866, 0.06905411, 0.34862834, 0.07503144]\n",
      "2025-09-10 11:56:22,246 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:22,252 : INFO : topic #0 (0.391): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:22,253 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:22,254 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.004*\"back\" + 0.004*\"students\" + 0.004*\"travel\" + 0.004*\"time\" + 0.004*\"like\" + 0.004*\"internet\"\n",
      "2025-09-10 11:56:22,254 : INFO : topic #3 (0.349): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"really\" + 0.005*\"said\"\n",
      "2025-09-10 11:56:22,255 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.009*\"time\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.006*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:56:22,256 : INFO : topic diff=0.118277, rho=0.211211\n",
      "2025-09-10 11:56:23,365 : INFO : -8.119 per-word bound, 277.9 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:56:23,366 : INFO : PROGRESS: pass 14, at document #12000/14833\n",
      "2025-09-10 11:56:23,974 : INFO : optimized alpha [0.3819406, 0.06934238, 0.06824999, 0.34665224, 0.07281266]\n",
      "2025-09-10 11:56:23,979 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:23,985 : INFO : topic #0 (0.382): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:23,986 : INFO : topic #1 (0.069): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"language\"\n",
      "2025-09-10 11:56:23,987 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"time\" + 0.004*\"math\" + 0.004*\"back\" + 0.003*\"travel\"\n",
      "2025-09-10 11:56:23,988 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"said\"\n",
      "2025-09-10 11:56:23,988 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.008*\"time\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"idea\"\n",
      "2025-09-10 11:56:23,989 : INFO : topic diff=0.130579, rho=0.211211\n",
      "2025-09-10 11:56:25,218 : INFO : -8.573 per-word bound, 380.9 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:56:25,218 : INFO : PROGRESS: pass 14, at document #14000/14833\n",
      "2025-09-10 11:56:25,909 : INFO : optimized alpha [0.36918196, 0.071345575, 0.07244084, 0.36984703, 0.07570191]\n",
      "2025-09-10 11:56:25,914 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:25,920 : INFO : topic #0 (0.369): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:25,921 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.008*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"people\" + 0.004*\"even\"\n",
      "2025-09-10 11:56:25,922 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.005*\"time\" + 0.004*\"travel\" + 0.004*\"students\" + 0.004*\"back\" + 0.003*\"speed\" + 0.003*\"like\"\n",
      "2025-09-10 11:56:25,923 : INFO : topic #3 (0.370): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:25,923 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.010*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"black\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:25,924 : INFO : topic diff=0.262813, rho=0.211211\n",
      "2025-09-10 11:56:26,438 : INFO : -8.437 per-word bound, 346.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:56:26,438 : INFO : PROGRESS: pass 14, at document #14833/14833\n",
      "2025-09-10 11:56:26,709 : INFO : optimized alpha [0.32758325, 0.07284954, 0.07475655, 0.3897282, 0.0755027]\n",
      "2025-09-10 11:56:26,714 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:26,721 : INFO : topic #0 (0.328): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:26,722 : INFO : topic #1 (0.073): 0.010*\"money\" + 0.007*\"teeth\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:26,722 : INFO : topic #2 (0.075): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"time\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"travel\"\n",
      "2025-09-10 11:56:26,723 : INFO : topic #3 (0.390): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:56:26,724 : INFO : topic #4 (0.076): 0.010*\"like\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"think\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:26,724 : INFO : topic diff=0.209243, rho=0.211211\n",
      "2025-09-10 11:56:28,136 : INFO : -7.990 per-word bound, 254.3 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:56:28,137 : INFO : PROGRESS: pass 15, at document #2000/14833\n",
      "2025-09-10 11:56:28,830 : INFO : optimized alpha [0.355241, 0.07144038, 0.07366461, 0.33637494, 0.07647061]\n",
      "2025-09-10 11:56:28,836 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:28,841 : INFO : topic #0 (0.355): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:28,842 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:28,843 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"time\" + 0.004*\"back\" + 0.004*\"human\" + 0.003*\"travel\"\n",
      "2025-09-10 11:56:28,844 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:28,845 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.010*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:28,846 : INFO : topic diff=0.190251, rho=0.206652\n",
      "2025-09-10 11:56:30,145 : INFO : -7.882 per-word bound, 235.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:56:30,145 : INFO : PROGRESS: pass 15, at document #4000/14833\n",
      "2025-09-10 11:56:30,840 : INFO : optimized alpha [0.38436687, 0.07067264, 0.07222609, 0.3366625, 0.07690561]\n",
      "2025-09-10 11:56:30,845 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:30,851 : INFO : topic #0 (0.384): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:30,852 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:56:30,853 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"time\" + 0.004*\"war\" + 0.003*\"travel\" + 0.003*\"internet\"\n",
      "2025-09-10 11:56:30,854 : INFO : topic #3 (0.337): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:30,855 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:30,855 : INFO : topic diff=0.118858, rho=0.206652\n",
      "2025-09-10 11:56:32,073 : INFO : -7.782 per-word bound, 220.0 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:56:32,074 : INFO : PROGRESS: pass 15, at document #6000/14833\n",
      "2025-09-10 11:56:32,752 : INFO : optimized alpha [0.4056909, 0.0701019, 0.06972942, 0.35238853, 0.07413421]\n",
      "2025-09-10 11:56:32,758 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:32,764 : INFO : topic #0 (0.406): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:56:32,765 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:56:32,766 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"time\" + 0.004*\"math\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:32,767 : INFO : topic #3 (0.352): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:32,768 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.009*\"think\" + 0.009*\"time\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:32,768 : INFO : topic diff=0.118280, rho=0.206652\n",
      "2025-09-10 11:56:33,980 : INFO : -8.017 per-word bound, 259.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:56:33,981 : INFO : PROGRESS: pass 15, at document #8000/14833\n",
      "2025-09-10 11:56:34,644 : INFO : optimized alpha [0.40479976, 0.07002899, 0.06964332, 0.35141528, 0.074492306]\n",
      "2025-09-10 11:56:34,649 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:34,655 : INFO : topic #0 (0.405): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:56:34,656 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:34,657 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"time\" + 0.003*\"human\"\n",
      "2025-09-10 11:56:34,657 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:34,658 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"time\" + 0.009*\"something\" + 0.006*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:56:34,659 : INFO : topic diff=0.116584, rho=0.206652\n",
      "2025-09-10 11:56:35,810 : INFO : -8.075 per-word bound, 269.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:56:35,811 : INFO : PROGRESS: pass 15, at document #10000/14833\n",
      "2025-09-10 11:56:36,454 : INFO : optimized alpha [0.39255565, 0.06993705, 0.06887376, 0.3479899, 0.075008355]\n",
      "2025-09-10 11:56:36,459 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:36,465 : INFO : topic #0 (0.393): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:36,467 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:36,468 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"time\" + 0.004*\"internet\" + 0.004*\"human\"\n",
      "2025-09-10 11:56:36,469 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"really\"\n",
      "2025-09-10 11:56:36,470 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.009*\"time\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:56:36,470 : INFO : topic diff=0.115418, rho=0.206652\n",
      "2025-09-10 11:56:37,575 : INFO : -8.117 per-word bound, 277.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:56:37,576 : INFO : PROGRESS: pass 15, at document #12000/14833\n",
      "2025-09-10 11:56:38,184 : INFO : optimized alpha [0.38408968, 0.06972517, 0.06808771, 0.34603277, 0.072805665]\n",
      "2025-09-10 11:56:38,189 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:38,195 : INFO : topic #0 (0.384): 0.016*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:38,196 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"teeth\" + 0.005*\"even\" + 0.005*\"company\" + 0.005*\"language\"\n",
      "2025-09-10 11:56:38,197 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"math\" + 0.004*\"back\" + 0.004*\"time\" + 0.003*\"war\"\n",
      "2025-09-10 11:56:38,197 : INFO : topic #3 (0.346): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"said\"\n",
      "2025-09-10 11:56:38,198 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.009*\"time\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:56:38,199 : INFO : topic diff=0.127467, rho=0.206652\n",
      "2025-09-10 11:56:39,424 : INFO : -8.568 per-word bound, 379.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:56:39,425 : INFO : PROGRESS: pass 15, at document #14000/14833\n",
      "2025-09-10 11:56:40,112 : INFO : optimized alpha [0.37128434, 0.07168091, 0.07217445, 0.36865506, 0.07559989]\n",
      "2025-09-10 11:56:40,117 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:40,123 : INFO : topic #0 (0.371): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:40,124 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"people\"\n",
      "2025-09-10 11:56:40,125 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"time\" + 0.004*\"back\" + 0.004*\"travel\" + 0.003*\"speed\" + 0.003*\"internet\"\n",
      "2025-09-10 11:56:40,126 : INFO : topic #3 (0.369): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:40,127 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.010*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:40,127 : INFO : topic diff=0.255612, rho=0.206652\n",
      "2025-09-10 11:56:40,638 : INFO : -8.434 per-word bound, 345.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:56:40,639 : INFO : PROGRESS: pass 15, at document #14833/14833\n",
      "2025-09-10 11:56:40,908 : INFO : optimized alpha [0.33020902, 0.073196545, 0.07444496, 0.38811523, 0.07538719]\n",
      "2025-09-10 11:56:40,914 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:40,919 : INFO : topic #0 (0.330): 0.016*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:40,920 : INFO : topic #1 (0.073): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:40,921 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"time\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\"\n",
      "2025-09-10 11:56:40,922 : INFO : topic #3 (0.388): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"get\" + 0.005*\"even\"\n",
      "2025-09-10 11:56:40,922 : INFO : topic #4 (0.075): 0.010*\"like\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:40,923 : INFO : topic diff=0.204389, rho=0.206652\n",
      "2025-09-10 11:56:42,321 : INFO : -7.988 per-word bound, 253.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:56:42,322 : INFO : PROGRESS: pass 16, at document #2000/14833\n",
      "2025-09-10 11:56:43,020 : INFO : optimized alpha [0.35730922, 0.07181985, 0.073398255, 0.33618528, 0.076367795]\n",
      "2025-09-10 11:56:43,025 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:43,031 : INFO : topic #0 (0.357): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:43,032 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:43,033 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"time\" + 0.004*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:56:43,033 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:43,034 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.010*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:43,035 : INFO : topic diff=0.185406, rho=0.202376\n",
      "2025-09-10 11:56:44,334 : INFO : -7.880 per-word bound, 235.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:56:44,335 : INFO : PROGRESS: pass 16, at document #4000/14833\n",
      "2025-09-10 11:56:45,029 : INFO : optimized alpha [0.38587993, 0.07107428, 0.07198155, 0.33631906, 0.07686835]\n",
      "2025-09-10 11:56:45,034 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:45,040 : INFO : topic #0 (0.386): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:45,040 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.006*\"apple\" + 0.005*\"like\" + 0.005*\"health\"\n",
      "2025-09-10 11:56:45,041 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"time\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 11:56:45,042 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:56:45,043 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:45,043 : INFO : topic diff=0.116132, rho=0.202376\n",
      "2025-09-10 11:56:46,259 : INFO : -7.781 per-word bound, 219.9 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:56:46,260 : INFO : PROGRESS: pass 16, at document #6000/14833\n",
      "2025-09-10 11:56:46,932 : INFO : optimized alpha [0.4069329, 0.0705011, 0.06957676, 0.35165498, 0.074193925]\n",
      "2025-09-10 11:56:46,937 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:46,943 : INFO : topic #0 (0.407): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:56:46,944 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:56:46,945 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"like\" + 0.003*\"time\"\n",
      "2025-09-10 11:56:46,946 : INFO : topic #3 (0.352): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:46,946 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:46,947 : INFO : topic diff=0.115548, rho=0.202376\n",
      "2025-09-10 11:56:48,110 : INFO : -8.016 per-word bound, 258.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:56:48,111 : INFO : PROGRESS: pass 16, at document #8000/14833\n",
      "2025-09-10 11:56:48,771 : INFO : optimized alpha [0.40607175, 0.07041161, 0.06948833, 0.35068923, 0.07454509]\n",
      "2025-09-10 11:56:48,776 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:48,782 : INFO : topic #0 (0.406): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:56:48,784 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.006*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:48,785 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"like\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 11:56:48,786 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:56:48,787 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"think\" + 0.009*\"time\" + 0.009*\"something\" + 0.006*\"death\" + 0.006*\"one\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:56:48,787 : INFO : topic diff=0.113504, rho=0.202376\n",
      "2025-09-10 11:56:49,938 : INFO : -8.074 per-word bound, 269.5 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:56:49,939 : INFO : PROGRESS: pass 16, at document #10000/14833\n",
      "2025-09-10 11:56:50,581 : INFO : optimized alpha [0.3941044, 0.0703351, 0.068749845, 0.34736055, 0.075057514]\n",
      "2025-09-10 11:56:50,586 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:50,593 : INFO : topic #0 (0.394): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:50,594 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:56:50,595 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"war\"\n",
      "2025-09-10 11:56:50,595 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"really\"\n",
      "2025-09-10 11:56:50,596 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"think\" + 0.009*\"time\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:56:50,597 : INFO : topic diff=0.112815, rho=0.202376\n",
      "2025-09-10 11:56:51,707 : INFO : -8.116 per-word bound, 277.4 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:56:51,708 : INFO : PROGRESS: pass 16, at document #12000/14833\n",
      "2025-09-10 11:56:52,315 : INFO : optimized alpha [0.38574305, 0.07011324, 0.068002954, 0.34531438, 0.07287611]\n",
      "2025-09-10 11:56:52,320 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:52,326 : INFO : topic #0 (0.386): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:52,327 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"even\" + 0.005*\"language\"\n",
      "2025-09-10 11:56:52,328 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"math\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:56:52,328 : INFO : topic #3 (0.345): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"still\" + 0.005*\"said\"\n",
      "2025-09-10 11:56:52,329 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"time\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:56:52,330 : INFO : topic diff=0.124446, rho=0.202376\n",
      "2025-09-10 11:56:53,549 : INFO : -8.563 per-word bound, 378.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:56:53,549 : INFO : PROGRESS: pass 16, at document #14000/14833\n",
      "2025-09-10 11:56:54,234 : INFO : optimized alpha [0.37287435, 0.07203322, 0.071958445, 0.36740562, 0.075630784]\n",
      "2025-09-10 11:56:54,239 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:54,245 : INFO : topic #0 (0.373): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:54,246 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"people\"\n",
      "2025-09-10 11:56:54,246 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"time\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"speed\" + 0.003*\"war\"\n",
      "2025-09-10 11:56:54,247 : INFO : topic #3 (0.367): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:54,248 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:56:54,249 : INFO : topic diff=0.248815, rho=0.202376\n",
      "2025-09-10 11:56:54,755 : INFO : -8.431 per-word bound, 345.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:56:54,756 : INFO : PROGRESS: pass 16, at document #14833/14833\n",
      "2025-09-10 11:56:55,023 : INFO : optimized alpha [0.3323165, 0.07350586, 0.07416403, 0.38644746, 0.07541358]\n",
      "2025-09-10 11:56:55,028 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:55,034 : INFO : topic #0 (0.332): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:56:55,035 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:55,036 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"time\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\"\n",
      "2025-09-10 11:56:55,037 : INFO : topic #3 (0.386): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:55,038 : INFO : topic #4 (0.075): 0.010*\"like\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:55,038 : INFO : topic diff=0.199838, rho=0.202376\n",
      "2025-09-10 11:56:56,429 : INFO : -7.985 per-word bound, 253.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:56:56,430 : INFO : PROGRESS: pass 17, at document #2000/14833\n",
      "2025-09-10 11:56:57,117 : INFO : optimized alpha [0.35887325, 0.072148815, 0.073171005, 0.33583513, 0.07637689]\n",
      "2025-09-10 11:56:57,122 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:57,128 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:57,129 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:56:57,130 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"human\" + 0.003*\"time\" + 0.003*\"internet\"\n",
      "2025-09-10 11:56:57,131 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:56:57,132 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.010*\"time\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:56:57,132 : INFO : topic diff=0.180920, rho=0.198355\n",
      "2025-09-10 11:56:58,425 : INFO : -7.879 per-word bound, 235.4 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:56:58,426 : INFO : PROGRESS: pass 17, at document #4000/14833\n",
      "2025-09-10 11:56:59,110 : INFO : optimized alpha [0.3869027, 0.07139934, 0.071799785, 0.335914, 0.076867074]\n",
      "2025-09-10 11:56:59,115 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:56:59,121 : INFO : topic #0 (0.387): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:56:59,122 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.006*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:56:59,123 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"time\"\n",
      "2025-09-10 11:56:59,124 : INFO : topic #3 (0.336): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:56:59,124 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:56:59,125 : INFO : topic diff=0.113485, rho=0.198355\n",
      "2025-09-10 11:57:00,330 : INFO : -7.780 per-word bound, 219.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:57:00,331 : INFO : PROGRESS: pass 17, at document #6000/14833\n",
      "2025-09-10 11:57:01,001 : INFO : optimized alpha [0.40760666, 0.07083923, 0.06946346, 0.350852, 0.07424766]\n",
      "2025-09-10 11:57:01,007 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:01,013 : INFO : topic #0 (0.408): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:57:01,014 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:57:01,015 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:01,016 : INFO : topic #3 (0.351): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:01,017 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:01,017 : INFO : topic diff=0.113049, rho=0.198355\n",
      "2025-09-10 11:57:02,177 : INFO : -8.015 per-word bound, 258.6 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:57:02,178 : INFO : PROGRESS: pass 17, at document #8000/14833\n",
      "2025-09-10 11:57:02,834 : INFO : optimized alpha [0.40677115, 0.07074159, 0.069364406, 0.34986976, 0.074591786]\n",
      "2025-09-10 11:57:02,839 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:02,845 : INFO : topic #0 (0.407): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:57:02,846 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:02,847 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"math\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:02,848 : INFO : topic #3 (0.350): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:02,849 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"think\" + 0.010*\"time\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:57:02,849 : INFO : topic diff=0.110816, rho=0.198355\n",
      "2025-09-10 11:57:03,990 : INFO : -8.073 per-word bound, 269.4 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:57:03,990 : INFO : PROGRESS: pass 17, at document #10000/14833\n",
      "2025-09-10 11:57:04,624 : INFO : optimized alpha [0.39499742, 0.07066703, 0.06863166, 0.34652048, 0.075089164]\n",
      "2025-09-10 11:57:04,629 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:04,635 : INFO : topic #0 (0.395): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:04,636 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:04,637 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"internet\" + 0.004*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:04,638 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:04,639 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:57:04,639 : INFO : topic diff=0.110354, rho=0.198355\n",
      "2025-09-10 11:57:05,747 : INFO : -8.115 per-word bound, 277.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:57:05,748 : INFO : PROGRESS: pass 17, at document #12000/14833\n",
      "2025-09-10 11:57:06,349 : INFO : optimized alpha [0.38683364, 0.0704373, 0.067907676, 0.34450763, 0.07291955]\n",
      "2025-09-10 11:57:06,354 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:06,360 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"even\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:57:06,361 : INFO : topic #1 (0.070): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"even\" + 0.005*\"language\"\n",
      "2025-09-10 11:57:06,362 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"could\" + 0.005*\"students\" + 0.004*\"like\" + 0.004*\"math\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:06,363 : INFO : topic #3 (0.345): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:57:06,364 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"time\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:57:06,365 : INFO : topic diff=0.121800, rho=0.198355\n",
      "2025-09-10 11:57:07,572 : INFO : -8.559 per-word bound, 377.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:57:07,573 : INFO : PROGRESS: pass 17, at document #14000/14833\n",
      "2025-09-10 11:57:08,253 : INFO : optimized alpha [0.37395036, 0.07228941, 0.071793124, 0.36606458, 0.075606115]\n",
      "2025-09-10 11:57:08,258 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:08,264 : INFO : topic #0 (0.374): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.007*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:08,265 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"people\"\n",
      "2025-09-10 11:57:08,266 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"time\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"speed\" + 0.003*\"war\"\n",
      "2025-09-10 11:57:08,267 : INFO : topic #3 (0.366): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:08,268 : INFO : topic #4 (0.076): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:08,268 : INFO : topic diff=0.242434, rho=0.198355\n",
      "2025-09-10 11:57:08,772 : INFO : -8.428 per-word bound, 344.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:57:08,772 : INFO : PROGRESS: pass 17, at document #14833/14833\n",
      "2025-09-10 11:57:09,042 : INFO : optimized alpha [0.33394244, 0.073703416, 0.07392831, 0.38466114, 0.07535289]\n",
      "2025-09-10 11:57:09,047 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:09,053 : INFO : topic #0 (0.334): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:09,054 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:09,055 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"time\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\"\n",
      "2025-09-10 11:57:09,056 : INFO : topic #3 (0.385): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:09,057 : INFO : topic #4 (0.075): 0.010*\"like\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"god\" + 0.007*\"something\" + 0.007*\"one\" + 0.007*\"think\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:09,057 : INFO : topic diff=0.195629, rho=0.198355\n",
      "2025-09-10 11:57:10,456 : INFO : -7.983 per-word bound, 252.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:57:10,457 : INFO : PROGRESS: pass 18, at document #2000/14833\n",
      "2025-09-10 11:57:11,145 : INFO : optimized alpha [0.35996392, 0.072373495, 0.072974265, 0.33530334, 0.07628836]\n",
      "2025-09-10 11:57:11,150 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:11,156 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.012*\"like\" + 0.009*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:11,158 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:11,159 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"human\" + 0.003*\"time\" + 0.003*\"internet\"\n",
      "2025-09-10 11:57:11,159 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:11,160 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.010*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:11,161 : INFO : topic diff=0.176751, rho=0.194564\n",
      "2025-09-10 11:57:12,464 : INFO : -7.878 per-word bound, 235.2 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:57:12,464 : INFO : PROGRESS: pass 18, at document #4000/14833\n",
      "2025-09-10 11:57:13,156 : INFO : optimized alpha [0.38744286, 0.071631365, 0.07164289, 0.3352301, 0.07677366]\n",
      "2025-09-10 11:57:13,162 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:13,168 : INFO : topic #0 (0.387): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:13,169 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"business\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.006*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:57:13,170 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:13,171 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:57:13,172 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"life\" + 0.006*\"way\"\n",
      "2025-09-10 11:57:13,173 : INFO : topic diff=0.111143, rho=0.194564\n",
      "2025-09-10 11:57:14,402 : INFO : -7.779 per-word bound, 219.6 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:57:14,402 : INFO : PROGRESS: pass 18, at document #6000/14833\n",
      "2025-09-10 11:57:15,084 : INFO : optimized alpha [0.40788448, 0.071076386, 0.06936515, 0.3498449, 0.07420827]\n",
      "2025-09-10 11:57:15,090 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:15,095 : INFO : topic #0 (0.408): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:57:15,096 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:57:15,097 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:15,098 : INFO : topic #3 (0.350): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:15,099 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:15,100 : INFO : topic diff=0.110762, rho=0.194564\n",
      "2025-09-10 11:57:16,292 : INFO : -8.014 per-word bound, 258.5 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:57:16,293 : INFO : PROGRESS: pass 18, at document #8000/14833\n",
      "2025-09-10 11:57:16,968 : INFO : optimized alpha [0.40710527, 0.0709747, 0.06927366, 0.34888685, 0.07455382]\n",
      "2025-09-10 11:57:16,973 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:16,980 : INFO : topic #0 (0.407): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:57:16,981 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:16,982 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"math\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:16,983 : INFO : topic #3 (0.349): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:16,984 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:57:16,984 : INFO : topic diff=0.108240, rho=0.194564\n",
      "2025-09-10 11:57:18,146 : INFO : -8.072 per-word bound, 269.2 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:57:18,147 : INFO : PROGRESS: pass 18, at document #10000/14833\n",
      "2025-09-10 11:57:18,793 : INFO : optimized alpha [0.39553183, 0.070893265, 0.068530105, 0.34560966, 0.075067565]\n",
      "2025-09-10 11:57:18,798 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:18,804 : INFO : topic #0 (0.396): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:18,804 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:18,805 : INFO : topic #2 (0.069): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"like\" + 0.004*\"internet\" + 0.004*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:18,806 : INFO : topic #3 (0.346): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:18,807 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:57:18,807 : INFO : topic diff=0.108123, rho=0.194564\n",
      "2025-09-10 11:57:19,922 : INFO : -8.114 per-word bound, 277.0 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:57:19,923 : INFO : PROGRESS: pass 18, at document #12000/14833\n",
      "2025-09-10 11:57:20,537 : INFO : optimized alpha [0.38759282, 0.07065079, 0.06782627, 0.34365278, 0.07291156]\n",
      "2025-09-10 11:57:20,542 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:20,548 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:57:20,549 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:57:20,550 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.006*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"like\" + 0.004*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:20,551 : INFO : topic #3 (0.344): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:57:20,552 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"time\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:57:20,552 : INFO : topic diff=0.119173, rho=0.194564\n",
      "2025-09-10 11:57:21,791 : INFO : -8.555 per-word bound, 376.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:57:21,792 : INFO : PROGRESS: pass 18, at document #14000/14833\n",
      "2025-09-10 11:57:22,483 : INFO : optimized alpha [0.37478137, 0.0724619, 0.07159926, 0.3647648, 0.0755668]\n",
      "2025-09-10 11:57:22,489 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:22,495 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:22,495 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:57:22,496 : INFO : topic #2 (0.072): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"speed\" + 0.003*\"war\" + 0.003*\"time\"\n",
      "2025-09-10 11:57:22,497 : INFO : topic #3 (0.365): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:22,498 : INFO : topic #4 (0.076): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.007*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:22,498 : INFO : topic diff=0.236496, rho=0.194564\n",
      "2025-09-10 11:57:23,017 : INFO : -8.426 per-word bound, 343.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:57:23,018 : INFO : PROGRESS: pass 18, at document #14833/14833\n",
      "2025-09-10 11:57:23,291 : INFO : optimized alpha [0.33529776, 0.073842585, 0.07368405, 0.38296062, 0.07533118]\n",
      "2025-09-10 11:57:23,297 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:23,303 : INFO : topic #0 (0.335): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:23,304 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:23,304 : INFO : topic #2 (0.074): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"time\" + 0.003*\"human\" + 0.003*\"two\"\n",
      "2025-09-10 11:57:23,305 : INFO : topic #3 (0.383): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:23,306 : INFO : topic #4 (0.075): 0.010*\"time\" + 0.010*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:23,307 : INFO : topic diff=0.191666, rho=0.194564\n",
      "2025-09-10 11:57:24,706 : INFO : -7.980 per-word bound, 252.6 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:57:24,706 : INFO : PROGRESS: pass 19, at document #2000/14833\n",
      "2025-09-10 11:57:25,403 : INFO : optimized alpha [0.3608138, 0.07253069, 0.072759695, 0.3348483, 0.07623334]\n",
      "2025-09-10 11:57:25,408 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:25,414 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:25,415 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:25,416 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"time\"\n",
      "2025-09-10 11:57:25,417 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:25,418 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:25,418 : INFO : topic diff=0.172931, rho=0.190983\n",
      "2025-09-10 11:57:26,724 : INFO : -7.877 per-word bound, 235.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:57:26,725 : INFO : PROGRESS: pass 19, at document #4000/14833\n",
      "2025-09-10 11:57:27,420 : INFO : optimized alpha [0.3877757, 0.07179887, 0.07145166, 0.3346944, 0.076712824]\n",
      "2025-09-10 11:57:27,425 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:27,431 : INFO : topic #0 (0.388): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:27,432 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.007*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:57:27,433 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:27,433 : INFO : topic #3 (0.335): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:57:27,434 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:27,435 : INFO : topic diff=0.108849, rho=0.190983\n",
      "2025-09-10 11:57:28,668 : INFO : -7.778 per-word bound, 219.5 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:57:28,669 : INFO : PROGRESS: pass 19, at document #6000/14833\n",
      "2025-09-10 11:57:29,352 : INFO : optimized alpha [0.40796897, 0.07124, 0.06923347, 0.348956, 0.074195415]\n",
      "2025-09-10 11:57:29,357 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:29,363 : INFO : topic #0 (0.408): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:57:29,364 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:57:29,365 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:29,366 : INFO : topic #3 (0.349): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"really\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:29,367 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:29,368 : INFO : topic diff=0.108546, rho=0.190983\n",
      "2025-09-10 11:57:30,561 : INFO : -8.013 per-word bound, 258.3 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:57:30,562 : INFO : PROGRESS: pass 19, at document #8000/14833\n",
      "2025-09-10 11:57:31,232 : INFO : optimized alpha [0.40720338, 0.071138605, 0.0691556, 0.34796542, 0.07453925]\n",
      "2025-09-10 11:57:31,237 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:31,243 : INFO : topic #0 (0.407): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:57:31,244 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:31,245 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"like\" + 0.003*\"math\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:31,246 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:31,247 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:57:31,248 : INFO : topic diff=0.105820, rho=0.190983\n",
      "2025-09-10 11:57:32,397 : INFO : -8.072 per-word bound, 269.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:57:32,398 : INFO : PROGRESS: pass 19, at document #10000/14833\n",
      "2025-09-10 11:57:33,041 : INFO : optimized alpha [0.3959238, 0.071065135, 0.06840366, 0.3447711, 0.07504736]\n",
      "2025-09-10 11:57:33,047 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:33,052 : INFO : topic #0 (0.396): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:33,053 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:33,054 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"internet\" + 0.004*\"like\" + 0.004*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:33,055 : INFO : topic #3 (0.345): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:33,056 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:57:33,056 : INFO : topic diff=0.106018, rho=0.190983\n",
      "2025-09-10 11:57:34,164 : INFO : -8.113 per-word bound, 276.8 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:57:34,165 : INFO : PROGRESS: pass 19, at document #12000/14833\n",
      "2025-09-10 11:57:34,774 : INFO : optimized alpha [0.38814557, 0.070821114, 0.06772018, 0.34284008, 0.07292899]\n",
      "2025-09-10 11:57:34,780 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:34,786 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:57:34,786 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:57:34,787 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:34,788 : INFO : topic #3 (0.343): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:57:34,789 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"time\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:57:34,789 : INFO : topic diff=0.116797, rho=0.190983\n",
      "2025-09-10 11:57:36,012 : INFO : -8.551 per-word bound, 375.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:57:36,013 : INFO : PROGRESS: pass 19, at document #14000/14833\n",
      "2025-09-10 11:57:36,702 : INFO : optimized alpha [0.37534904, 0.07260718, 0.07143226, 0.36350045, 0.07553095]\n",
      "2025-09-10 11:57:36,707 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:36,713 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:36,714 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:57:36,715 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"speed\" + 0.003*\"math\"\n",
      "2025-09-10 11:57:36,716 : INFO : topic #3 (0.364): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:36,717 : INFO : topic #4 (0.076): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:36,717 : INFO : topic diff=0.231090, rho=0.190983\n",
      "2025-09-10 11:57:37,233 : INFO : -8.423 per-word bound, 343.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:57:37,234 : INFO : PROGRESS: pass 19, at document #14833/14833\n",
      "2025-09-10 11:57:37,507 : INFO : optimized alpha [0.33638382, 0.07397332, 0.073477924, 0.38132435, 0.075236544]\n",
      "2025-09-10 11:57:37,513 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:37,519 : INFO : topic #0 (0.336): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:37,519 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:37,520 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"time\" + 0.003*\"two\"\n",
      "2025-09-10 11:57:37,521 : INFO : topic #3 (0.381): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:37,522 : INFO : topic #4 (0.075): 0.010*\"time\" + 0.010*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"black\" + 0.006*\"life\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:37,522 : INFO : topic diff=0.187966, rho=0.190983\n",
      "2025-09-10 11:57:38,962 : INFO : -7.978 per-word bound, 252.2 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:57:38,963 : INFO : PROGRESS: pass 20, at document #2000/14833\n",
      "2025-09-10 11:57:39,665 : INFO : optimized alpha [0.36142397, 0.07267091, 0.07258102, 0.33441806, 0.076106794]\n",
      "2025-09-10 11:57:39,670 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:39,676 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:39,677 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:39,677 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:39,678 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:39,679 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:39,679 : INFO : topic diff=0.169250, rho=0.187592\n",
      "2025-09-10 11:57:40,984 : INFO : -7.876 per-word bound, 234.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:57:40,985 : INFO : PROGRESS: pass 20, at document #4000/14833\n",
      "2025-09-10 11:57:41,686 : INFO : optimized alpha [0.38790002, 0.07196457, 0.071280435, 0.33418328, 0.076606765]\n",
      "2025-09-10 11:57:41,692 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:41,698 : INFO : topic #0 (0.388): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:41,699 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:57:41,700 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:41,701 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:57:41,702 : INFO : topic #4 (0.077): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:41,702 : INFO : topic diff=0.106813, rho=0.187592\n",
      "2025-09-10 11:57:42,924 : INFO : -7.777 per-word bound, 219.3 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:57:42,924 : INFO : PROGRESS: pass 20, at document #6000/14833\n",
      "2025-09-10 11:57:43,604 : INFO : optimized alpha [0.4077539, 0.071413025, 0.06911527, 0.34810027, 0.07416613]\n",
      "2025-09-10 11:57:43,610 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:43,616 : INFO : topic #0 (0.408): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:57:43,618 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:57:43,618 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:43,619 : INFO : topic #3 (0.348): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:43,620 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:43,621 : INFO : topic diff=0.106507, rho=0.187592\n",
      "2025-09-10 11:57:44,794 : INFO : -8.012 per-word bound, 258.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:57:44,795 : INFO : PROGRESS: pass 20, at document #8000/14833\n",
      "2025-09-10 11:57:45,464 : INFO : optimized alpha [0.4069654, 0.07130236, 0.06903579, 0.3470528, 0.0744934]\n",
      "2025-09-10 11:57:45,469 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:45,475 : INFO : topic #0 (0.407): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:57:45,476 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:45,477 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:45,477 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:45,478 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"even\" + 0.006*\"sound\"\n",
      "2025-09-10 11:57:45,479 : INFO : topic diff=0.103583, rho=0.187592\n",
      "2025-09-10 11:57:46,649 : INFO : -8.071 per-word bound, 268.9 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:57:46,650 : INFO : PROGRESS: pass 20, at document #10000/14833\n",
      "2025-09-10 11:57:47,332 : INFO : optimized alpha [0.39595664, 0.0712324, 0.06829711, 0.34395656, 0.07498803]\n",
      "2025-09-10 11:57:47,337 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:47,343 : INFO : topic #0 (0.396): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:47,344 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:47,344 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"internet\" + 0.004*\"war\" + 0.004*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:47,345 : INFO : topic #3 (0.344): 0.012*\"like\" + 0.010*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:47,346 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:57:47,346 : INFO : topic diff=0.103938, rho=0.187592\n",
      "2025-09-10 11:57:48,453 : INFO : -8.112 per-word bound, 276.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:57:48,454 : INFO : PROGRESS: pass 20, at document #12000/14833\n",
      "2025-09-10 11:57:49,065 : INFO : optimized alpha [0.38831756, 0.07098929, 0.06763807, 0.34200573, 0.0728974]\n",
      "2025-09-10 11:57:49,070 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:49,076 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:57:49,077 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:57:49,078 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:57:49,079 : INFO : topic #3 (0.342): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:57:49,080 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"something\" + 0.009*\"time\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:57:49,080 : INFO : topic diff=0.114574, rho=0.187592\n",
      "2025-09-10 11:57:50,289 : INFO : -8.547 per-word bound, 374.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:57:50,290 : INFO : PROGRESS: pass 20, at document #14000/14833\n",
      "2025-09-10 11:57:50,963 : INFO : optimized alpha [0.37563515, 0.0727389, 0.07127348, 0.3621849, 0.07546411]\n",
      "2025-09-10 11:57:50,968 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:50,974 : INFO : topic #0 (0.376): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:50,975 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"even\" + 0.004*\"teeth\"\n",
      "2025-09-10 11:57:50,976 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"speed\" + 0.003*\"math\"\n",
      "2025-09-10 11:57:50,977 : INFO : topic #3 (0.362): 0.011*\"one\" + 0.011*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:50,977 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:50,978 : INFO : topic diff=0.225799, rho=0.187592\n",
      "2025-09-10 11:57:51,482 : INFO : -8.421 per-word bound, 342.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:57:51,483 : INFO : PROGRESS: pass 20, at document #14833/14833\n",
      "2025-09-10 11:57:51,750 : INFO : optimized alpha [0.3371412, 0.07407469, 0.07328074, 0.37961623, 0.07512288]\n",
      "2025-09-10 11:57:51,755 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:51,761 : INFO : topic #0 (0.337): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:57:51,762 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:51,763 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\" + 0.003*\"time\"\n",
      "2025-09-10 11:57:51,764 : INFO : topic #3 (0.380): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:51,765 : INFO : topic #4 (0.075): 0.010*\"time\" + 0.010*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"god\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:51,765 : INFO : topic diff=0.184532, rho=0.187592\n",
      "2025-09-10 11:57:53,156 : INFO : -7.976 per-word bound, 251.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:57:53,157 : INFO : PROGRESS: pass 21, at document #2000/14833\n",
      "2025-09-10 11:57:53,860 : INFO : optimized alpha [0.36174807, 0.072814845, 0.07239701, 0.33394986, 0.07596507]\n",
      "2025-09-10 11:57:53,865 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:53,871 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:53,872 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"apple\" + 0.007*\"business\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"also\" + 0.004*\"like\"\n",
      "2025-09-10 11:57:53,873 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:57:53,874 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:57:53,875 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:57:53,876 : INFO : topic diff=0.165822, rho=0.184376\n",
      "2025-09-10 11:57:55,191 : INFO : -7.875 per-word bound, 234.7 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:57:55,192 : INFO : PROGRESS: pass 21, at document #4000/14833\n",
      "2025-09-10 11:57:55,905 : INFO : optimized alpha [0.38773602, 0.0721251, 0.07112768, 0.3335519, 0.07647133]\n",
      "2025-09-10 11:57:55,910 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:55,916 : INFO : topic #0 (0.388): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:57:55,917 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:57:55,918 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:55,919 : INFO : topic #3 (0.334): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:57:55,920 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:55,921 : INFO : topic diff=0.104856, rho=0.184376\n",
      "2025-09-10 11:57:57,129 : INFO : -7.776 per-word bound, 219.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:57:57,130 : INFO : PROGRESS: pass 21, at document #6000/14833\n",
      "2025-09-10 11:57:57,805 : INFO : optimized alpha [0.4073207, 0.07158095, 0.06900571, 0.34716615, 0.07408367]\n",
      "2025-09-10 11:57:57,810 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:57,816 : INFO : topic #0 (0.407): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:57:57,817 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:57:57,818 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:57:57,819 : INFO : topic #3 (0.347): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:57,820 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:57:57,820 : INFO : topic diff=0.104554, rho=0.184376\n",
      "2025-09-10 11:57:58,987 : INFO : -8.011 per-word bound, 258.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:57:58,987 : INFO : PROGRESS: pass 21, at document #8000/14833\n",
      "2025-09-10 11:57:59,644 : INFO : optimized alpha [0.4065886, 0.07146941, 0.06893737, 0.3461098, 0.074419685]\n",
      "2025-09-10 11:57:59,650 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:57:59,656 : INFO : topic #0 (0.407): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:57:59,657 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:57:59,657 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:57:59,658 : INFO : topic #3 (0.346): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"really\"\n",
      "2025-09-10 11:57:59,659 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"sound\"\n",
      "2025-09-10 11:57:59,660 : INFO : topic diff=0.101420, rho=0.184376\n",
      "2025-09-10 11:58:00,805 : INFO : -8.070 per-word bound, 268.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:58:00,806 : INFO : PROGRESS: pass 21, at document #10000/14833\n",
      "2025-09-10 11:58:01,451 : INFO : optimized alpha [0.3958415, 0.07139922, 0.06820815, 0.34306726, 0.074908696]\n",
      "2025-09-10 11:58:01,456 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:01,462 : INFO : topic #0 (0.396): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:01,463 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"actually\" + 0.005*\"like\"\n",
      "2025-09-10 11:58:01,464 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"internet\" + 0.004*\"war\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:01,465 : INFO : topic #3 (0.343): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:01,466 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:58:01,466 : INFO : topic diff=0.102067, rho=0.184376\n",
      "2025-09-10 11:58:02,581 : INFO : -8.111 per-word bound, 276.4 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:58:02,582 : INFO : PROGRESS: pass 21, at document #12000/14833\n",
      "2025-09-10 11:58:03,199 : INFO : optimized alpha [0.38835436, 0.07115024, 0.06755748, 0.34112188, 0.07286632]\n",
      "2025-09-10 11:58:03,204 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:03,210 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:03,211 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:58:03,212 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"like\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:03,213 : INFO : topic #3 (0.341): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:58:03,213 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:58:03,214 : INFO : topic diff=0.112403, rho=0.184376\n",
      "2025-09-10 11:58:04,465 : INFO : -8.543 per-word bound, 373.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:58:04,465 : INFO : PROGRESS: pass 21, at document #14000/14833\n",
      "2025-09-10 11:58:05,146 : INFO : optimized alpha [0.37569812, 0.072862536, 0.071145676, 0.36087132, 0.07537126]\n",
      "2025-09-10 11:58:05,151 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:05,157 : INFO : topic #0 (0.376): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:05,158 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:58:05,159 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"speed\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:05,160 : INFO : topic #3 (0.361): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:05,161 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:05,161 : INFO : topic diff=0.220856, rho=0.184376\n",
      "2025-09-10 11:58:05,664 : INFO : -8.419 per-word bound, 342.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:58:05,664 : INFO : PROGRESS: pass 21, at document #14833/14833\n",
      "2025-09-10 11:58:05,931 : INFO : optimized alpha [0.33767477, 0.07418434, 0.07311518, 0.3778737, 0.0750184]\n",
      "2025-09-10 11:58:05,936 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:05,942 : INFO : topic #0 (0.338): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:05,943 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:58:05,944 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:05,944 : INFO : topic #3 (0.378): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:05,945 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.010*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:58:05,946 : INFO : topic diff=0.181143, rho=0.184376\n",
      "2025-09-10 11:58:07,334 : INFO : -7.974 per-word bound, 251.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:58:07,334 : INFO : PROGRESS: pass 22, at document #2000/14833\n",
      "2025-09-10 11:58:08,022 : INFO : optimized alpha [0.3618559, 0.07295599, 0.07224429, 0.33333004, 0.07583913]\n",
      "2025-09-10 11:58:08,027 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:08,033 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:08,034 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.007*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:08,035 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:08,036 : INFO : topic #3 (0.333): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:08,037 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 11:58:08,037 : INFO : topic diff=0.162472, rho=0.181320\n",
      "2025-09-10 11:58:09,326 : INFO : -7.874 per-word bound, 234.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:58:09,327 : INFO : PROGRESS: pass 22, at document #4000/14833\n",
      "2025-09-10 11:58:10,020 : INFO : optimized alpha [0.38732725, 0.072268955, 0.07100582, 0.3328046, 0.07632709]\n",
      "2025-09-10 11:58:10,025 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:10,031 : INFO : topic #0 (0.387): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:10,032 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:10,033 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:10,034 : INFO : topic #3 (0.333): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:10,034 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:10,035 : INFO : topic diff=0.102966, rho=0.181320\n",
      "2025-09-10 11:58:11,264 : INFO : -7.775 per-word bound, 219.1 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:58:11,264 : INFO : PROGRESS: pass 22, at document #6000/14833\n",
      "2025-09-10 11:58:11,946 : INFO : optimized alpha [0.40662456, 0.071724564, 0.06894129, 0.34609026, 0.07398552]\n",
      "2025-09-10 11:58:11,951 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:11,957 : INFO : topic #0 (0.407): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:58:11,958 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:58:11,959 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:11,960 : INFO : topic #3 (0.346): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:58:11,961 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:11,962 : INFO : topic diff=0.102642, rho=0.181320\n",
      "2025-09-10 11:58:13,159 : INFO : -8.011 per-word bound, 257.9 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:58:13,159 : INFO : PROGRESS: pass 22, at document #8000/14833\n",
      "2025-09-10 11:58:13,817 : INFO : optimized alpha [0.4059816, 0.07161924, 0.068879575, 0.34502742, 0.07431574]\n",
      "2025-09-10 11:58:13,822 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:13,828 : INFO : topic #0 (0.406): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:58:13,829 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:58:13,830 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:13,831 : INFO : topic #3 (0.345): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"years\"\n",
      "2025-09-10 11:58:13,832 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"sound\"\n",
      "2025-09-10 11:58:13,832 : INFO : topic diff=0.099416, rho=0.181320\n",
      "2025-09-10 11:58:14,968 : INFO : -8.069 per-word bound, 268.6 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:58:14,969 : INFO : PROGRESS: pass 22, at document #10000/14833\n",
      "2025-09-10 11:58:15,602 : INFO : optimized alpha [0.39550623, 0.07154941, 0.06815299, 0.34208304, 0.07481071]\n",
      "2025-09-10 11:58:15,607 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:15,613 : INFO : topic #0 (0.396): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:15,614 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:58:15,615 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:15,616 : INFO : topic #3 (0.342): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:15,617 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"way\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:58:15,617 : INFO : topic diff=0.100217, rho=0.181320\n",
      "2025-09-10 11:58:16,702 : INFO : -8.110 per-word bound, 276.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:58:16,703 : INFO : PROGRESS: pass 22, at document #12000/14833\n",
      "2025-09-10 11:58:17,299 : INFO : optimized alpha [0.38817918, 0.0712951, 0.06751498, 0.34019244, 0.07279668]\n",
      "2025-09-10 11:58:17,305 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:17,310 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:17,311 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:58:17,312 : INFO : topic #2 (0.068): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.003*\"like\" + 0.003*\"back\" + 0.003*\"war\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:17,313 : INFO : topic #3 (0.340): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:58:17,314 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"way\" + 0.005*\"idea\"\n",
      "2025-09-10 11:58:17,315 : INFO : topic diff=0.110294, rho=0.181320\n",
      "2025-09-10 11:58:18,542 : INFO : -8.540 per-word bound, 372.2 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:58:18,543 : INFO : PROGRESS: pass 22, at document #14000/14833\n",
      "2025-09-10 11:58:19,228 : INFO : optimized alpha [0.37562314, 0.07296928, 0.07102627, 0.35955817, 0.07525136]\n",
      "2025-09-10 11:58:19,233 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:19,239 : INFO : topic #0 (0.376): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:19,240 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:58:19,241 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"speed\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:19,242 : INFO : topic #3 (0.360): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:19,243 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:19,243 : INFO : topic diff=0.216225, rho=0.181320\n",
      "2025-09-10 11:58:19,756 : INFO : -8.418 per-word bound, 341.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:58:19,756 : INFO : PROGRESS: pass 22, at document #14833/14833\n",
      "2025-09-10 11:58:20,028 : INFO : optimized alpha [0.33817387, 0.074246265, 0.07294282, 0.3762036, 0.07490682]\n",
      "2025-09-10 11:58:20,033 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:20,039 : INFO : topic #0 (0.338): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:20,040 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:58:20,041 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:20,042 : INFO : topic #3 (0.376): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:20,042 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.010*\"like\" + 0.008*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:58:20,043 : INFO : topic diff=0.177954, rho=0.181320\n",
      "2025-09-10 11:58:21,453 : INFO : -7.973 per-word bound, 251.2 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:58:21,454 : INFO : PROGRESS: pass 23, at document #2000/14833\n",
      "2025-09-10 11:58:22,135 : INFO : optimized alpha [0.36195153, 0.07306522, 0.07208509, 0.33272207, 0.07571635]\n",
      "2025-09-10 11:58:22,140 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:22,146 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:58:22,147 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:22,148 : INFO : topic #2 (0.072): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:22,149 : INFO : topic #3 (0.333): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:22,150 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 11:58:22,150 : INFO : topic diff=0.159376, rho=0.178411\n",
      "2025-09-10 11:58:23,435 : INFO : -7.873 per-word bound, 234.4 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:58:23,436 : INFO : PROGRESS: pass 23, at document #4000/14833\n",
      "2025-09-10 11:58:24,123 : INFO : optimized alpha [0.38696122, 0.07240014, 0.07087531, 0.3320409, 0.076212995]\n",
      "2025-09-10 11:58:24,129 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:24,135 : INFO : topic #0 (0.387): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:24,135 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:24,136 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:24,137 : INFO : topic #3 (0.332): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:24,138 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:24,139 : INFO : topic diff=0.101097, rho=0.178411\n",
      "2025-09-10 11:58:25,343 : INFO : -7.775 per-word bound, 219.0 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:58:25,344 : INFO : PROGRESS: pass 23, at document #6000/14833\n",
      "2025-09-10 11:58:26,013 : INFO : optimized alpha [0.4060043, 0.0718498, 0.068853185, 0.34509847, 0.073900856]\n",
      "2025-09-10 11:58:26,018 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:26,024 : INFO : topic #0 (0.406): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:58:26,025 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:58:26,026 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:26,027 : INFO : topic #3 (0.345): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:58:26,028 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:26,028 : INFO : topic diff=0.100866, rho=0.178411\n",
      "2025-09-10 11:58:27,205 : INFO : -8.010 per-word bound, 257.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:58:27,206 : INFO : PROGRESS: pass 23, at document #8000/14833\n",
      "2025-09-10 11:58:27,872 : INFO : optimized alpha [0.40541673, 0.07175244, 0.06879627, 0.3440277, 0.074223004]\n",
      "2025-09-10 11:58:27,878 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:27,883 : INFO : topic #0 (0.405): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:58:27,884 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"apple\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:58:27,886 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:27,886 : INFO : topic #3 (0.344): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"years\"\n",
      "2025-09-10 11:58:27,887 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:58:27,888 : INFO : topic diff=0.097507, rho=0.178411\n",
      "2025-09-10 11:58:29,054 : INFO : -8.069 per-word bound, 268.5 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:58:29,055 : INFO : PROGRESS: pass 23, at document #10000/14833\n",
      "2025-09-10 11:58:29,693 : INFO : optimized alpha [0.39516386, 0.071694635, 0.06807448, 0.34114644, 0.07471769]\n",
      "2025-09-10 11:58:29,698 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:29,704 : INFO : topic #0 (0.395): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:29,705 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:58:29,705 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:29,706 : INFO : topic #3 (0.341): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:29,707 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"way\" + 0.007*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:58:29,707 : INFO : topic diff=0.098435, rho=0.178411\n",
      "2025-09-10 11:58:30,793 : INFO : -8.109 per-word bound, 276.1 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:58:30,793 : INFO : PROGRESS: pass 23, at document #12000/14833\n",
      "2025-09-10 11:58:31,396 : INFO : optimized alpha [0.38798162, 0.07143509, 0.06744911, 0.33929494, 0.07274227]\n",
      "2025-09-10 11:58:31,401 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:31,407 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:31,408 : INFO : topic #1 (0.071): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"like\" + 0.005*\"language\" + 0.005*\"even\"\n",
      "2025-09-10 11:58:31,409 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:31,410 : INFO : topic #3 (0.339): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:58:31,410 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:58:31,411 : INFO : topic diff=0.108326, rho=0.178411\n",
      "2025-09-10 11:58:32,618 : INFO : -8.537 per-word bound, 371.4 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:58:32,618 : INFO : PROGRESS: pass 23, at document #14000/14833\n",
      "2025-09-10 11:58:33,297 : INFO : optimized alpha [0.37553248, 0.073075905, 0.0709062, 0.3582758, 0.07512902]\n",
      "2025-09-10 11:58:33,302 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:33,308 : INFO : topic #0 (0.376): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:33,309 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:58:33,310 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"speed\"\n",
      "2025-09-10 11:58:33,311 : INFO : topic #3 (0.358): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:33,312 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:33,312 : INFO : topic diff=0.211687, rho=0.178411\n",
      "2025-09-10 11:58:33,820 : INFO : -8.416 per-word bound, 341.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:58:33,821 : INFO : PROGRESS: pass 23, at document #14833/14833\n",
      "2025-09-10 11:58:34,090 : INFO : optimized alpha [0.33862853, 0.07433702, 0.07275826, 0.3745945, 0.074810356]\n",
      "2025-09-10 11:58:34,095 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:34,101 : INFO : topic #0 (0.339): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:34,102 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:58:34,103 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:34,104 : INFO : topic #3 (0.375): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"said\" + 0.005*\"back\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:34,104 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"god\" + 0.007*\"one\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:58:34,105 : INFO : topic diff=0.174966, rho=0.178411\n",
      "2025-09-10 11:58:35,503 : INFO : -7.971 per-word bound, 250.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:58:35,504 : INFO : PROGRESS: pass 24, at document #2000/14833\n",
      "2025-09-10 11:58:36,197 : INFO : optimized alpha [0.36201772, 0.07317273, 0.071912736, 0.33210552, 0.07563669]\n",
      "2025-09-10 11:58:36,202 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:36,208 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:58:36,209 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:36,210 : INFO : topic #2 (0.072): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:36,211 : INFO : topic #3 (0.332): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:36,211 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 11:58:36,212 : INFO : topic diff=0.156499, rho=0.175637\n",
      "2025-09-10 11:58:37,522 : INFO : -7.872 per-word bound, 234.3 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:58:37,522 : INFO : PROGRESS: pass 24, at document #4000/14833\n",
      "2025-09-10 11:58:38,206 : INFO : optimized alpha [0.38659856, 0.072521925, 0.07073167, 0.33137235, 0.076135494]\n",
      "2025-09-10 11:58:38,211 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:38,217 : INFO : topic #0 (0.387): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:38,218 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:38,219 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:38,220 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:38,221 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:38,221 : INFO : topic diff=0.099383, rho=0.175637\n",
      "2025-09-10 11:58:39,421 : INFO : -7.774 per-word bound, 218.9 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:58:39,422 : INFO : PROGRESS: pass 24, at document #6000/14833\n",
      "2025-09-10 11:58:40,082 : INFO : optimized alpha [0.40535897, 0.07197242, 0.06873725, 0.3441242, 0.07386556]\n",
      "2025-09-10 11:58:40,087 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:40,093 : INFO : topic #0 (0.405): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:58:40,094 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:58:40,095 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:40,096 : INFO : topic #3 (0.344): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:58:40,097 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:40,097 : INFO : topic diff=0.099189, rho=0.175637\n",
      "2025-09-10 11:58:41,238 : INFO : -8.009 per-word bound, 257.6 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:58:41,239 : INFO : PROGRESS: pass 24, at document #8000/14833\n",
      "2025-09-10 11:58:41,883 : INFO : optimized alpha [0.40482277, 0.07188107, 0.06868524, 0.34302324, 0.07419516]\n",
      "2025-09-10 11:58:41,889 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:41,895 : INFO : topic #0 (0.405): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"also\"\n",
      "2025-09-10 11:58:41,895 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:58:41,896 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:41,897 : INFO : topic #3 (0.343): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"years\"\n",
      "2025-09-10 11:58:41,898 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:58:41,899 : INFO : topic diff=0.095675, rho=0.175637\n",
      "2025-09-10 11:58:43,042 : INFO : -8.068 per-word bound, 268.3 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:58:43,043 : INFO : PROGRESS: pass 24, at document #10000/14833\n",
      "2025-09-10 11:58:43,685 : INFO : optimized alpha [0.39479095, 0.07182433, 0.06794589, 0.34024802, 0.0746936]\n",
      "2025-09-10 11:58:43,690 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:43,696 : INFO : topic #0 (0.395): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:43,697 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:58:43,698 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:43,699 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:43,700 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"way\" + 0.007*\"even\" + 0.006*\"death\" + 0.006*\"question\"\n",
      "2025-09-10 11:58:43,701 : INFO : topic diff=0.096786, rho=0.175637\n",
      "2025-09-10 11:58:44,824 : INFO : -8.108 per-word bound, 276.0 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:58:44,825 : INFO : PROGRESS: pass 24, at document #12000/14833\n",
      "2025-09-10 11:58:45,430 : INFO : optimized alpha [0.38775608, 0.0715538, 0.067329645, 0.33843973, 0.072752886]\n",
      "2025-09-10 11:58:45,435 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:45,441 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:45,442 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:58:45,443 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:45,444 : INFO : topic #3 (0.338): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:58:45,445 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:58:45,445 : INFO : topic diff=0.106471, rho=0.175637\n",
      "2025-09-10 11:58:46,658 : INFO : -8.534 per-word bound, 370.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:58:46,658 : INFO : PROGRESS: pass 24, at document #14000/14833\n",
      "2025-09-10 11:58:47,328 : INFO : optimized alpha [0.37542439, 0.0731603, 0.07072863, 0.35708308, 0.07511058]\n",
      "2025-09-10 11:58:47,333 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:47,339 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:47,339 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:58:47,340 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"speed\"\n",
      "2025-09-10 11:58:47,341 : INFO : topic #3 (0.357): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:47,342 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:47,342 : INFO : topic diff=0.207479, rho=0.175637\n",
      "2025-09-10 11:58:47,846 : INFO : -8.414 per-word bound, 341.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:58:47,846 : INFO : PROGRESS: pass 24, at document #14833/14833\n",
      "2025-09-10 11:58:48,109 : INFO : optimized alpha [0.3390503, 0.074396305, 0.072527215, 0.37309828, 0.07479348]\n",
      "2025-09-10 11:58:48,114 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:48,120 : INFO : topic #0 (0.339): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:58:48,121 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:58:48,122 : INFO : topic #2 (0.073): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"two\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:48,123 : INFO : topic #3 (0.373): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"would\" + 0.005*\"even\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:48,124 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:58:48,124 : INFO : topic diff=0.172108, rho=0.175637\n",
      "2025-09-10 11:58:49,494 : INFO : -7.970 per-word bound, 250.7 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:58:49,494 : INFO : PROGRESS: pass 25, at document #2000/14833\n",
      "2025-09-10 11:58:50,163 : INFO : optimized alpha [0.3620785, 0.0732573, 0.071703956, 0.33155838, 0.075610556]\n",
      "2025-09-10 11:58:50,168 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:50,174 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:58:50,175 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:50,176 : INFO : topic #2 (0.072): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:58:50,177 : INFO : topic #3 (0.332): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:50,178 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 11:58:50,178 : INFO : topic diff=0.153607, rho=0.172989\n",
      "2025-09-10 11:58:51,449 : INFO : -7.871 per-word bound, 234.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:58:51,449 : INFO : PROGRESS: pass 25, at document #4000/14833\n",
      "2025-09-10 11:58:52,124 : INFO : optimized alpha [0.38628313, 0.072617196, 0.070534386, 0.33082658, 0.076104045]\n",
      "2025-09-10 11:58:52,129 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:52,135 : INFO : topic #0 (0.386): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:52,136 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:58:52,136 : INFO : topic #2 (0.071): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:52,137 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:58:52,138 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:52,138 : INFO : topic diff=0.097819, rho=0.172989\n",
      "2025-09-10 11:58:53,326 : INFO : -7.773 per-word bound, 218.8 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:58:53,326 : INFO : PROGRESS: pass 25, at document #6000/14833\n",
      "2025-09-10 11:58:53,979 : INFO : optimized alpha [0.40479076, 0.07206385, 0.06857297, 0.3433829, 0.07386718]\n",
      "2025-09-10 11:58:53,984 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:53,989 : INFO : topic #0 (0.405): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"even\" + 0.008*\"something\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:58:53,991 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:58:53,991 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:58:53,992 : INFO : topic #3 (0.343): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:58:53,993 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:53,994 : INFO : topic diff=0.097581, rho=0.172989\n",
      "2025-09-10 11:58:55,129 : INFO : -8.009 per-word bound, 257.5 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:58:55,130 : INFO : PROGRESS: pass 25, at document #8000/14833\n",
      "2025-09-10 11:58:55,770 : INFO : optimized alpha [0.40431595, 0.071982846, 0.06852167, 0.34228414, 0.074203566]\n",
      "2025-09-10 11:58:55,775 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:55,781 : INFO : topic #0 (0.404): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:58:55,782 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:58:55,783 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:55,784 : INFO : topic #3 (0.342): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"years\"\n",
      "2025-09-10 11:58:55,785 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:58:55,785 : INFO : topic diff=0.093942, rho=0.172989\n",
      "2025-09-10 11:58:56,907 : INFO : -8.067 per-word bound, 268.2 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:58:56,907 : INFO : PROGRESS: pass 25, at document #10000/14833\n",
      "2025-09-10 11:58:57,536 : INFO : optimized alpha [0.3945069, 0.07193817, 0.06779636, 0.3395772, 0.07470172]\n",
      "2025-09-10 11:58:57,541 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:57,547 : INFO : topic #0 (0.395): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:58:57,548 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:58:57,549 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:58:57,549 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:58:57,550 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"way\" + 0.007*\"even\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 11:58:57,551 : INFO : topic diff=0.095167, rho=0.172989\n",
      "2025-09-10 11:58:58,633 : INFO : -8.108 per-word bound, 275.8 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:58:58,634 : INFO : PROGRESS: pass 25, at document #12000/14833\n",
      "2025-09-10 11:58:59,228 : INFO : optimized alpha [0.3876075, 0.07166449, 0.06718289, 0.33780155, 0.07278606]\n",
      "2025-09-10 11:58:59,233 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:58:59,239 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:58:59,240 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:58:59,241 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:58:59,242 : INFO : topic #3 (0.338): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:58:59,243 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:58:59,243 : INFO : topic diff=0.104671, rho=0.172989\n",
      "2025-09-10 11:59:00,446 : INFO : -8.531 per-word bound, 369.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:59:00,447 : INFO : PROGRESS: pass 25, at document #14000/14833\n",
      "2025-09-10 11:59:01,112 : INFO : optimized alpha [0.37539455, 0.07325261, 0.07052036, 0.35612562, 0.075114764]\n",
      "2025-09-10 11:59:01,118 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:01,124 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:01,125 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:59:01,125 : INFO : topic #2 (0.071): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"internet\" + 0.003*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"speed\"\n",
      "2025-09-10 11:59:01,126 : INFO : topic #3 (0.356): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:01,127 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:01,127 : INFO : topic diff=0.203420, rho=0.172989\n",
      "2025-09-10 11:59:01,628 : INFO : -8.413 per-word bound, 340.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:59:01,628 : INFO : PROGRESS: pass 25, at document #14833/14833\n",
      "2025-09-10 11:59:01,893 : INFO : optimized alpha [0.3394692, 0.07447003, 0.07228902, 0.3718393, 0.074815035]\n",
      "2025-09-10 11:59:01,898 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:01,903 : INFO : topic #0 (0.339): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:01,905 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:59:01,905 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 11:59:01,906 : INFO : topic #3 (0.372): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:01,907 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:59:01,908 : INFO : topic diff=0.169536, rho=0.172989\n",
      "2025-09-10 11:59:03,274 : INFO : -7.968 per-word bound, 250.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:59:03,275 : INFO : PROGRESS: pass 26, at document #2000/14833\n",
      "2025-09-10 11:59:03,949 : INFO : optimized alpha [0.36215734, 0.0733471, 0.07149318, 0.33120358, 0.07562092]\n",
      "2025-09-10 11:59:03,954 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:03,960 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:59:03,961 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:59:03,962 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:03,963 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:03,964 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 11:59:03,964 : INFO : topic diff=0.150949, rho=0.170458\n",
      "2025-09-10 11:59:05,243 : INFO : -7.870 per-word bound, 234.0 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:59:05,243 : INFO : PROGRESS: pass 26, at document #4000/14833\n",
      "2025-09-10 11:59:05,925 : INFO : optimized alpha [0.3859508, 0.07271406, 0.070359975, 0.3304064, 0.07609639]\n",
      "2025-09-10 11:59:05,931 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:05,937 : INFO : topic #0 (0.386): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:05,938 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 11:59:05,939 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:05,940 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:05,940 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:05,941 : INFO : topic diff=0.096228, rho=0.170458\n",
      "2025-09-10 11:59:07,144 : INFO : -7.773 per-word bound, 218.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:59:07,144 : INFO : PROGRESS: pass 26, at document #6000/14833\n",
      "2025-09-10 11:59:07,804 : INFO : optimized alpha [0.40420786, 0.072169855, 0.068440005, 0.34271097, 0.073890224]\n",
      "2025-09-10 11:59:07,809 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:07,815 : INFO : topic #0 (0.404): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:59:07,816 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:59:07,817 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:59:07,818 : INFO : topic #3 (0.343): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:59:07,819 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.007*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:07,819 : INFO : topic diff=0.096022, rho=0.170458\n",
      "2025-09-10 11:59:08,966 : INFO : -8.008 per-word bound, 257.4 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:59:08,966 : INFO : PROGRESS: pass 26, at document #8000/14833\n",
      "2025-09-10 11:59:09,607 : INFO : optimized alpha [0.40379435, 0.0720948, 0.06838759, 0.3415974, 0.074221276]\n",
      "2025-09-10 11:59:09,612 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:09,617 : INFO : topic #0 (0.404): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:09,618 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:59:09,619 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"students\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:09,620 : INFO : topic #3 (0.342): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"years\"\n",
      "2025-09-10 11:59:09,621 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.006*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:59:09,622 : INFO : topic diff=0.092347, rho=0.170458\n",
      "2025-09-10 11:59:10,743 : INFO : -8.067 per-word bound, 268.1 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:59:10,744 : INFO : PROGRESS: pass 26, at document #10000/14833\n",
      "2025-09-10 11:59:11,379 : INFO : optimized alpha [0.3941924, 0.072044015, 0.06767939, 0.33890083, 0.07471636]\n",
      "2025-09-10 11:59:11,384 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:11,390 : INFO : topic #0 (0.394): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:11,391 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:59:11,392 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:11,392 : INFO : topic #3 (0.339): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:11,393 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 11:59:11,394 : INFO : topic diff=0.093687, rho=0.170458\n",
      "2025-09-10 11:59:12,471 : INFO : -8.107 per-word bound, 275.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:59:12,472 : INFO : PROGRESS: pass 26, at document #12000/14833\n",
      "2025-09-10 11:59:13,059 : INFO : optimized alpha [0.38742653, 0.07177323, 0.06706939, 0.33714405, 0.07282266]\n",
      "2025-09-10 11:59:13,064 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:13,070 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"even\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:13,071 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"teeth\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:59:13,071 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:59:13,072 : INFO : topic #3 (0.337): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:59:13,073 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:59:13,073 : INFO : topic diff=0.102963, rho=0.170458\n",
      "2025-09-10 11:59:14,268 : INFO : -8.528 per-word bound, 369.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:59:14,269 : INFO : PROGRESS: pass 26, at document #14000/14833\n",
      "2025-09-10 11:59:14,932 : INFO : optimized alpha [0.37530947, 0.07333477, 0.07033634, 0.35514536, 0.07510153]\n",
      "2025-09-10 11:59:14,937 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:14,943 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:14,944 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:59:14,945 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"war\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"speed\"\n",
      "2025-09-10 11:59:14,945 : INFO : topic #3 (0.355): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:14,946 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:14,947 : INFO : topic diff=0.199688, rho=0.170458\n",
      "2025-09-10 11:59:15,446 : INFO : -8.411 per-word bound, 340.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:59:15,447 : INFO : PROGRESS: pass 26, at document #14833/14833\n",
      "2025-09-10 11:59:15,707 : INFO : optimized alpha [0.33985308, 0.07451458, 0.07208026, 0.370596, 0.07480194]\n",
      "2025-09-10 11:59:15,712 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:15,717 : INFO : topic #0 (0.340): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.006*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:15,718 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:59:15,719 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 11:59:15,720 : INFO : topic #3 (0.371): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:15,721 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:59:15,721 : INFO : topic diff=0.166920, rho=0.170458\n",
      "2025-09-10 11:59:17,092 : INFO : -7.967 per-word bound, 250.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:59:17,092 : INFO : PROGRESS: pass 27, at document #2000/14833\n",
      "2025-09-10 11:59:17,766 : INFO : optimized alpha [0.3622103, 0.07339988, 0.07130473, 0.33081898, 0.07559834]\n",
      "2025-09-10 11:59:17,771 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:17,777 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:59:17,778 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:59:17,779 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:17,780 : INFO : topic #3 (0.331): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:17,781 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 11:59:17,781 : INFO : topic diff=0.148409, rho=0.168034\n",
      "2025-09-10 11:59:19,059 : INFO : -7.869 per-word bound, 233.8 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:59:19,059 : INFO : PROGRESS: pass 27, at document #4000/14833\n",
      "2025-09-10 11:59:19,735 : INFO : optimized alpha [0.38568154, 0.07277383, 0.070199504, 0.33001623, 0.07605885]\n",
      "2025-09-10 11:59:19,740 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:19,746 : INFO : topic #0 (0.386): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:19,747 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 11:59:19,748 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:19,749 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:19,750 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:19,750 : INFO : topic diff=0.094714, rho=0.168034\n",
      "2025-09-10 11:59:20,939 : INFO : -7.772 per-word bound, 218.6 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:59:20,940 : INFO : PROGRESS: pass 27, at document #6000/14833\n",
      "2025-09-10 11:59:21,594 : INFO : optimized alpha [0.4036969, 0.07223422, 0.06832454, 0.34211862, 0.073883]\n",
      "2025-09-10 11:59:21,599 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:21,605 : INFO : topic #0 (0.404): 0.015*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:59:21,606 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:59:21,607 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:59:21,608 : INFO : topic #3 (0.342): 0.012*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:59:21,608 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:21,609 : INFO : topic diff=0.094517, rho=0.168034\n",
      "2025-09-10 11:59:22,749 : INFO : -8.007 per-word bound, 257.3 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:59:22,750 : INFO : PROGRESS: pass 27, at document #8000/14833\n",
      "2025-09-10 11:59:23,388 : INFO : optimized alpha [0.40334433, 0.07216217, 0.068270355, 0.34098652, 0.07422763]\n",
      "2025-09-10 11:59:23,393 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:23,399 : INFO : topic #0 (0.403): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:23,400 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:59:23,401 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:23,402 : INFO : topic #3 (0.341): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 11:59:23,403 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:59:23,403 : INFO : topic diff=0.090754, rho=0.168034\n",
      "2025-09-10 11:59:24,521 : INFO : -8.066 per-word bound, 268.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:59:24,522 : INFO : PROGRESS: pass 27, at document #10000/14833\n",
      "2025-09-10 11:59:25,138 : INFO : optimized alpha [0.3939569, 0.07211251, 0.06757842, 0.33832836, 0.07471605]\n",
      "2025-09-10 11:59:25,143 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:25,149 : INFO : topic #0 (0.394): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:25,150 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:59:25,151 : INFO : topic #2 (0.068): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:25,152 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:25,152 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 11:59:25,153 : INFO : topic diff=0.092231, rho=0.168034\n",
      "2025-09-10 11:59:26,229 : INFO : -8.106 per-word bound, 275.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:59:26,230 : INFO : PROGRESS: pass 27, at document #12000/14833\n",
      "2025-09-10 11:59:26,820 : INFO : optimized alpha [0.3873058, 0.07183935, 0.06698731, 0.33654854, 0.072854325]\n",
      "2025-09-10 11:59:26,825 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:26,832 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:26,833 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:59:26,834 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:59:26,835 : INFO : topic #3 (0.337): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:59:26,835 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:59:26,836 : INFO : topic diff=0.101292, rho=0.168034\n",
      "2025-09-10 11:59:28,030 : INFO : -8.525 per-word bound, 368.5 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:59:28,031 : INFO : PROGRESS: pass 27, at document #14000/14833\n",
      "2025-09-10 11:59:28,694 : INFO : optimized alpha [0.37527394, 0.07336554, 0.07019837, 0.35422716, 0.07509476]\n",
      "2025-09-10 11:59:28,699 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:28,705 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:28,706 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:59:28,707 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"war\" + 0.003*\"internet\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"speed\"\n",
      "2025-09-10 11:59:28,708 : INFO : topic #3 (0.354): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:28,709 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:28,709 : INFO : topic diff=0.196027, rho=0.168034\n",
      "2025-09-10 11:59:29,207 : INFO : -8.410 per-word bound, 340.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:59:29,207 : INFO : PROGRESS: pass 27, at document #14833/14833\n",
      "2025-09-10 11:59:29,469 : INFO : optimized alpha [0.34023604, 0.07449885, 0.07191703, 0.36941317, 0.074767426]\n",
      "2025-09-10 11:59:29,474 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:29,480 : INFO : topic #0 (0.340): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:29,481 : INFO : topic #1 (0.074): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:59:29,482 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 11:59:29,483 : INFO : topic #3 (0.369): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:29,484 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:59:29,484 : INFO : topic diff=0.164528, rho=0.168034\n",
      "2025-09-10 11:59:30,859 : INFO : -7.965 per-word bound, 249.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:59:30,859 : INFO : PROGRESS: pass 28, at document #2000/14833\n",
      "2025-09-10 11:59:31,531 : INFO : optimized alpha [0.36227554, 0.07341139, 0.07115671, 0.33044875, 0.075545944]\n",
      "2025-09-10 11:59:31,536 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:31,542 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:59:31,543 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:59:31,544 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:31,545 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:31,546 : INFO : topic #4 (0.076): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 11:59:31,546 : INFO : topic diff=0.146037, rho=0.165711\n",
      "2025-09-10 11:59:32,818 : INFO : -7.869 per-word bound, 233.7 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:59:32,819 : INFO : PROGRESS: pass 28, at document #4000/14833\n",
      "2025-09-10 11:59:33,497 : INFO : optimized alpha [0.3853765, 0.07279952, 0.07007862, 0.3296026, 0.07599885]\n",
      "2025-09-10 11:59:33,502 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:33,508 : INFO : topic #0 (0.385): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:33,509 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 11:59:33,510 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:33,511 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:33,511 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:33,512 : INFO : topic diff=0.093327, rho=0.165711\n",
      "2025-09-10 11:59:34,695 : INFO : -7.771 per-word bound, 218.5 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:59:34,696 : INFO : PROGRESS: pass 28, at document #6000/14833\n",
      "2025-09-10 11:59:35,352 : INFO : optimized alpha [0.40313038, 0.07226284, 0.06823183, 0.34153777, 0.07385912]\n",
      "2025-09-10 11:59:35,357 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:35,363 : INFO : topic #0 (0.403): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:59:35,363 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:59:35,364 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:59:35,365 : INFO : topic #3 (0.342): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:59:35,366 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:35,367 : INFO : topic diff=0.093082, rho=0.165711\n",
      "2025-09-10 11:59:36,499 : INFO : -8.007 per-word bound, 257.2 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:59:36,500 : INFO : PROGRESS: pass 28, at document #8000/14833\n",
      "2025-09-10 11:59:37,140 : INFO : optimized alpha [0.40280482, 0.07221548, 0.06818145, 0.3403519, 0.074196756]\n",
      "2025-09-10 11:59:37,145 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:37,151 : INFO : topic #0 (0.403): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:37,152 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"language\" + 0.006*\"sleep\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:59:37,153 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:37,154 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 11:59:37,155 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:59:37,155 : INFO : topic diff=0.089266, rho=0.165711\n",
      "2025-09-10 11:59:38,278 : INFO : -8.066 per-word bound, 267.9 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:59:38,279 : INFO : PROGRESS: pass 28, at document #10000/14833\n",
      "2025-09-10 11:59:38,903 : INFO : optimized alpha [0.39361107, 0.07216389, 0.067497574, 0.3377397, 0.07468515]\n",
      "2025-09-10 11:59:38,907 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:38,913 : INFO : topic #0 (0.394): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:38,915 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:59:38,916 : INFO : topic #2 (0.067): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:38,916 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:38,917 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 11:59:38,918 : INFO : topic diff=0.090868, rho=0.165711\n",
      "2025-09-10 11:59:40,013 : INFO : -8.106 per-word bound, 275.4 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:59:40,013 : INFO : PROGRESS: pass 28, at document #12000/14833\n",
      "2025-09-10 11:59:40,610 : INFO : optimized alpha [0.38704434, 0.071892135, 0.06692323, 0.33598283, 0.07284308]\n",
      "2025-09-10 11:59:40,615 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:40,621 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:40,622 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:59:40,623 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:59:40,624 : INFO : topic #3 (0.336): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:59:40,625 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.009*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:59:40,625 : INFO : topic diff=0.099732, rho=0.165711\n",
      "2025-09-10 11:59:41,837 : INFO : -8.523 per-word bound, 367.9 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:59:41,838 : INFO : PROGRESS: pass 28, at document #14000/14833\n",
      "2025-09-10 11:59:42,511 : INFO : optimized alpha [0.37510934, 0.073410675, 0.070080824, 0.35333297, 0.07503508]\n",
      "2025-09-10 11:59:42,516 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:42,522 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:42,523 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:59:42,524 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.003*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"speed\"\n",
      "2025-09-10 11:59:42,525 : INFO : topic #3 (0.353): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:42,525 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"even\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:42,526 : INFO : topic diff=0.192543, rho=0.165711\n",
      "2025-09-10 11:59:43,029 : INFO : -8.408 per-word bound, 339.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:59:43,030 : INFO : PROGRESS: pass 28, at document #14833/14833\n",
      "2025-09-10 11:59:43,293 : INFO : optimized alpha [0.34057617, 0.07451181, 0.071775906, 0.36829516, 0.07471002]\n",
      "2025-09-10 11:59:43,298 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:43,304 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:43,305 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:59:43,306 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 11:59:43,307 : INFO : topic #3 (0.368): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:43,308 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:59:43,308 : INFO : topic diff=0.162110, rho=0.165711\n",
      "2025-09-10 11:59:44,677 : INFO : -7.964 per-word bound, 249.7 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:59:44,677 : INFO : PROGRESS: pass 29, at document #2000/14833\n",
      "2025-09-10 11:59:45,356 : INFO : optimized alpha [0.3623082, 0.0734515, 0.07101666, 0.33009535, 0.07548425]\n",
      "2025-09-10 11:59:45,361 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:45,367 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:59:45,368 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:59:45,368 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:45,369 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:45,370 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 11:59:45,371 : INFO : topic diff=0.143744, rho=0.163481\n",
      "2025-09-10 11:59:46,644 : INFO : -7.868 per-word bound, 233.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 11:59:46,645 : INFO : PROGRESS: pass 29, at document #4000/14833\n",
      "2025-09-10 11:59:47,323 : INFO : optimized alpha [0.38505298, 0.07284504, 0.06995716, 0.3291974, 0.07592443]\n",
      "2025-09-10 11:59:47,328 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:47,334 : INFO : topic #0 (0.385): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:47,335 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 11:59:47,336 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:47,336 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.010*\"like\" + 0.008*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:47,337 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:47,338 : INFO : topic diff=0.091948, rho=0.163481\n",
      "2025-09-10 11:59:48,527 : INFO : -7.771 per-word bound, 218.4 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 11:59:48,527 : INFO : PROGRESS: pass 29, at document #6000/14833\n",
      "2025-09-10 11:59:49,176 : INFO : optimized alpha [0.4025542, 0.072332256, 0.068137005, 0.34087715, 0.07381035]\n",
      "2025-09-10 11:59:49,181 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:49,187 : INFO : topic #0 (0.403): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 11:59:49,188 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 11:59:49,189 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:59:49,190 : INFO : topic #3 (0.341): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 11:59:49,191 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:49,191 : INFO : topic diff=0.091725, rho=0.163481\n",
      "2025-09-10 11:59:50,329 : INFO : -8.006 per-word bound, 257.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 11:59:50,330 : INFO : PROGRESS: pass 29, at document #8000/14833\n",
      "2025-09-10 11:59:50,972 : INFO : optimized alpha [0.40226144, 0.072290584, 0.06808787, 0.3396914, 0.07414346]\n",
      "2025-09-10 11:59:50,977 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:50,983 : INFO : topic #0 (0.402): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 11:59:50,983 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 11:59:50,984 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 11:59:50,985 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 11:59:50,986 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 11:59:50,986 : INFO : topic diff=0.087872, rho=0.163481\n",
      "2025-09-10 11:59:52,116 : INFO : -8.065 per-word bound, 267.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 11:59:52,117 : INFO : PROGRESS: pass 29, at document #10000/14833\n",
      "2025-09-10 11:59:52,740 : INFO : optimized alpha [0.39324436, 0.07223948, 0.06741643, 0.3371206, 0.0746322]\n",
      "2025-09-10 11:59:52,745 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:52,751 : INFO : topic #0 (0.393): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 11:59:52,751 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 11:59:52,752 : INFO : topic #2 (0.067): 0.024*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 11:59:52,753 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:52,754 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 11:59:52,755 : INFO : topic diff=0.089507, rho=0.163481\n",
      "2025-09-10 11:59:53,841 : INFO : -8.105 per-word bound, 275.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 11:59:53,842 : INFO : PROGRESS: pass 29, at document #12000/14833\n",
      "2025-09-10 11:59:54,434 : INFO : optimized alpha [0.3867943, 0.07196639, 0.066852845, 0.3353817, 0.07280986]\n",
      "2025-09-10 11:59:54,440 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:54,446 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.006*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:54,446 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 11:59:54,447 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 11:59:54,448 : INFO : topic #3 (0.335): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 11:59:54,449 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 11:59:54,449 : INFO : topic diff=0.098236, rho=0.163481\n",
      "2025-09-10 11:59:55,639 : INFO : -8.521 per-word bound, 367.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 11:59:55,640 : INFO : PROGRESS: pass 29, at document #14000/14833\n",
      "2025-09-10 11:59:56,304 : INFO : optimized alpha [0.3749467, 0.07345274, 0.06997037, 0.35245723, 0.07496941]\n",
      "2025-09-10 11:59:56,309 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:56,315 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:56,316 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 11:59:56,316 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"speed\"\n",
      "2025-09-10 11:59:56,317 : INFO : topic #3 (0.352): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:56,318 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 11:59:56,318 : INFO : topic diff=0.189211, rho=0.163481\n",
      "2025-09-10 11:59:56,815 : INFO : -8.407 per-word bound, 339.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 11:59:56,816 : INFO : PROGRESS: pass 29, at document #14833/14833\n",
      "2025-09-10 11:59:57,078 : INFO : optimized alpha [0.3408319, 0.07453849, 0.0716503, 0.36717945, 0.07463304]\n",
      "2025-09-10 11:59:57,083 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:57,089 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 11:59:57,090 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.007*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 11:59:57,090 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 11:59:57,091 : INFO : topic #3 (0.367): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 11:59:57,092 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"black\" + 0.006*\"even\"\n",
      "2025-09-10 11:59:57,093 : INFO : topic diff=0.159986, rho=0.163481\n",
      "2025-09-10 11:59:58,454 : INFO : -7.963 per-word bound, 249.5 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 11:59:58,455 : INFO : PROGRESS: pass 30, at document #2000/14833\n",
      "2025-09-10 11:59:59,123 : INFO : optimized alpha [0.36222196, 0.07349269, 0.07090234, 0.32969308, 0.07539519]\n",
      "2025-09-10 11:59:59,128 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 11:59:59,134 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 11:59:59,135 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 11:59:59,136 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 11:59:59,137 : INFO : topic #3 (0.330): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 11:59:59,138 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.007*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 11:59:59,138 : INFO : topic diff=0.141549, rho=0.161340\n",
      "2025-09-10 12:00:00,411 : INFO : -7.867 per-word bound, 233.5 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:00:00,412 : INFO : PROGRESS: pass 30, at document #4000/14833\n",
      "2025-09-10 12:00:01,091 : INFO : optimized alpha [0.38466245, 0.07289116, 0.06985057, 0.32874164, 0.07582675]\n",
      "2025-09-10 12:00:01,096 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:01,101 : INFO : topic #0 (0.385): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:01,102 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:00:01,103 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:01,104 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:01,105 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:01,106 : INFO : topic diff=0.090658, rho=0.161340\n",
      "2025-09-10 12:00:02,293 : INFO : -7.770 per-word bound, 218.3 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:00:02,293 : INFO : PROGRESS: pass 30, at document #6000/14833\n",
      "2025-09-10 12:00:02,943 : INFO : optimized alpha [0.40193754, 0.072382994, 0.06806097, 0.34024113, 0.07372827]\n",
      "2025-09-10 12:00:02,948 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:02,954 : INFO : topic #0 (0.402): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:00:02,955 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.007*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:00:02,956 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:00:02,957 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 12:00:02,958 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:02,958 : INFO : topic diff=0.090406, rho=0.161340\n",
      "2025-09-10 12:00:04,092 : INFO : -8.006 per-word bound, 257.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:00:04,093 : INFO : PROGRESS: pass 30, at document #8000/14833\n",
      "2025-09-10 12:00:04,729 : INFO : optimized alpha [0.4016727, 0.0723399, 0.068014, 0.33904633, 0.07405019]\n",
      "2025-09-10 12:00:04,735 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:04,740 : INFO : topic #0 (0.402): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:04,741 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:00:04,742 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:00:04,743 : INFO : topic #3 (0.339): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:00:04,744 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:00:04,744 : INFO : topic diff=0.086494, rho=0.161340\n",
      "2025-09-10 12:00:05,867 : INFO : -8.065 per-word bound, 267.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:00:05,868 : INFO : PROGRESS: pass 30, at document #10000/14833\n",
      "2025-09-10 12:00:06,496 : INFO : optimized alpha [0.39282787, 0.07228788, 0.06734788, 0.3365161, 0.074534595]\n",
      "2025-09-10 12:00:06,502 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:06,508 : INFO : topic #0 (0.393): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:06,508 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 12:00:06,509 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:00:06,510 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:06,511 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:00:06,511 : INFO : topic diff=0.088283, rho=0.161340\n",
      "2025-09-10 12:00:07,589 : INFO : -8.104 per-word bound, 275.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:00:07,590 : INFO : PROGRESS: pass 30, at document #12000/14833\n",
      "2025-09-10 12:00:08,178 : INFO : optimized alpha [0.38651696, 0.07200937, 0.06678225, 0.33477393, 0.072743595]\n",
      "2025-09-10 12:00:08,183 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:08,189 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:08,190 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:00:08,191 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:08,192 : INFO : topic #3 (0.335): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:00:08,193 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:00:08,193 : INFO : topic diff=0.096780, rho=0.161340\n",
      "2025-09-10 12:00:09,385 : INFO : -8.519 per-word bound, 366.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:00:09,385 : INFO : PROGRESS: pass 30, at document #14000/14833\n",
      "2025-09-10 12:00:10,042 : INFO : optimized alpha [0.37475502, 0.07348104, 0.0698559, 0.35158917, 0.07486962]\n",
      "2025-09-10 12:00:10,047 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:10,053 : INFO : topic #0 (0.375): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:10,054 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:00:10,055 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"speed\"\n",
      "2025-09-10 12:00:10,056 : INFO : topic #3 (0.352): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:10,057 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:10,057 : INFO : topic diff=0.186027, rho=0.161340\n",
      "2025-09-10 12:00:10,558 : INFO : -8.406 per-word bound, 339.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:00:10,559 : INFO : PROGRESS: pass 30, at document #14833/14833\n",
      "2025-09-10 12:00:10,817 : INFO : optimized alpha [0.34092364, 0.074561246, 0.07150317, 0.3660584, 0.07452253]\n",
      "2025-09-10 12:00:10,822 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:10,828 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:10,829 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:00:10,830 : INFO : topic #2 (0.072): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:00:10,831 : INFO : topic #3 (0.366): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:10,832 : INFO : topic #4 (0.075): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:10,832 : INFO : topic diff=0.157761, rho=0.161340\n",
      "2025-09-10 12:00:12,198 : INFO : -7.962 per-word bound, 249.3 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:00:12,199 : INFO : PROGRESS: pass 31, at document #2000/14833\n",
      "2025-09-10 12:00:12,868 : INFO : optimized alpha [0.36201292, 0.07352853, 0.070773326, 0.32923567, 0.0752743]\n",
      "2025-09-10 12:00:12,873 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:12,879 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:12,880 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:00:12,881 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:12,882 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:12,883 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:12,883 : INFO : topic diff=0.139495, rho=0.159280\n",
      "2025-09-10 12:00:14,160 : INFO : -7.866 per-word bound, 233.4 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:00:14,160 : INFO : PROGRESS: pass 31, at document #4000/14833\n",
      "2025-09-10 12:00:14,842 : INFO : optimized alpha [0.38410223, 0.07293172, 0.06974125, 0.32828626, 0.07568695]\n",
      "2025-09-10 12:00:14,847 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:14,853 : INFO : topic #0 (0.384): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:14,854 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:00:14,855 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:14,856 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:14,857 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:14,857 : INFO : topic diff=0.089354, rho=0.159280\n",
      "2025-09-10 12:00:16,061 : INFO : -7.770 per-word bound, 218.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:00:16,063 : INFO : PROGRESS: pass 31, at document #6000/14833\n",
      "2025-09-10 12:00:16,721 : INFO : optimized alpha [0.4011446, 0.072427236, 0.067980476, 0.33960754, 0.07361033]\n",
      "2025-09-10 12:00:16,726 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:16,732 : INFO : topic #0 (0.401): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:00:16,733 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:00:16,734 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:16,735 : INFO : topic #3 (0.340): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"really\"\n",
      "2025-09-10 12:00:16,736 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:16,736 : INFO : topic diff=0.089168, rho=0.159280\n",
      "2025-09-10 12:00:17,878 : INFO : -8.005 per-word bound, 256.9 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:00:17,879 : INFO : PROGRESS: pass 31, at document #8000/14833\n",
      "2025-09-10 12:00:18,520 : INFO : optimized alpha [0.40092525, 0.07238586, 0.06793623, 0.33841258, 0.073931664]\n",
      "2025-09-10 12:00:18,525 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:18,531 : INFO : topic #0 (0.401): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:18,531 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:00:18,532 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:00:18,533 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:00:18,534 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:00:18,534 : INFO : topic diff=0.085174, rho=0.159280\n",
      "2025-09-10 12:00:19,662 : INFO : -8.064 per-word bound, 267.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:00:19,662 : INFO : PROGRESS: pass 31, at document #10000/14833\n",
      "2025-09-10 12:00:20,291 : INFO : optimized alpha [0.39223728, 0.07233608, 0.067285955, 0.33587244, 0.07440709]\n",
      "2025-09-10 12:00:20,296 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:20,302 : INFO : topic #0 (0.392): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:20,303 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 12:00:20,304 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:20,305 : INFO : topic #3 (0.336): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:20,306 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:00:20,306 : INFO : topic diff=0.087031, rho=0.159280\n",
      "2025-09-10 12:00:21,381 : INFO : -8.104 per-word bound, 275.1 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:00:21,382 : INFO : PROGRESS: pass 31, at document #12000/14833\n",
      "2025-09-10 12:00:21,967 : INFO : optimized alpha [0.38604897, 0.07205735, 0.06672908, 0.33415315, 0.072647214]\n",
      "2025-09-10 12:00:21,972 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:21,978 : INFO : topic #0 (0.386): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:21,979 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:00:21,980 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"like\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:21,981 : INFO : topic #3 (0.334): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:00:21,982 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:00:21,982 : INFO : topic diff=0.095358, rho=0.159280\n",
      "2025-09-10 12:00:23,181 : INFO : -8.517 per-word bound, 366.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:00:23,181 : INFO : PROGRESS: pass 31, at document #14000/14833\n",
      "2025-09-10 12:00:23,851 : INFO : optimized alpha [0.37438858, 0.07351584, 0.06976073, 0.35072666, 0.074742034]\n",
      "2025-09-10 12:00:23,856 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:23,862 : INFO : topic #0 (0.374): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:23,863 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:00:23,864 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"speed\"\n",
      "2025-09-10 12:00:23,865 : INFO : topic #3 (0.351): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:23,866 : INFO : topic #4 (0.075): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:23,866 : INFO : topic diff=0.183053, rho=0.159280\n",
      "2025-09-10 12:00:24,365 : INFO : -8.405 per-word bound, 338.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:00:24,366 : INFO : PROGRESS: pass 31, at document #14833/14833\n",
      "2025-09-10 12:00:24,631 : INFO : optimized alpha [0.34096423, 0.074595116, 0.07138611, 0.3649784, 0.07439183]\n",
      "2025-09-10 12:00:24,636 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:24,642 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"something\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:24,643 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:00:24,644 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:00:24,645 : INFO : topic #3 (0.365): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:24,646 : INFO : topic #4 (0.074): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:24,646 : INFO : topic diff=0.155701, rho=0.159280\n",
      "2025-09-10 12:00:26,031 : INFO : -7.961 per-word bound, 249.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:00:26,032 : INFO : PROGRESS: pass 32, at document #2000/14833\n",
      "2025-09-10 12:00:26,708 : INFO : optimized alpha [0.36179766, 0.07357105, 0.07067245, 0.32880396, 0.07513692]\n",
      "2025-09-10 12:00:26,713 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:26,719 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:26,720 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:00:26,721 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:26,721 : INFO : topic #3 (0.329): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:26,722 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:26,723 : INFO : topic diff=0.137488, rho=0.157297\n",
      "2025-09-10 12:00:28,004 : INFO : -7.866 per-word bound, 233.3 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:00:28,005 : INFO : PROGRESS: pass 32, at document #4000/14833\n",
      "2025-09-10 12:00:28,682 : INFO : optimized alpha [0.3835777, 0.0729851, 0.06965745, 0.3278218, 0.07554733]\n",
      "2025-09-10 12:00:28,687 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:28,693 : INFO : topic #0 (0.384): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:28,694 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:00:28,695 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:28,696 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:28,697 : INFO : topic #4 (0.076): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:28,698 : INFO : topic diff=0.088170, rho=0.157297\n",
      "2025-09-10 12:00:29,883 : INFO : -7.769 per-word bound, 218.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:00:29,884 : INFO : PROGRESS: pass 32, at document #6000/14833\n",
      "2025-09-10 12:00:30,539 : INFO : optimized alpha [0.40041462, 0.072493844, 0.06791824, 0.33898097, 0.073505975]\n",
      "2025-09-10 12:00:30,544 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:30,550 : INFO : topic #0 (0.400): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:00:30,551 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:00:30,552 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:30,553 : INFO : topic #3 (0.339): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:00:30,554 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:30,554 : INFO : topic diff=0.087971, rho=0.157297\n",
      "2025-09-10 12:00:31,701 : INFO : -8.005 per-word bound, 256.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:00:31,701 : INFO : PROGRESS: pass 32, at document #8000/14833\n",
      "2025-09-10 12:00:32,349 : INFO : optimized alpha [0.4002234, 0.072456114, 0.067875884, 0.33778813, 0.07383706]\n",
      "2025-09-10 12:00:32,354 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:32,360 : INFO : topic #0 (0.400): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:32,361 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:00:32,362 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:00:32,363 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:00:32,364 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:00:32,364 : INFO : topic diff=0.083967, rho=0.157297\n",
      "2025-09-10 12:00:33,487 : INFO : -8.064 per-word bound, 267.6 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:00:33,488 : INFO : PROGRESS: pass 32, at document #10000/14833\n",
      "2025-09-10 12:00:34,120 : INFO : optimized alpha [0.39170164, 0.07239779, 0.06723534, 0.33527654, 0.074312724]\n",
      "2025-09-10 12:00:34,125 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:34,131 : INFO : topic #0 (0.392): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:34,132 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 12:00:34,133 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:34,134 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:34,134 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:00:34,135 : INFO : topic diff=0.085876, rho=0.157297\n",
      "2025-09-10 12:00:35,224 : INFO : -8.103 per-word bound, 275.0 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:00:35,225 : INFO : PROGRESS: pass 32, at document #12000/14833\n",
      "2025-09-10 12:00:35,821 : INFO : optimized alpha [0.3855948, 0.072117954, 0.06669118, 0.33357924, 0.072573505]\n",
      "2025-09-10 12:00:35,827 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:35,833 : INFO : topic #0 (0.386): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:35,834 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:00:35,835 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:00:35,836 : INFO : topic #3 (0.334): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:00:35,836 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:00:35,837 : INFO : topic diff=0.094044, rho=0.157297\n",
      "2025-09-10 12:00:37,040 : INFO : -8.515 per-word bound, 365.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:00:37,040 : INFO : PROGRESS: pass 32, at document #14000/14833\n",
      "2025-09-10 12:00:37,709 : INFO : optimized alpha [0.37399507, 0.07356013, 0.0696764, 0.3498973, 0.07462159]\n",
      "2025-09-10 12:00:37,714 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:37,720 : INFO : topic #0 (0.374): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:37,721 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:00:37,722 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:37,723 : INFO : topic #3 (0.350): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:37,724 : INFO : topic #4 (0.075): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:37,724 : INFO : topic diff=0.180179, rho=0.157297\n",
      "2025-09-10 12:00:38,226 : INFO : -8.404 per-word bound, 338.6 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:00:38,227 : INFO : PROGRESS: pass 32, at document #14833/14833\n",
      "2025-09-10 12:00:38,489 : INFO : optimized alpha [0.3409492, 0.07462299, 0.071280465, 0.36394003, 0.07427602]\n",
      "2025-09-10 12:00:38,494 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:38,500 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:38,501 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:00:38,502 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:00:38,503 : INFO : topic #3 (0.364): 0.010*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:38,503 : INFO : topic #4 (0.074): 0.011*\"time\" + 0.011*\"like\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:38,504 : INFO : topic diff=0.153664, rho=0.157297\n",
      "2025-09-10 12:00:39,880 : INFO : -7.960 per-word bound, 248.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:00:39,881 : INFO : PROGRESS: pass 33, at document #2000/14833\n",
      "2025-09-10 12:00:40,558 : INFO : optimized alpha [0.36153024, 0.07361463, 0.07057612, 0.32848582, 0.07502551]\n",
      "2025-09-10 12:00:40,563 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:40,569 : INFO : topic #0 (0.362): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:40,570 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:00:40,571 : INFO : topic #2 (0.071): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:40,572 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:40,573 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:40,574 : INFO : topic diff=0.135668, rho=0.155387\n",
      "2025-09-10 12:00:41,841 : INFO : -7.865 per-word bound, 233.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:00:41,842 : INFO : PROGRESS: pass 33, at document #4000/14833\n",
      "2025-09-10 12:00:42,522 : INFO : optimized alpha [0.3830459, 0.073034726, 0.069561355, 0.32753196, 0.07542917]\n",
      "2025-09-10 12:00:42,528 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:42,533 : INFO : topic #0 (0.383): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:42,534 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:00:42,535 : INFO : topic #2 (0.070): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:42,536 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:42,537 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:42,538 : INFO : topic diff=0.087056, rho=0.155387\n",
      "2025-09-10 12:00:43,732 : INFO : -7.769 per-word bound, 218.1 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:00:43,732 : INFO : PROGRESS: pass 33, at document #6000/14833\n",
      "2025-09-10 12:00:44,393 : INFO : optimized alpha [0.39968738, 0.07254584, 0.06784939, 0.33855686, 0.07341391]\n",
      "2025-09-10 12:00:44,398 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:44,404 : INFO : topic #0 (0.400): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:00:44,405 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:00:44,406 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:44,407 : INFO : topic #3 (0.339): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:00:44,408 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:44,408 : INFO : topic diff=0.086797, rho=0.155387\n",
      "2025-09-10 12:00:45,555 : INFO : -8.004 per-word bound, 256.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:00:45,556 : INFO : PROGRESS: pass 33, at document #8000/14833\n",
      "2025-09-10 12:00:46,207 : INFO : optimized alpha [0.39952666, 0.07250684, 0.067803614, 0.33735007, 0.073748596]\n",
      "2025-09-10 12:00:46,212 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:46,218 : INFO : topic #0 (0.400): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:00:46,219 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:00:46,220 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:00:46,220 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:00:46,221 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:00:46,222 : INFO : topic diff=0.082749, rho=0.155387\n",
      "2025-09-10 12:00:47,358 : INFO : -8.063 per-word bound, 267.5 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:00:47,359 : INFO : PROGRESS: pass 33, at document #10000/14833\n",
      "2025-09-10 12:00:47,994 : INFO : optimized alpha [0.39110613, 0.07244926, 0.06717781, 0.33481595, 0.074203834]\n",
      "2025-09-10 12:00:47,999 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:48,005 : INFO : topic #0 (0.391): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:48,006 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"actually\"\n",
      "2025-09-10 12:00:48,007 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:48,008 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:48,009 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:00:48,009 : INFO : topic diff=0.084714, rho=0.155387\n",
      "2025-09-10 12:00:49,107 : INFO : -8.103 per-word bound, 274.9 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:00:49,108 : INFO : PROGRESS: pass 33, at document #12000/14833\n",
      "2025-09-10 12:00:49,708 : INFO : optimized alpha [0.38514715, 0.0721691, 0.06662623, 0.3331075, 0.0724955]\n",
      "2025-09-10 12:00:49,713 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:49,720 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:49,721 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"language\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:00:49,722 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:00:49,723 : INFO : topic #3 (0.333): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:00:49,723 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:00:49,724 : INFO : topic diff=0.092794, rho=0.155387\n",
      "2025-09-10 12:00:50,936 : INFO : -8.513 per-word bound, 365.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:00:50,937 : INFO : PROGRESS: pass 33, at document #14000/14833\n",
      "2025-09-10 12:00:51,606 : INFO : optimized alpha [0.37364435, 0.07358671, 0.06956583, 0.34919122, 0.07453333]\n",
      "2025-09-10 12:00:51,611 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:51,617 : INFO : topic #0 (0.374): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:51,618 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:00:51,619 : INFO : topic #2 (0.070): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:51,620 : INFO : topic #3 (0.349): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:51,621 : INFO : topic #4 (0.075): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:00:51,621 : INFO : topic diff=0.177529, rho=0.155387\n",
      "2025-09-10 12:00:52,122 : INFO : -8.402 per-word bound, 338.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:00:52,122 : INFO : PROGRESS: pass 33, at document #14833/14833\n",
      "2025-09-10 12:00:52,385 : INFO : optimized alpha [0.34093234, 0.07463375, 0.07114979, 0.363021, 0.07419341]\n",
      "2025-09-10 12:00:52,390 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:52,396 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:00:52,397 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:00:52,398 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:00:52,399 : INFO : topic #3 (0.363): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:00:52,399 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:52,400 : INFO : topic diff=0.151756, rho=0.155387\n",
      "2025-09-10 12:00:53,773 : INFO : -7.959 per-word bound, 248.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:00:53,773 : INFO : PROGRESS: pass 34, at document #2000/14833\n",
      "2025-09-10 12:00:54,452 : INFO : optimized alpha [0.3612201, 0.07365302, 0.07046132, 0.3281597, 0.07493346]\n",
      "2025-09-10 12:00:54,458 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:54,463 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:54,464 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:00:54,465 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:54,466 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:54,467 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:00:54,467 : INFO : topic diff=0.133788, rho=0.153544\n",
      "2025-09-10 12:00:55,736 : INFO : -7.864 per-word bound, 233.0 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:00:55,737 : INFO : PROGRESS: pass 34, at document #4000/14833\n",
      "2025-09-10 12:00:56,415 : INFO : optimized alpha [0.3824449, 0.07307871, 0.06945772, 0.3272003, 0.075333595]\n",
      "2025-09-10 12:00:56,420 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:56,426 : INFO : topic #0 (0.382): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:00:56,427 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"financial\"\n",
      "2025-09-10 12:00:56,428 : INFO : topic #2 (0.069): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:00:56,429 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:00:56,430 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:56,430 : INFO : topic diff=0.085899, rho=0.153544\n",
      "2025-09-10 12:00:57,614 : INFO : -7.768 per-word bound, 218.0 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:00:57,614 : INFO : PROGRESS: pass 34, at document #6000/14833\n",
      "2025-09-10 12:00:58,266 : INFO : optimized alpha [0.39887583, 0.07259909, 0.06777183, 0.33803147, 0.07334219]\n",
      "2025-09-10 12:00:58,271 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:00:58,276 : INFO : topic #0 (0.399): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:00:58,278 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:00:58,278 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:00:58,279 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:00:58,280 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:00:58,280 : INFO : topic diff=0.085731, rho=0.153544\n",
      "2025-09-10 12:00:59,419 : INFO : -8.004 per-word bound, 256.6 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:00:59,420 : INFO : PROGRESS: pass 34, at document #8000/14833\n",
      "2025-09-10 12:01:00,059 : INFO : optimized alpha [0.39876264, 0.07255211, 0.067728505, 0.33683184, 0.073675305]\n",
      "2025-09-10 12:01:00,065 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:00,070 : INFO : topic #0 (0.399): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:01:00,071 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:00,072 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:01:00,073 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:01:00,074 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:01:00,074 : INFO : topic diff=0.081631, rho=0.153544\n",
      "2025-09-10 12:01:01,197 : INFO : -8.063 per-word bound, 267.4 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:01:01,198 : INFO : PROGRESS: pass 34, at document #10000/14833\n",
      "2025-09-10 12:01:01,820 : INFO : optimized alpha [0.390541, 0.072498165, 0.06709686, 0.33436355, 0.074127324]\n",
      "2025-09-10 12:01:01,825 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:01,831 : INFO : topic #0 (0.391): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:01,832 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"actually\"\n",
      "2025-09-10 12:01:01,833 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:01,834 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:01,835 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:01:01,835 : INFO : topic diff=0.083674, rho=0.153544\n",
      "2025-09-10 12:01:02,915 : INFO : -8.102 per-word bound, 274.8 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:01:02,916 : INFO : PROGRESS: pass 34, at document #12000/14833\n",
      "2025-09-10 12:01:03,507 : INFO : optimized alpha [0.38469747, 0.07221781, 0.06655445, 0.332662, 0.07244102]\n",
      "2025-09-10 12:01:03,512 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:03,518 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:03,519 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:01:03,520 : INFO : topic #2 (0.067): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:01:03,521 : INFO : topic #3 (0.333): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:01:03,522 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:01:03,522 : INFO : topic diff=0.091517, rho=0.153544\n",
      "2025-09-10 12:01:04,720 : INFO : -8.511 per-word bound, 364.9 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:01:04,721 : INFO : PROGRESS: pass 34, at document #14000/14833\n",
      "2025-09-10 12:01:05,387 : INFO : optimized alpha [0.37328354, 0.07362317, 0.06947492, 0.348527, 0.07444118]\n",
      "2025-09-10 12:01:05,392 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:05,398 : INFO : topic #0 (0.373): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:05,399 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:01:05,400 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:05,401 : INFO : topic #3 (0.349): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:05,402 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:05,402 : INFO : topic diff=0.174879, rho=0.153544\n",
      "2025-09-10 12:01:05,897 : INFO : -8.401 per-word bound, 338.1 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:01:05,898 : INFO : PROGRESS: pass 34, at document #14833/14833\n",
      "2025-09-10 12:01:06,159 : INFO : optimized alpha [0.3409369, 0.07465483, 0.07103939, 0.3621573, 0.074107364]\n",
      "2025-09-10 12:01:06,164 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:06,169 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:06,170 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:01:06,171 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:01:06,172 : INFO : topic #3 (0.362): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:06,173 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"death\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:06,174 : INFO : topic diff=0.149940, rho=0.153544\n",
      "2025-09-10 12:01:07,537 : INFO : -7.958 per-word bound, 248.6 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:01:07,538 : INFO : PROGRESS: pass 35, at document #2000/14833\n",
      "2025-09-10 12:01:08,208 : INFO : optimized alpha [0.36100814, 0.07370055, 0.070371486, 0.32784432, 0.0748411]\n",
      "2025-09-10 12:01:08,213 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:08,219 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:08,220 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:01:08,221 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:08,221 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:08,223 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"life\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:08,223 : INFO : topic diff=0.131989, rho=0.151765\n",
      "2025-09-10 12:01:09,497 : INFO : -7.864 per-word bound, 232.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:01:09,497 : INFO : PROGRESS: pass 35, at document #4000/14833\n",
      "2025-09-10 12:01:10,178 : INFO : optimized alpha [0.3819779, 0.073130764, 0.06937748, 0.3268593, 0.07523786]\n",
      "2025-09-10 12:01:10,183 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:10,189 : INFO : topic #0 (0.382): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:10,190 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"financial\"\n",
      "2025-09-10 12:01:10,191 : INFO : topic #2 (0.069): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:10,191 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:10,192 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:10,193 : INFO : topic diff=0.084869, rho=0.151765\n",
      "2025-09-10 12:01:11,378 : INFO : -7.768 per-word bound, 218.0 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:01:11,379 : INFO : PROGRESS: pass 35, at document #6000/14833\n",
      "2025-09-10 12:01:12,025 : INFO : optimized alpha [0.39821777, 0.0726506, 0.06771567, 0.33753225, 0.07327526]\n",
      "2025-09-10 12:01:12,030 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:12,036 : INFO : topic #0 (0.398): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:01:12,037 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"sleep\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:01:12,038 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:12,039 : INFO : topic #3 (0.338): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:01:12,040 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:12,041 : INFO : topic diff=0.084661, rho=0.151765\n",
      "2025-09-10 12:01:13,178 : INFO : -8.003 per-word bound, 256.5 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:01:13,179 : INFO : PROGRESS: pass 35, at document #8000/14833\n",
      "2025-09-10 12:01:13,816 : INFO : optimized alpha [0.39814273, 0.07260592, 0.06767402, 0.3363444, 0.07360378]\n",
      "2025-09-10 12:01:13,821 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:13,827 : INFO : topic #0 (0.398): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:01:13,828 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:13,829 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:01:13,830 : INFO : topic #3 (0.336): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:01:13,830 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:01:13,831 : INFO : topic diff=0.080492, rho=0.151765\n",
      "2025-09-10 12:01:14,950 : INFO : -8.063 per-word bound, 267.3 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:01:14,951 : INFO : PROGRESS: pass 35, at document #10000/14833\n",
      "2025-09-10 12:01:15,584 : INFO : optimized alpha [0.390018, 0.07255017, 0.067045845, 0.33390987, 0.07405903]\n",
      "2025-09-10 12:01:15,589 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:15,595 : INFO : topic #0 (0.390): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:15,596 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"actually\"\n",
      "2025-09-10 12:01:15,597 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:15,598 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:15,599 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:01:15,599 : INFO : topic diff=0.082640, rho=0.151765\n",
      "2025-09-10 12:01:16,684 : INFO : -8.102 per-word bound, 274.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:01:16,685 : INFO : PROGRESS: pass 35, at document #12000/14833\n",
      "2025-09-10 12:01:17,274 : INFO : optimized alpha [0.3842776, 0.072269194, 0.06649986, 0.33221397, 0.07238342]\n",
      "2025-09-10 12:01:17,279 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:17,285 : INFO : topic #0 (0.384): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:17,286 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:01:17,287 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:01:17,288 : INFO : topic #3 (0.332): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"still\"\n",
      "2025-09-10 12:01:17,288 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"something\" + 0.009*\"think\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:01:17,289 : INFO : topic diff=0.090358, rho=0.151765\n",
      "2025-09-10 12:01:18,490 : INFO : -8.510 per-word bound, 364.5 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:01:18,490 : INFO : PROGRESS: pass 35, at document #14000/14833\n",
      "2025-09-10 12:01:19,158 : INFO : optimized alpha [0.37295043, 0.07367061, 0.06938573, 0.34785518, 0.07434379]\n",
      "2025-09-10 12:01:19,163 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:19,169 : INFO : topic #0 (0.373): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:19,170 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:01:19,171 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:19,172 : INFO : topic #3 (0.348): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:19,173 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:19,173 : INFO : topic diff=0.172369, rho=0.151765\n",
      "2025-09-10 12:01:19,671 : INFO : -8.400 per-word bound, 337.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:01:19,671 : INFO : PROGRESS: pass 35, at document #14833/14833\n",
      "2025-09-10 12:01:19,932 : INFO : optimized alpha [0.3409534, 0.07468663, 0.07091811, 0.36129093, 0.07401538]\n",
      "2025-09-10 12:01:19,937 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:19,943 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:19,944 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:01:19,945 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:01:19,946 : INFO : topic #3 (0.361): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:19,946 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:19,947 : INFO : topic diff=0.148150, rho=0.151765\n",
      "2025-09-10 12:01:21,310 : INFO : -7.957 per-word bound, 248.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:01:21,311 : INFO : PROGRESS: pass 36, at document #2000/14833\n",
      "2025-09-10 12:01:21,981 : INFO : optimized alpha [0.36078322, 0.07374476, 0.0702647, 0.3275275, 0.07474472]\n",
      "2025-09-10 12:01:21,986 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:21,992 : INFO : topic #0 (0.361): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:21,993 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:01:21,994 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:21,995 : INFO : topic #3 (0.328): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:21,996 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:21,996 : INFO : topic diff=0.130298, rho=0.150047\n",
      "2025-09-10 12:01:23,279 : INFO : -7.863 per-word bound, 232.8 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:01:23,280 : INFO : PROGRESS: pass 36, at document #4000/14833\n",
      "2025-09-10 12:01:23,965 : INFO : optimized alpha [0.3814819, 0.0731841, 0.06927564, 0.3265, 0.07513283]\n",
      "2025-09-10 12:01:23,970 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:23,976 : INFO : topic #0 (0.381): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:23,977 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"financial\"\n",
      "2025-09-10 12:01:23,978 : INFO : topic #2 (0.069): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:23,979 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:23,979 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:23,980 : INFO : topic diff=0.083821, rho=0.150047\n",
      "2025-09-10 12:01:25,182 : INFO : -7.767 per-word bound, 217.9 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:01:25,183 : INFO : PROGRESS: pass 36, at document #6000/14833\n",
      "2025-09-10 12:01:25,838 : INFO : optimized alpha [0.39753318, 0.072704695, 0.067638606, 0.33702287, 0.073198594]\n",
      "2025-09-10 12:01:25,844 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:25,849 : INFO : topic #0 (0.398): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:01:25,850 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:01:25,851 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:25,852 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:01:25,853 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"maybe\" + 0.006*\"even\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:25,853 : INFO : topic diff=0.083638, rho=0.150047\n",
      "2025-09-10 12:01:26,987 : INFO : -8.003 per-word bound, 256.5 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:01:26,988 : INFO : PROGRESS: pass 36, at document #8000/14833\n",
      "2025-09-10 12:01:27,627 : INFO : optimized alpha [0.3974854, 0.07264761, 0.06759905, 0.3358412, 0.073518656]\n",
      "2025-09-10 12:01:27,632 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:27,638 : INFO : topic #0 (0.397): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:01:27,639 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:27,640 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:01:27,641 : INFO : topic #3 (0.336): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:01:27,642 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:01:27,642 : INFO : topic diff=0.079460, rho=0.150047\n",
      "2025-09-10 12:01:28,762 : INFO : -8.062 per-word bound, 267.3 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:01:28,762 : INFO : PROGRESS: pass 36, at document #10000/14833\n",
      "2025-09-10 12:01:29,387 : INFO : optimized alpha [0.38953352, 0.07259971, 0.06697439, 0.33344665, 0.07397938]\n",
      "2025-09-10 12:01:29,392 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:29,398 : INFO : topic #0 (0.390): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:29,398 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"actually\"\n",
      "2025-09-10 12:01:29,399 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:29,400 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:29,401 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.007*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:01:29,401 : INFO : topic diff=0.081627, rho=0.150047\n",
      "2025-09-10 12:01:30,479 : INFO : -8.101 per-word bound, 274.6 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:01:30,480 : INFO : PROGRESS: pass 36, at document #12000/14833\n",
      "2025-09-10 12:01:31,072 : INFO : optimized alpha [0.38385352, 0.07232343, 0.06644696, 0.33176616, 0.07231465]\n",
      "2025-09-10 12:01:31,077 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:31,083 : INFO : topic #0 (0.384): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:31,084 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:01:31,085 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:01:31,085 : INFO : topic #3 (0.332): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:01:31,086 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:01:31,087 : INFO : topic diff=0.089194, rho=0.150047\n",
      "2025-09-10 12:01:32,288 : INFO : -8.508 per-word bound, 364.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:01:32,288 : INFO : PROGRESS: pass 36, at document #14000/14833\n",
      "2025-09-10 12:01:32,952 : INFO : optimized alpha [0.3725959, 0.073724136, 0.06927762, 0.34717822, 0.07425665]\n",
      "2025-09-10 12:01:32,957 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:32,963 : INFO : topic #0 (0.373): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:32,964 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:01:32,964 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:32,965 : INFO : topic #3 (0.347): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:32,966 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:32,967 : INFO : topic diff=0.169919, rho=0.150047\n",
      "2025-09-10 12:01:33,463 : INFO : -8.399 per-word bound, 337.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:01:33,464 : INFO : PROGRESS: pass 36, at document #14833/14833\n",
      "2025-09-10 12:01:33,727 : INFO : optimized alpha [0.34092954, 0.074724, 0.07079156, 0.3603882, 0.07391893]\n",
      "2025-09-10 12:01:33,732 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:33,737 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:33,738 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:01:33,739 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:01:33,740 : INFO : topic #3 (0.360): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:33,741 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:33,742 : INFO : topic diff=0.146443, rho=0.150047\n",
      "2025-09-10 12:01:35,112 : INFO : -7.956 per-word bound, 248.3 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:01:35,113 : INFO : PROGRESS: pass 37, at document #2000/14833\n",
      "2025-09-10 12:01:35,789 : INFO : optimized alpha [0.36049542, 0.07379848, 0.070152275, 0.32716328, 0.07464286]\n",
      "2025-09-10 12:01:35,795 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:35,801 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:35,802 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:01:35,803 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:35,803 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:35,804 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:35,805 : INFO : topic diff=0.128702, rho=0.148386\n",
      "2025-09-10 12:01:37,081 : INFO : -7.863 per-word bound, 232.7 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:01:37,082 : INFO : PROGRESS: pass 37, at document #4000/14833\n",
      "2025-09-10 12:01:37,763 : INFO : optimized alpha [0.38094282, 0.073240146, 0.0691797, 0.32613426, 0.07503313]\n",
      "2025-09-10 12:01:37,768 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:37,774 : INFO : topic #0 (0.381): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:37,775 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"like\" + 0.005*\"financial\"\n",
      "2025-09-10 12:01:37,776 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:37,777 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:37,778 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:37,778 : INFO : topic diff=0.082839, rho=0.148386\n",
      "2025-09-10 12:01:38,968 : INFO : -7.767 per-word bound, 217.8 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:01:38,969 : INFO : PROGRESS: pass 37, at document #6000/14833\n",
      "2025-09-10 12:01:39,627 : INFO : optimized alpha [0.3968069, 0.07276377, 0.067566186, 0.33650935, 0.07312577]\n",
      "2025-09-10 12:01:39,633 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:39,638 : INFO : topic #0 (0.397): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:01:39,639 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:01:39,640 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:39,641 : INFO : topic #3 (0.337): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:01:39,642 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:39,642 : INFO : topic diff=0.082663, rho=0.148386\n",
      "2025-09-10 12:01:40,782 : INFO : -8.002 per-word bound, 256.4 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:01:40,783 : INFO : PROGRESS: pass 37, at document #8000/14833\n",
      "2025-09-10 12:01:41,429 : INFO : optimized alpha [0.39677933, 0.072710805, 0.06752841, 0.33532357, 0.07344663]\n",
      "2025-09-10 12:01:41,435 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:41,440 : INFO : topic #0 (0.397): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"also\"\n",
      "2025-09-10 12:01:41,441 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:41,442 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:01:41,443 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:01:41,444 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"sound\"\n",
      "2025-09-10 12:01:41,444 : INFO : topic diff=0.078375, rho=0.148386\n",
      "2025-09-10 12:01:42,570 : INFO : -8.062 per-word bound, 267.2 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:01:42,570 : INFO : PROGRESS: pass 37, at document #10000/14833\n",
      "2025-09-10 12:01:43,203 : INFO : optimized alpha [0.38895014, 0.07268028, 0.066904575, 0.33295122, 0.07390455]\n",
      "2025-09-10 12:01:43,208 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:43,214 : INFO : topic #0 (0.389): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:43,215 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"actually\"\n",
      "2025-09-10 12:01:43,216 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:43,217 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:43,217 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:01:43,218 : INFO : topic diff=0.080663, rho=0.148386\n",
      "2025-09-10 12:01:44,307 : INFO : -8.101 per-word bound, 274.5 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:01:44,308 : INFO : PROGRESS: pass 37, at document #12000/14833\n",
      "2025-09-10 12:01:44,906 : INFO : optimized alpha [0.38340542, 0.072403915, 0.066375576, 0.33130133, 0.07226123]\n",
      "2025-09-10 12:01:44,911 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:44,917 : INFO : topic #0 (0.383): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:44,918 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:01:44,919 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:01:44,920 : INFO : topic #3 (0.331): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:01:44,920 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:01:44,921 : INFO : topic diff=0.088048, rho=0.148386\n",
      "2025-09-10 12:01:46,126 : INFO : -8.506 per-word bound, 363.7 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:01:46,127 : INFO : PROGRESS: pass 37, at document #14000/14833\n",
      "2025-09-10 12:01:46,798 : INFO : optimized alpha [0.37224108, 0.07381151, 0.06916113, 0.34651017, 0.0741741]\n",
      "2025-09-10 12:01:46,803 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:46,809 : INFO : topic #0 (0.372): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:46,810 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:01:46,811 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:46,812 : INFO : topic #3 (0.347): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:46,813 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"something\" + 0.008*\"think\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:46,813 : INFO : topic diff=0.167617, rho=0.148386\n",
      "2025-09-10 12:01:47,315 : INFO : -8.399 per-word bound, 337.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:01:47,316 : INFO : PROGRESS: pass 37, at document #14833/14833\n",
      "2025-09-10 12:01:47,576 : INFO : optimized alpha [0.34089926, 0.0747955, 0.07065778, 0.3595404, 0.07384561]\n",
      "2025-09-10 12:01:47,581 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:47,587 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.012*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:01:47,588 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:01:47,589 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:01:47,590 : INFO : topic #3 (0.360): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:47,591 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:47,592 : INFO : topic diff=0.144744, rho=0.148386\n",
      "2025-09-10 12:01:48,965 : INFO : -7.955 per-word bound, 248.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:01:48,965 : INFO : PROGRESS: pass 38, at document #2000/14833\n",
      "2025-09-10 12:01:49,639 : INFO : optimized alpha [0.3602425, 0.0738785, 0.07003303, 0.32683718, 0.074564256]\n",
      "2025-09-10 12:01:49,644 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:49,650 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"might\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:49,651 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:01:49,652 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:49,653 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:49,654 : INFO : topic #4 (0.075): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:01:49,654 : INFO : topic diff=0.127114, rho=0.146779\n",
      "2025-09-10 12:01:50,927 : INFO : -7.862 per-word bound, 232.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:01:50,928 : INFO : PROGRESS: pass 38, at document #4000/14833\n",
      "2025-09-10 12:01:51,604 : INFO : optimized alpha [0.38044006, 0.0733164, 0.069066234, 0.32579672, 0.07494925]\n",
      "2025-09-10 12:01:51,610 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:51,615 : INFO : topic #0 (0.380): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:01:51,616 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:51,617 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:51,618 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:01:51,619 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:51,619 : INFO : topic diff=0.081877, rho=0.146779\n",
      "2025-09-10 12:01:52,804 : INFO : -7.767 per-word bound, 217.8 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:01:52,805 : INFO : PROGRESS: pass 38, at document #6000/14833\n",
      "2025-09-10 12:01:53,459 : INFO : optimized alpha [0.3961567, 0.072844945, 0.067477025, 0.33601752, 0.07306711]\n",
      "2025-09-10 12:01:53,464 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:53,470 : INFO : topic #0 (0.396): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:01:53,471 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:01:53,472 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:01:53,473 : INFO : topic #3 (0.336): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:01:53,473 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:53,474 : INFO : topic diff=0.081686, rho=0.146779\n",
      "2025-09-10 12:01:54,613 : INFO : -8.002 per-word bound, 256.3 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:01:54,614 : INFO : PROGRESS: pass 38, at document #8000/14833\n",
      "2025-09-10 12:01:55,258 : INFO : optimized alpha [0.39615977, 0.07279347, 0.06744185, 0.33482486, 0.07337464]\n",
      "2025-09-10 12:01:55,263 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:55,269 : INFO : topic #0 (0.396): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:01:55,270 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:01:55,271 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:01:55,271 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"know\"\n",
      "2025-09-10 12:01:55,272 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:01:55,273 : INFO : topic diff=0.077390, rho=0.146779\n",
      "2025-09-10 12:01:56,410 : INFO : -8.062 per-word bound, 267.1 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:01:56,411 : INFO : PROGRESS: pass 38, at document #10000/14833\n",
      "2025-09-10 12:01:57,035 : INFO : optimized alpha [0.3884533, 0.07276407, 0.06682803, 0.33248195, 0.073827006]\n",
      "2025-09-10 12:01:57,040 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:57,046 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:01:57,047 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:01:57,048 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:01:57,049 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:01:57,049 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:01:57,050 : INFO : topic diff=0.079663, rho=0.146779\n",
      "2025-09-10 12:01:58,142 : INFO : -8.100 per-word bound, 274.4 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:01:58,142 : INFO : PROGRESS: pass 38, at document #12000/14833\n",
      "2025-09-10 12:01:58,740 : INFO : optimized alpha [0.38299775, 0.07248587, 0.06630751, 0.3308508, 0.07220395]\n",
      "2025-09-10 12:01:58,745 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:01:58,751 : INFO : topic #0 (0.383): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:01:58,752 : INFO : topic #1 (0.072): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:01:58,753 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:01:58,753 : INFO : topic #3 (0.331): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:01:58,754 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:01:58,755 : INFO : topic diff=0.086984, rho=0.146779\n",
      "2025-09-10 12:01:59,952 : INFO : -8.505 per-word bound, 363.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:01:59,953 : INFO : PROGRESS: pass 38, at document #14000/14833\n",
      "2025-09-10 12:02:00,619 : INFO : optimized alpha [0.37190676, 0.073864475, 0.0690749, 0.3458536, 0.07408864]\n",
      "2025-09-10 12:02:00,624 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:00,630 : INFO : topic #0 (0.372): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:00,631 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:02:00,632 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:00,633 : INFO : topic #3 (0.346): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:00,633 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:00,634 : INFO : topic diff=0.165343, rho=0.146779\n",
      "2025-09-10 12:02:01,137 : INFO : -8.398 per-word bound, 337.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:02:01,137 : INFO : PROGRESS: pass 38, at document #14833/14833\n",
      "2025-09-10 12:02:01,399 : INFO : optimized alpha [0.3408714, 0.07484496, 0.07054174, 0.35871914, 0.07376063]\n",
      "2025-09-10 12:02:01,404 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:01,410 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:01,411 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:02:01,412 : INFO : topic #2 (0.071): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:02:01,413 : INFO : topic #3 (0.359): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:01,414 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"death\" + 0.007*\"something\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:01,414 : INFO : topic diff=0.143094, rho=0.146779\n",
      "2025-09-10 12:02:02,788 : INFO : -7.954 per-word bound, 248.0 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:02:02,789 : INFO : PROGRESS: pass 39, at document #2000/14833\n",
      "2025-09-10 12:02:03,467 : INFO : optimized alpha [0.35999697, 0.073936425, 0.069929734, 0.32650676, 0.074474916]\n",
      "2025-09-10 12:02:03,472 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:03,478 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:03,479 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:03,480 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"human\" + 0.003*\"back\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:03,480 : INFO : topic #3 (0.327): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:03,481 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:03,482 : INFO : topic diff=0.125551, rho=0.145223\n",
      "2025-09-10 12:02:04,751 : INFO : -7.861 per-word bound, 232.6 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:02:04,752 : INFO : PROGRESS: pass 39, at document #4000/14833\n",
      "2025-09-10 12:02:05,427 : INFO : optimized alpha [0.3799631, 0.07338654, 0.06897823, 0.32547355, 0.0748585]\n",
      "2025-09-10 12:02:05,432 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:05,438 : INFO : topic #0 (0.380): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:05,439 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:05,439 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:05,440 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:05,441 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:05,441 : INFO : topic diff=0.080873, rho=0.145223\n",
      "2025-09-10 12:02:06,628 : INFO : -7.766 per-word bound, 217.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:02:06,629 : INFO : PROGRESS: pass 39, at document #6000/14833\n",
      "2025-09-10 12:02:07,289 : INFO : optimized alpha [0.395475, 0.07291583, 0.06741048, 0.33556196, 0.072996]\n",
      "2025-09-10 12:02:07,294 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:07,300 : INFO : topic #0 (0.395): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:02:07,301 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:02:07,301 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:07,302 : INFO : topic #3 (0.336): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:02:07,303 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:07,304 : INFO : topic diff=0.080768, rho=0.145223\n",
      "2025-09-10 12:02:08,452 : INFO : -8.001 per-word bound, 256.2 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:02:08,453 : INFO : PROGRESS: pass 39, at document #8000/14833\n",
      "2025-09-10 12:02:09,097 : INFO : optimized alpha [0.3955094, 0.07286487, 0.06737726, 0.3343865, 0.073301785]\n",
      "2025-09-10 12:02:09,102 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:09,108 : INFO : topic #0 (0.396): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:02:09,109 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:09,110 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:02:09,111 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:02:09,112 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:02:09,112 : INFO : topic diff=0.076386, rho=0.145223\n",
      "2025-09-10 12:02:10,242 : INFO : -8.061 per-word bound, 267.1 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:02:10,243 : INFO : PROGRESS: pass 39, at document #10000/14833\n",
      "2025-09-10 12:02:10,872 : INFO : optimized alpha [0.38788813, 0.07284506, 0.06676606, 0.3320367, 0.073745996]\n",
      "2025-09-10 12:02:10,877 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:10,883 : INFO : topic #0 (0.388): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:10,885 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:10,886 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:10,887 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:10,887 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:02:10,888 : INFO : topic diff=0.078752, rho=0.145223\n",
      "2025-09-10 12:02:11,976 : INFO : -8.100 per-word bound, 274.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:02:11,976 : INFO : PROGRESS: pass 39, at document #12000/14833\n",
      "2025-09-10 12:02:12,570 : INFO : optimized alpha [0.38253054, 0.07256531, 0.066253126, 0.33042794, 0.07214226]\n",
      "2025-09-10 12:02:12,575 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:12,581 : INFO : topic #0 (0.383): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:12,582 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:02:12,583 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:02:12,584 : INFO : topic #3 (0.330): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:02:12,585 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:02:12,585 : INFO : topic diff=0.085941, rho=0.145223\n",
      "2025-09-10 12:02:13,791 : INFO : -8.504 per-word bound, 362.9 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:02:13,791 : INFO : PROGRESS: pass 39, at document #14000/14833\n",
      "2025-09-10 12:02:14,460 : INFO : optimized alpha [0.37153798, 0.07394596, 0.06897339, 0.34525186, 0.07398944]\n",
      "2025-09-10 12:02:14,465 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:14,471 : INFO : topic #0 (0.372): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:14,471 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:02:14,472 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:14,473 : INFO : topic #3 (0.345): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:14,474 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:14,475 : INFO : topic diff=0.163251, rho=0.145223\n",
      "2025-09-10 12:02:14,974 : INFO : -8.397 per-word bound, 337.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:02:14,974 : INFO : PROGRESS: pass 39, at document #14833/14833\n",
      "2025-09-10 12:02:15,236 : INFO : optimized alpha [0.34080777, 0.07492848, 0.07042448, 0.35796013, 0.073662706]\n",
      "2025-09-10 12:02:15,241 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:15,246 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:15,247 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:02:15,248 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:02:15,249 : INFO : topic #3 (0.358): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:15,250 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:15,250 : INFO : topic diff=0.141576, rho=0.145223\n",
      "2025-09-10 12:02:16,610 : INFO : -7.953 per-word bound, 247.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:02:16,611 : INFO : PROGRESS: pass 40, at document #2000/14833\n",
      "2025-09-10 12:02:17,276 : INFO : optimized alpha [0.35970956, 0.07402858, 0.06981918, 0.326144, 0.07437806]\n",
      "2025-09-10 12:02:17,281 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:17,287 : INFO : topic #0 (0.360): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:17,288 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:17,289 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:17,290 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:17,290 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:17,291 : INFO : topic diff=0.124165, rho=0.143715\n",
      "2025-09-10 12:02:18,559 : INFO : -7.861 per-word bound, 232.5 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:02:18,559 : INFO : PROGRESS: pass 40, at document #4000/14833\n",
      "2025-09-10 12:02:19,235 : INFO : optimized alpha [0.37944546, 0.07349605, 0.0688778, 0.3250866, 0.07476001]\n",
      "2025-09-10 12:02:19,240 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:19,246 : INFO : topic #0 (0.379): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:19,247 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:19,248 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:19,249 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:19,250 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:19,250 : INFO : topic diff=0.079954, rho=0.143715\n",
      "2025-09-10 12:02:20,442 : INFO : -7.766 per-word bound, 217.7 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:02:20,442 : INFO : PROGRESS: pass 40, at document #6000/14833\n",
      "2025-09-10 12:02:21,089 : INFO : optimized alpha [0.39480057, 0.07302917, 0.06733247, 0.33508798, 0.07292158]\n",
      "2025-09-10 12:02:21,094 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:21,100 : INFO : topic #0 (0.395): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:02:21,101 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:02:21,102 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:21,103 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"know\" + 0.005*\"get\" + 0.005*\"said\"\n",
      "2025-09-10 12:02:21,103 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:21,104 : INFO : topic diff=0.079874, rho=0.143715\n",
      "2025-09-10 12:02:22,237 : INFO : -8.001 per-word bound, 256.2 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:02:22,238 : INFO : PROGRESS: pass 40, at document #8000/14833\n",
      "2025-09-10 12:02:22,872 : INFO : optimized alpha [0.39486915, 0.072968245, 0.06730189, 0.3339627, 0.07322611]\n",
      "2025-09-10 12:02:22,877 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:22,883 : INFO : topic #0 (0.395): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:02:22,884 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:22,885 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:02:22,886 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:02:22,886 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:02:22,887 : INFO : topic diff=0.075463, rho=0.143715\n",
      "2025-09-10 12:02:24,009 : INFO : -8.061 per-word bound, 267.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:02:24,010 : INFO : PROGRESS: pass 40, at document #10000/14833\n",
      "2025-09-10 12:02:24,631 : INFO : optimized alpha [0.38735536, 0.072957985, 0.066704325, 0.33162177, 0.073667]\n",
      "2025-09-10 12:02:24,636 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:24,641 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:24,643 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:24,643 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:24,644 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:24,645 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:02:24,646 : INFO : topic diff=0.077871, rho=0.143715\n",
      "2025-09-10 12:02:25,718 : INFO : -8.099 per-word bound, 274.3 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:02:25,719 : INFO : PROGRESS: pass 40, at document #12000/14833\n",
      "2025-09-10 12:02:26,299 : INFO : optimized alpha [0.38208556, 0.07267706, 0.06619863, 0.33000708, 0.07208361]\n",
      "2025-09-10 12:02:26,304 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:26,310 : INFO : topic #0 (0.382): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:26,311 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:02:26,312 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:02:26,312 : INFO : topic #3 (0.330): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:02:26,313 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:02:26,314 : INFO : topic diff=0.084947, rho=0.143715\n",
      "2025-09-10 12:02:27,504 : INFO : -8.502 per-word bound, 362.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:02:27,505 : INFO : PROGRESS: pass 40, at document #14000/14833\n",
      "2025-09-10 12:02:28,167 : INFO : optimized alpha [0.3711646, 0.07404026, 0.06889392, 0.34465322, 0.07391556]\n",
      "2025-09-10 12:02:28,172 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:28,178 : INFO : topic #0 (0.371): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:28,179 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:02:28,179 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:28,180 : INFO : topic #3 (0.345): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:28,181 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:28,181 : INFO : topic diff=0.161117, rho=0.143715\n",
      "2025-09-10 12:02:28,679 : INFO : -8.396 per-word bound, 336.9 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:02:28,680 : INFO : PROGRESS: pass 40, at document #14833/14833\n",
      "2025-09-10 12:02:28,943 : INFO : optimized alpha [0.34073508, 0.07500933, 0.07032935, 0.35721192, 0.07359326]\n",
      "2025-09-10 12:02:28,948 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:28,954 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:28,955 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:02:28,956 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:02:28,957 : INFO : topic #3 (0.357): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:28,958 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:28,958 : INFO : topic diff=0.140002, rho=0.143715\n",
      "2025-09-10 12:02:30,343 : INFO : -7.952 per-word bound, 247.7 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:02:30,344 : INFO : PROGRESS: pass 41, at document #2000/14833\n",
      "2025-09-10 12:02:31,019 : INFO : optimized alpha [0.35943982, 0.07412138, 0.069730364, 0.32587343, 0.07430439]\n",
      "2025-09-10 12:02:31,024 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:31,030 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:31,031 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:31,032 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:31,033 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:31,034 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:31,034 : INFO : topic diff=0.122711, rho=0.142254\n",
      "2025-09-10 12:02:32,313 : INFO : -7.860 per-word bound, 232.4 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:02:32,313 : INFO : PROGRESS: pass 41, at document #4000/14833\n",
      "2025-09-10 12:02:32,993 : INFO : optimized alpha [0.37891486, 0.07359141, 0.068807915, 0.32481787, 0.074683905]\n",
      "2025-09-10 12:02:32,998 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:33,004 : INFO : topic #0 (0.379): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:33,005 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:33,006 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:33,007 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:33,007 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:33,008 : INFO : topic diff=0.079086, rho=0.142254\n",
      "2025-09-10 12:02:34,194 : INFO : -7.766 per-word bound, 217.6 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:02:34,194 : INFO : PROGRESS: pass 41, at document #6000/14833\n",
      "2025-09-10 12:02:34,843 : INFO : optimized alpha [0.3941056, 0.0731306, 0.067282006, 0.3347002, 0.07286956]\n",
      "2025-09-10 12:02:34,848 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:34,854 : INFO : topic #0 (0.394): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:02:34,855 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:02:34,856 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:34,857 : INFO : topic #3 (0.335): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:02:34,858 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:34,858 : INFO : topic diff=0.079021, rho=0.142254\n",
      "2025-09-10 12:02:35,991 : INFO : -8.000 per-word bound, 256.1 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:02:35,992 : INFO : PROGRESS: pass 41, at document #8000/14833\n",
      "2025-09-10 12:02:36,629 : INFO : optimized alpha [0.39420056, 0.073075056, 0.067252435, 0.3335531, 0.073171504]\n",
      "2025-09-10 12:02:36,634 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:36,640 : INFO : topic #0 (0.394): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:02:36,641 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:36,642 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:02:36,643 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:02:36,644 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:02:36,644 : INFO : topic diff=0.074575, rho=0.142254\n",
      "2025-09-10 12:02:37,760 : INFO : -8.061 per-word bound, 267.0 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:02:37,761 : INFO : PROGRESS: pass 41, at document #10000/14833\n",
      "2025-09-10 12:02:38,383 : INFO : optimized alpha [0.3867929, 0.07306034, 0.06665687, 0.33123633, 0.073611334]\n",
      "2025-09-10 12:02:38,388 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:38,394 : INFO : topic #0 (0.387): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:38,395 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"language\" + 0.006*\"business\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:38,396 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:38,397 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:38,398 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:02:38,398 : INFO : topic diff=0.076977, rho=0.142254\n",
      "2025-09-10 12:02:39,473 : INFO : -8.099 per-word bound, 274.2 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:02:39,474 : INFO : PROGRESS: pass 41, at document #12000/14833\n",
      "2025-09-10 12:02:40,060 : INFO : optimized alpha [0.38161388, 0.072779104, 0.06615761, 0.32962596, 0.07204043]\n",
      "2025-09-10 12:02:40,065 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:40,071 : INFO : topic #0 (0.382): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:40,071 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:02:40,072 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:02:40,073 : INFO : topic #3 (0.330): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:02:40,074 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:02:40,074 : INFO : topic diff=0.083979, rho=0.142254\n",
      "2025-09-10 12:02:41,273 : INFO : -8.501 per-word bound, 362.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:02:41,273 : INFO : PROGRESS: pass 41, at document #14000/14833\n",
      "2025-09-10 12:02:41,940 : INFO : optimized alpha [0.370792, 0.074129365, 0.06881408, 0.3441142, 0.07386478]\n",
      "2025-09-10 12:02:41,946 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:41,952 : INFO : topic #0 (0.371): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:41,952 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:02:41,953 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:41,955 : INFO : topic #3 (0.344): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:41,955 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:41,956 : INFO : topic diff=0.159058, rho=0.142254\n",
      "2025-09-10 12:02:42,457 : INFO : -8.395 per-word bound, 336.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:02:42,457 : INFO : PROGRESS: pass 41, at document #14833/14833\n",
      "2025-09-10 12:02:42,718 : INFO : optimized alpha [0.34065127, 0.07510158, 0.07023471, 0.356533, 0.07354169]\n",
      "2025-09-10 12:02:42,723 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:42,729 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:42,730 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"teeth\" + 0.006*\"business\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:02:42,731 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:02:42,732 : INFO : topic #3 (0.357): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:42,733 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:42,733 : INFO : topic diff=0.138573, rho=0.142254\n",
      "2025-09-10 12:02:44,098 : INFO : -7.952 per-word bound, 247.6 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:02:44,099 : INFO : PROGRESS: pass 42, at document #2000/14833\n",
      "2025-09-10 12:02:44,775 : INFO : optimized alpha [0.35916415, 0.074230276, 0.06964681, 0.32562786, 0.07424167]\n",
      "2025-09-10 12:02:44,780 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:44,786 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:44,787 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:44,787 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:44,788 : INFO : topic #3 (0.326): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:44,789 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:44,790 : INFO : topic diff=0.121285, rho=0.140836\n",
      "2025-09-10 12:02:46,076 : INFO : -7.860 per-word bound, 232.3 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:02:46,076 : INFO : PROGRESS: pass 42, at document #4000/14833\n",
      "2025-09-10 12:02:46,751 : INFO : optimized alpha [0.37845334, 0.07371132, 0.068731904, 0.32450825, 0.07461862]\n",
      "2025-09-10 12:02:46,756 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:46,762 : INFO : topic #0 (0.378): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:46,763 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:46,764 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:46,765 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:46,765 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:46,766 : INFO : topic diff=0.078216, rho=0.140836\n",
      "2025-09-10 12:02:47,961 : INFO : -7.765 per-word bound, 217.6 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:02:47,961 : INFO : PROGRESS: pass 42, at document #6000/14833\n",
      "2025-09-10 12:02:48,615 : INFO : optimized alpha [0.39348522, 0.07324901, 0.06722526, 0.33427787, 0.07282113]\n",
      "2025-09-10 12:02:48,620 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:48,626 : INFO : topic #0 (0.393): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:02:48,627 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:02:48,628 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:48,629 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:02:48,630 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:48,630 : INFO : topic diff=0.078130, rho=0.140836\n",
      "2025-09-10 12:02:49,761 : INFO : -8.000 per-word bound, 256.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:02:49,762 : INFO : PROGRESS: pass 42, at document #8000/14833\n",
      "2025-09-10 12:02:50,404 : INFO : optimized alpha [0.39360747, 0.073188975, 0.06719726, 0.33314854, 0.073121205]\n",
      "2025-09-10 12:02:50,409 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:50,415 : INFO : topic #0 (0.394): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:02:50,416 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:02:50,417 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:02:50,418 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:02:50,419 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:02:50,419 : INFO : topic diff=0.073666, rho=0.140836\n",
      "2025-09-10 12:02:51,552 : INFO : -8.060 per-word bound, 266.9 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:02:51,552 : INFO : PROGRESS: pass 42, at document #10000/14833\n",
      "2025-09-10 12:02:52,172 : INFO : optimized alpha [0.3863062, 0.07317438, 0.066604644, 0.33085516, 0.07354761]\n",
      "2025-09-10 12:02:52,177 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:52,183 : INFO : topic #0 (0.386): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:52,184 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:52,185 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"back\" + 0.004*\"war\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:52,186 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:52,186 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:02:52,187 : INFO : topic diff=0.076151, rho=0.140836\n",
      "2025-09-10 12:02:53,268 : INFO : -8.099 per-word bound, 274.1 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:02:53,269 : INFO : PROGRESS: pass 42, at document #12000/14833\n",
      "2025-09-10 12:02:53,859 : INFO : optimized alpha [0.38121134, 0.072885565, 0.06610699, 0.32926297, 0.071995616]\n",
      "2025-09-10 12:02:53,864 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:53,870 : INFO : topic #0 (0.381): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:02:53,871 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:02:53,871 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"math\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:02:53,872 : INFO : topic #3 (0.329): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:02:53,873 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:02:53,874 : INFO : topic diff=0.083033, rho=0.140836\n",
      "2025-09-10 12:02:55,069 : INFO : -8.500 per-word bound, 362.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:02:55,070 : INFO : PROGRESS: pass 42, at document #14000/14833\n",
      "2025-09-10 12:02:55,733 : INFO : optimized alpha [0.37049073, 0.07421576, 0.06873067, 0.34359038, 0.07380322]\n",
      "2025-09-10 12:02:55,738 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:55,744 : INFO : topic #0 (0.370): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:55,744 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:02:55,745 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:02:55,746 : INFO : topic #3 (0.344): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:55,747 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:02:55,748 : INFO : topic diff=0.157171, rho=0.140836\n",
      "2025-09-10 12:02:56,244 : INFO : -8.394 per-word bound, 336.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:02:56,245 : INFO : PROGRESS: pass 42, at document #14833/14833\n",
      "2025-09-10 12:02:56,506 : INFO : optimized alpha [0.3406192, 0.07520463, 0.070137516, 0.3558708, 0.07346812]\n",
      "2025-09-10 12:02:56,511 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:56,517 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:02:56,518 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:02:56,519 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:02:56,520 : INFO : topic #3 (0.356): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:02:56,521 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:56,521 : INFO : topic diff=0.137193, rho=0.140836\n",
      "2025-09-10 12:02:57,890 : INFO : -7.951 per-word bound, 247.4 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:02:57,891 : INFO : PROGRESS: pass 43, at document #2000/14833\n",
      "2025-09-10 12:02:58,563 : INFO : optimized alpha [0.3589454, 0.07434236, 0.06956068, 0.32538906, 0.0741638]\n",
      "2025-09-10 12:02:58,568 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:02:58,574 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:02:58,575 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:02:58,576 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:02:58,577 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:02:58,578 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:02:58,578 : INFO : topic diff=0.120013, rho=0.139460\n",
      "2025-09-10 12:02:59,849 : INFO : -7.860 per-word bound, 232.3 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:02:59,850 : INFO : PROGRESS: pass 43, at document #4000/14833\n",
      "2025-09-10 12:03:00,534 : INFO : optimized alpha [0.37803248, 0.073825546, 0.06865899, 0.3242751, 0.07454512]\n",
      "2025-09-10 12:03:00,539 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:00,545 : INFO : topic #0 (0.378): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:00,546 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:00,547 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:00,548 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:00,548 : INFO : topic #4 (0.075): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:00,549 : INFO : topic diff=0.077394, rho=0.139460\n",
      "2025-09-10 12:03:01,732 : INFO : -7.765 per-word bound, 217.5 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:03:01,733 : INFO : PROGRESS: pass 43, at document #6000/14833\n",
      "2025-09-10 12:03:02,386 : INFO : optimized alpha [0.3929051, 0.07336684, 0.067170784, 0.33390954, 0.072768874]\n",
      "2025-09-10 12:03:02,391 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:02,397 : INFO : topic #0 (0.393): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:03:02,398 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:03:02,399 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:02,400 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:03:02,401 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:02,401 : INFO : topic diff=0.077323, rho=0.139460\n",
      "2025-09-10 12:03:03,548 : INFO : -8.000 per-word bound, 256.0 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:03:03,550 : INFO : PROGRESS: pass 43, at document #8000/14833\n",
      "2025-09-10 12:03:04,194 : INFO : optimized alpha [0.39306065, 0.07329702, 0.06714439, 0.33279768, 0.07307073]\n",
      "2025-09-10 12:03:04,199 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:04,205 : INFO : topic #0 (0.393): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:03:04,207 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:04,208 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:03:04,209 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:03:04,209 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:03:04,210 : INFO : topic diff=0.072847, rho=0.139460\n",
      "2025-09-10 12:03:05,346 : INFO : -8.060 per-word bound, 266.9 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:03:05,347 : INFO : PROGRESS: pass 43, at document #10000/14833\n",
      "2025-09-10 12:03:05,975 : INFO : optimized alpha [0.38587695, 0.073286474, 0.06654943, 0.33052757, 0.07349247]\n",
      "2025-09-10 12:03:05,980 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:05,986 : INFO : topic #0 (0.386): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:05,987 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:03:05,988 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:05,989 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:05,990 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:03:05,990 : INFO : topic diff=0.075357, rho=0.139460\n",
      "2025-09-10 12:03:07,076 : INFO : -8.098 per-word bound, 274.1 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:03:07,077 : INFO : PROGRESS: pass 43, at document #12000/14833\n",
      "2025-09-10 12:03:07,668 : INFO : optimized alpha [0.38086334, 0.07300411, 0.066054, 0.3289533, 0.07195146]\n",
      "2025-09-10 12:03:07,673 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:07,679 : INFO : topic #0 (0.381): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:07,680 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:03:07,681 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:03:07,682 : INFO : topic #3 (0.329): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:03:07,683 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:03:07,683 : INFO : topic diff=0.082113, rho=0.139460\n",
      "2025-09-10 12:03:08,871 : INFO : -8.498 per-word bound, 361.7 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:03:08,871 : INFO : PROGRESS: pass 43, at document #14000/14833\n",
      "2025-09-10 12:03:09,535 : INFO : optimized alpha [0.37023428, 0.07431856, 0.068656504, 0.34312135, 0.07373572]\n",
      "2025-09-10 12:03:09,541 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:09,546 : INFO : topic #0 (0.370): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:09,547 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:03:09,548 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:09,549 : INFO : topic #3 (0.343): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:09,550 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:09,550 : INFO : topic diff=0.155255, rho=0.139460\n",
      "2025-09-10 12:03:10,048 : INFO : -8.394 per-word bound, 336.3 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:03:10,048 : INFO : PROGRESS: pass 43, at document #14833/14833\n",
      "2025-09-10 12:03:10,308 : INFO : optimized alpha [0.34063354, 0.07529635, 0.07004956, 0.3552659, 0.073414184]\n",
      "2025-09-10 12:03:10,313 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:10,319 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:10,320 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:03:10,321 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:03:10,322 : INFO : topic #3 (0.355): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:10,323 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:03:10,323 : INFO : topic diff=0.135816, rho=0.139460\n",
      "2025-09-10 12:03:11,683 : INFO : -7.950 per-word bound, 247.3 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:03:11,684 : INFO : PROGRESS: pass 44, at document #2000/14833\n",
      "2025-09-10 12:03:12,350 : INFO : optimized alpha [0.3587761, 0.07444435, 0.06948298, 0.32517943, 0.0740996]\n",
      "2025-09-10 12:03:12,355 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:12,361 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:12,362 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:03:12,363 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:12,364 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:12,365 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"black\"\n",
      "2025-09-10 12:03:12,365 : INFO : topic diff=0.118674, rho=0.138123\n",
      "2025-09-10 12:03:13,632 : INFO : -7.859 per-word bound, 232.2 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:03:13,633 : INFO : PROGRESS: pass 44, at document #4000/14833\n",
      "2025-09-10 12:03:14,316 : INFO : optimized alpha [0.37765005, 0.0739423, 0.06859286, 0.32401732, 0.07448139]\n",
      "2025-09-10 12:03:14,321 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:14,327 : INFO : topic #0 (0.378): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:14,328 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:14,329 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:14,330 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:14,331 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:14,331 : INFO : topic diff=0.076583, rho=0.138123\n",
      "2025-09-10 12:03:15,522 : INFO : -7.765 per-word bound, 217.5 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:03:15,523 : INFO : PROGRESS: pass 44, at document #6000/14833\n",
      "2025-09-10 12:03:16,178 : INFO : optimized alpha [0.39237332, 0.07348196, 0.06712239, 0.333548, 0.07272428]\n",
      "2025-09-10 12:03:16,183 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:16,189 : INFO : topic #0 (0.392): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:03:16,190 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:03:16,191 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:16,192 : INFO : topic #3 (0.334): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:03:16,193 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:16,193 : INFO : topic diff=0.076530, rho=0.138123\n",
      "2025-09-10 12:03:17,326 : INFO : -7.999 per-word bound, 255.9 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:03:17,327 : INFO : PROGRESS: pass 44, at document #8000/14833\n",
      "2025-09-10 12:03:17,966 : INFO : optimized alpha [0.39254892, 0.07341645, 0.0670973, 0.33245355, 0.07302839]\n",
      "2025-09-10 12:03:17,971 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:17,977 : INFO : topic #0 (0.393): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:03:17,978 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:17,979 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:03:17,980 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:03:17,980 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:03:17,981 : INFO : topic diff=0.072038, rho=0.138123\n",
      "2025-09-10 12:03:19,099 : INFO : -8.060 per-word bound, 266.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:03:19,100 : INFO : PROGRESS: pass 44, at document #10000/14833\n",
      "2025-09-10 12:03:19,721 : INFO : optimized alpha [0.38546172, 0.073406495, 0.066509545, 0.3301922, 0.07344995]\n",
      "2025-09-10 12:03:19,726 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:19,732 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:19,733 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:03:19,734 : INFO : topic #2 (0.067): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:19,735 : INFO : topic #3 (0.330): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:19,736 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:03:19,736 : INFO : topic diff=0.074542, rho=0.138123\n",
      "2025-09-10 12:03:20,812 : INFO : -8.098 per-word bound, 274.0 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:03:20,813 : INFO : PROGRESS: pass 44, at document #12000/14833\n",
      "2025-09-10 12:03:21,401 : INFO : optimized alpha [0.38052288, 0.073121384, 0.066014975, 0.32863995, 0.071927994]\n",
      "2025-09-10 12:03:21,406 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:21,412 : INFO : topic #0 (0.381): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:21,413 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:03:21,414 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:03:21,415 : INFO : topic #3 (0.329): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:03:21,416 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:03:21,417 : INFO : topic diff=0.081237, rho=0.138123\n",
      "2025-09-10 12:03:22,604 : INFO : -8.497 per-word bound, 361.4 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:03:22,605 : INFO : PROGRESS: pass 44, at document #14000/14833\n",
      "2025-09-10 12:03:23,266 : INFO : optimized alpha [0.36995307, 0.07442537, 0.06858655, 0.3426566, 0.07368839]\n",
      "2025-09-10 12:03:23,271 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:23,277 : INFO : topic #0 (0.370): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:23,278 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:03:23,279 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:23,280 : INFO : topic #3 (0.343): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:23,281 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:23,281 : INFO : topic diff=0.153511, rho=0.138123\n",
      "2025-09-10 12:03:23,780 : INFO : -8.393 per-word bound, 336.2 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:03:23,780 : INFO : PROGRESS: pass 44, at document #14833/14833\n",
      "2025-09-10 12:03:24,040 : INFO : optimized alpha [0.34067774, 0.07539194, 0.069966465, 0.3546779, 0.07334886]\n",
      "2025-09-10 12:03:24,045 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:24,051 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:24,052 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:03:24,053 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:03:24,054 : INFO : topic #3 (0.355): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"would\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:24,055 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:03:24,055 : INFO : topic diff=0.134436, rho=0.138123\n",
      "2025-09-10 12:03:25,421 : INFO : -7.950 per-word bound, 247.2 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:03:25,421 : INFO : PROGRESS: pass 45, at document #2000/14833\n",
      "2025-09-10 12:03:26,088 : INFO : optimized alpha [0.35863936, 0.074552074, 0.06940951, 0.3249603, 0.074030094]\n",
      "2025-09-10 12:03:26,093 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:26,099 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:26,100 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.006*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:03:26,101 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:26,102 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:26,102 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:26,103 : INFO : topic diff=0.117478, rho=0.136824\n",
      "2025-09-10 12:03:27,363 : INFO : -7.859 per-word bound, 232.1 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:03:27,364 : INFO : PROGRESS: pass 45, at document #4000/14833\n",
      "2025-09-10 12:03:28,042 : INFO : optimized alpha [0.3773247, 0.07405178, 0.06852687, 0.32381773, 0.07441273]\n",
      "2025-09-10 12:03:28,047 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:28,053 : INFO : topic #0 (0.377): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:28,054 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:28,055 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:28,055 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:28,056 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:28,057 : INFO : topic diff=0.075878, rho=0.136824\n",
      "2025-09-10 12:03:29,235 : INFO : -7.764 per-word bound, 217.4 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:03:29,235 : INFO : PROGRESS: pass 45, at document #6000/14833\n",
      "2025-09-10 12:03:29,886 : INFO : optimized alpha [0.39190322, 0.07359017, 0.06707379, 0.3332479, 0.072672136]\n",
      "2025-09-10 12:03:29,891 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:29,897 : INFO : topic #0 (0.392): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:03:29,898 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:03:29,899 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:29,900 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:03:29,900 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:29,901 : INFO : topic diff=0.075778, rho=0.136824\n",
      "2025-09-10 12:03:31,037 : INFO : -7.999 per-word bound, 255.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:03:31,038 : INFO : PROGRESS: pass 45, at document #8000/14833\n",
      "2025-09-10 12:03:31,674 : INFO : optimized alpha [0.39208803, 0.07352662, 0.06704559, 0.33215895, 0.072973914]\n",
      "2025-09-10 12:03:31,679 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:31,684 : INFO : topic #0 (0.392): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:03:31,685 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:31,687 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:03:31,687 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:03:31,688 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:03:31,689 : INFO : topic diff=0.071231, rho=0.136824\n",
      "2025-09-10 12:03:32,802 : INFO : -8.059 per-word bound, 266.8 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:03:32,802 : INFO : PROGRESS: pass 45, at document #10000/14833\n",
      "2025-09-10 12:03:33,425 : INFO : optimized alpha [0.38510174, 0.073517606, 0.066469654, 0.32991683, 0.0733888]\n",
      "2025-09-10 12:03:33,430 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:33,436 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:33,437 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:03:33,437 : INFO : topic #2 (0.066): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:33,438 : INFO : topic #3 (0.330): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:33,439 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:03:33,440 : INFO : topic diff=0.073796, rho=0.136824\n",
      "2025-09-10 12:03:34,514 : INFO : -8.098 per-word bound, 273.9 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:03:34,514 : INFO : PROGRESS: pass 45, at document #12000/14833\n",
      "2025-09-10 12:03:35,109 : INFO : optimized alpha [0.38022858, 0.07323481, 0.0659718, 0.32835963, 0.07188385]\n",
      "2025-09-10 12:03:35,114 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:35,120 : INFO : topic #0 (0.380): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:35,121 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:03:35,122 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:03:35,123 : INFO : topic #3 (0.328): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:03:35,124 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:03:35,124 : INFO : topic diff=0.080383, rho=0.136824\n",
      "2025-09-10 12:03:36,313 : INFO : -8.496 per-word bound, 361.1 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:03:36,314 : INFO : PROGRESS: pass 45, at document #14000/14833\n",
      "2025-09-10 12:03:36,972 : INFO : optimized alpha [0.36975113, 0.074522085, 0.06852346, 0.34222782, 0.07361932]\n",
      "2025-09-10 12:03:36,977 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:36,983 : INFO : topic #0 (0.370): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:36,984 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:03:36,985 : INFO : topic #2 (0.069): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:36,986 : INFO : topic #3 (0.342): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:36,986 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:36,987 : INFO : topic diff=0.151721, rho=0.136824\n",
      "2025-09-10 12:03:37,483 : INFO : -8.392 per-word bound, 336.0 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:03:37,484 : INFO : PROGRESS: pass 45, at document #14833/14833\n",
      "2025-09-10 12:03:37,747 : INFO : optimized alpha [0.3407178, 0.075475246, 0.06988997, 0.35409433, 0.07328879]\n",
      "2025-09-10 12:03:37,752 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:37,758 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:37,759 : INFO : topic #1 (0.075): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:03:37,760 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:03:37,761 : INFO : topic #3 (0.354): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:37,762 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:03:37,762 : INFO : topic diff=0.133163, rho=0.136824\n",
      "2025-09-10 12:03:39,137 : INFO : -7.949 per-word bound, 247.1 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:03:39,138 : INFO : PROGRESS: pass 46, at document #2000/14833\n",
      "2025-09-10 12:03:39,813 : INFO : optimized alpha [0.35850292, 0.07464664, 0.06934676, 0.32472187, 0.073965676]\n",
      "2025-09-10 12:03:39,818 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:39,824 : INFO : topic #0 (0.359): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:39,825 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:03:39,825 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:39,826 : INFO : topic #3 (0.325): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:39,827 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:39,828 : INFO : topic diff=0.116199, rho=0.135561\n",
      "2025-09-10 12:03:41,105 : INFO : -7.858 per-word bound, 232.0 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:03:41,106 : INFO : PROGRESS: pass 46, at document #4000/14833\n",
      "2025-09-10 12:03:41,776 : INFO : optimized alpha [0.37696153, 0.07414922, 0.068479896, 0.32357582, 0.07434999]\n",
      "2025-09-10 12:03:41,781 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:41,787 : INFO : topic #0 (0.377): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:41,788 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:41,789 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:41,790 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:41,790 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:41,791 : INFO : topic diff=0.075120, rho=0.135561\n",
      "2025-09-10 12:03:42,970 : INFO : -7.764 per-word bound, 217.4 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:03:42,971 : INFO : PROGRESS: pass 46, at document #6000/14833\n",
      "2025-09-10 12:03:43,621 : INFO : optimized alpha [0.39140013, 0.073686995, 0.067042515, 0.33290753, 0.072629]\n",
      "2025-09-10 12:03:43,626 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:43,632 : INFO : topic #0 (0.391): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:03:43,633 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:03:43,634 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:43,635 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:03:43,635 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:43,636 : INFO : topic diff=0.075036, rho=0.135561\n",
      "2025-09-10 12:03:44,763 : INFO : -7.999 per-word bound, 255.8 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:03:44,764 : INFO : PROGRESS: pass 46, at document #8000/14833\n",
      "2025-09-10 12:03:45,395 : INFO : optimized alpha [0.39160377, 0.07362128, 0.06701505, 0.3318335, 0.07292554]\n",
      "2025-09-10 12:03:45,400 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:45,406 : INFO : topic #0 (0.392): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:03:45,407 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:45,408 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:03:45,408 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:03:45,409 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:03:45,410 : INFO : topic diff=0.070451, rho=0.135561\n",
      "2025-09-10 12:03:46,521 : INFO : -8.059 per-word bound, 266.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:03:46,522 : INFO : PROGRESS: pass 46, at document #10000/14833\n",
      "2025-09-10 12:03:47,141 : INFO : optimized alpha [0.38470152, 0.07360828, 0.066435814, 0.3296079, 0.07333633]\n",
      "2025-09-10 12:03:47,146 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:47,151 : INFO : topic #0 (0.385): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:47,152 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:03:47,153 : INFO : topic #2 (0.066): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:47,154 : INFO : topic #3 (0.330): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:47,155 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:03:47,155 : INFO : topic diff=0.073078, rho=0.135561\n",
      "2025-09-10 12:03:48,231 : INFO : -8.097 per-word bound, 273.9 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:03:48,232 : INFO : PROGRESS: pass 46, at document #12000/14833\n",
      "2025-09-10 12:03:48,819 : INFO : optimized alpha [0.37986314, 0.07332797, 0.065952346, 0.3280553, 0.07184686]\n",
      "2025-09-10 12:03:48,824 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:48,830 : INFO : topic #0 (0.380): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:48,831 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:03:48,831 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:03:48,832 : INFO : topic #3 (0.328): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:03:48,833 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:03:48,833 : INFO : topic diff=0.079548, rho=0.135561\n",
      "2025-09-10 12:03:50,026 : INFO : -8.495 per-word bound, 360.8 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:03:50,026 : INFO : PROGRESS: pass 46, at document #14000/14833\n",
      "2025-09-10 12:03:50,691 : INFO : optimized alpha [0.36946806, 0.074581206, 0.06847755, 0.34178156, 0.07356859]\n",
      "2025-09-10 12:03:50,696 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:50,702 : INFO : topic #0 (0.369): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:50,703 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:03:50,704 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:50,705 : INFO : topic #3 (0.342): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:03:50,705 : INFO : topic #4 (0.074): 0.011*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:50,706 : INFO : topic diff=0.150096, rho=0.135561\n",
      "2025-09-10 12:03:51,202 : INFO : -8.392 per-word bound, 335.8 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:03:51,202 : INFO : PROGRESS: pass 46, at document #14833/14833\n",
      "2025-09-10 12:03:51,461 : INFO : optimized alpha [0.3406753, 0.07550979, 0.069830246, 0.35347477, 0.073236905]\n",
      "2025-09-10 12:03:51,466 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:51,472 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:03:51,473 : INFO : topic #1 (0.076): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:03:51,474 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:03:51,475 : INFO : topic #3 (0.353): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.005*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:51,476 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:03:51,476 : INFO : topic diff=0.131906, rho=0.135561\n",
      "2025-09-10 12:03:52,840 : INFO : -7.948 per-word bound, 247.0 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:03:52,840 : INFO : PROGRESS: pass 47, at document #2000/14833\n",
      "2025-09-10 12:03:53,511 : INFO : optimized alpha [0.35829726, 0.074692614, 0.06928605, 0.3244963, 0.073909394]\n",
      "2025-09-10 12:03:53,516 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:53,522 : INFO : topic #0 (0.358): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:53,523 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:03:53,524 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:53,525 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:53,526 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 12:03:53,526 : INFO : topic diff=0.115088, rho=0.134332\n",
      "2025-09-10 12:03:54,792 : INFO : -7.858 per-word bound, 232.0 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:03:54,792 : INFO : PROGRESS: pass 47, at document #4000/14833\n",
      "2025-09-10 12:03:55,464 : INFO : optimized alpha [0.3765971, 0.07420666, 0.06842489, 0.32333025, 0.07429157]\n",
      "2025-09-10 12:03:55,469 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:55,475 : INFO : topic #0 (0.377): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:03:55,476 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:55,477 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:03:55,478 : INFO : topic #3 (0.323): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:03:55,479 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:55,479 : INFO : topic diff=0.074363, rho=0.134332\n",
      "2025-09-10 12:03:56,661 : INFO : -7.764 per-word bound, 217.3 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:03:56,661 : INFO : PROGRESS: pass 47, at document #6000/14833\n",
      "2025-09-10 12:03:57,306 : INFO : optimized alpha [0.39089963, 0.07375068, 0.067003354, 0.33257082, 0.07258923]\n",
      "2025-09-10 12:03:57,311 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:57,316 : INFO : topic #0 (0.391): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:03:57,318 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:03:57,318 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:03:57,319 : INFO : topic #3 (0.333): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:03:57,320 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:03:57,320 : INFO : topic diff=0.074314, rho=0.134332\n",
      "2025-09-10 12:03:58,459 : INFO : -7.998 per-word bound, 255.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:03:58,459 : INFO : PROGRESS: pass 47, at document #8000/14833\n",
      "2025-09-10 12:03:59,095 : INFO : optimized alpha [0.39110216, 0.07368254, 0.066976726, 0.33150733, 0.07288884]\n",
      "2025-09-10 12:03:59,100 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:03:59,106 : INFO : topic #0 (0.391): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:03:59,107 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:03:59,108 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:03:59,109 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:03:59,110 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:03:59,110 : INFO : topic diff=0.069729, rho=0.134332\n",
      "2025-09-10 12:04:00,232 : INFO : -8.059 per-word bound, 266.7 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:04:00,232 : INFO : PROGRESS: pass 47, at document #10000/14833\n",
      "2025-09-10 12:04:00,862 : INFO : optimized alpha [0.38429475, 0.07367061, 0.0664036, 0.32930613, 0.07329759]\n",
      "2025-09-10 12:04:00,867 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:00,873 : INFO : topic #0 (0.384): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:00,874 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:04:00,874 : INFO : topic #2 (0.066): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:00,875 : INFO : topic #3 (0.329): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"years\" + 0.005*\"said\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:00,876 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"maybe\" + 0.007*\"one\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:04:00,877 : INFO : topic diff=0.072374, rho=0.134332\n",
      "2025-09-10 12:04:01,961 : INFO : -8.097 per-word bound, 273.8 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:04:01,962 : INFO : PROGRESS: pass 47, at document #12000/14833\n",
      "2025-09-10 12:04:02,557 : INFO : optimized alpha [0.37952897, 0.073389806, 0.065925434, 0.32777458, 0.071828924]\n",
      "2025-09-10 12:04:02,562 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:02,568 : INFO : topic #0 (0.380): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:02,569 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:04:02,570 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:04:02,571 : INFO : topic #3 (0.328): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"still\"\n",
      "2025-09-10 12:04:02,572 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:04:02,572 : INFO : topic diff=0.078735, rho=0.134332\n",
      "2025-09-10 12:04:03,774 : INFO : -8.494 per-word bound, 360.6 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:04:03,774 : INFO : PROGRESS: pass 47, at document #14000/14833\n",
      "2025-09-10 12:04:04,444 : INFO : optimized alpha [0.36922312, 0.074629396, 0.06843263, 0.34136245, 0.07352976]\n",
      "2025-09-10 12:04:04,449 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:04,455 : INFO : topic #0 (0.369): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:04,456 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:04:04,457 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:04:04,457 : INFO : topic #3 (0.341): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:04,458 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:04:04,459 : INFO : topic diff=0.148396, rho=0.134332\n",
      "2025-09-10 12:04:04,957 : INFO : -8.391 per-word bound, 335.7 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:04:04,958 : INFO : PROGRESS: pass 47, at document #14833/14833\n",
      "2025-09-10 12:04:05,223 : INFO : optimized alpha [0.34067369, 0.07556046, 0.06977279, 0.35293508, 0.073212676]\n",
      "2025-09-10 12:04:05,228 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:05,234 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:05,235 : INFO : topic #1 (0.076): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:04:05,236 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:04:05,237 : INFO : topic #3 (0.353): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:05,238 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:04:05,238 : INFO : topic diff=0.130593, rho=0.134332\n",
      "2025-09-10 12:04:06,618 : INFO : -7.948 per-word bound, 246.9 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:04:06,619 : INFO : PROGRESS: pass 48, at document #2000/14833\n",
      "2025-09-10 12:04:07,295 : INFO : optimized alpha [0.358126, 0.07474828, 0.06923611, 0.32429314, 0.07387494]\n",
      "2025-09-10 12:04:07,300 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:07,306 : INFO : topic #0 (0.358): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:04:07,307 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:04:07,308 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:07,309 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:07,310 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 12:04:07,310 : INFO : topic diff=0.113887, rho=0.133136\n",
      "2025-09-10 12:04:08,591 : INFO : -7.858 per-word bound, 231.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:04:08,592 : INFO : PROGRESS: pass 48, at document #4000/14833\n",
      "2025-09-10 12:04:09,274 : INFO : optimized alpha [0.3762478, 0.07426914, 0.06838738, 0.32312268, 0.074254364]\n",
      "2025-09-10 12:04:09,279 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:09,285 : INFO : topic #0 (0.376): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:04:09,286 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:04:09,287 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:09,288 : INFO : topic #3 (0.323): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:09,288 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:09,289 : INFO : topic diff=0.073666, rho=0.133136\n",
      "2025-09-10 12:04:10,480 : INFO : -7.763 per-word bound, 217.3 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:04:10,481 : INFO : PROGRESS: pass 48, at document #6000/14833\n",
      "2025-09-10 12:04:11,138 : INFO : optimized alpha [0.39042178, 0.07382174, 0.06698032, 0.3322717, 0.07256891]\n",
      "2025-09-10 12:04:11,143 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:11,149 : INFO : topic #0 (0.390): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:04:11,150 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:04:11,151 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:04:11,152 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:04:11,152 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:11,153 : INFO : topic diff=0.073611, rho=0.133136\n",
      "2025-09-10 12:04:12,307 : INFO : -7.998 per-word bound, 255.7 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:04:12,308 : INFO : PROGRESS: pass 48, at document #8000/14833\n",
      "2025-09-10 12:04:12,959 : INFO : optimized alpha [0.3906218, 0.073750734, 0.06695392, 0.33121556, 0.07286588]\n",
      "2025-09-10 12:04:12,964 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:12,970 : INFO : topic #0 (0.391): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:04:12,971 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:04:12,972 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:04:12,973 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:04:12,973 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:04:12,974 : INFO : topic diff=0.068963, rho=0.133136\n",
      "2025-09-10 12:04:14,108 : INFO : -8.059 per-word bound, 266.6 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:04:14,109 : INFO : PROGRESS: pass 48, at document #10000/14833\n",
      "2025-09-10 12:04:14,743 : INFO : optimized alpha [0.3839061, 0.07374427, 0.06637713, 0.32903582, 0.0732813]\n",
      "2025-09-10 12:04:14,748 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:14,754 : INFO : topic #0 (0.384): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:14,755 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:04:14,756 : INFO : topic #2 (0.066): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:14,756 : INFO : topic #3 (0.329): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:14,757 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:04:14,758 : INFO : topic diff=0.071638, rho=0.133136\n",
      "2025-09-10 12:04:15,848 : INFO : -8.097 per-word bound, 273.8 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:04:15,849 : INFO : PROGRESS: pass 48, at document #12000/14833\n",
      "2025-09-10 12:04:16,435 : INFO : optimized alpha [0.37923142, 0.07345805, 0.065899536, 0.32752824, 0.07182916]\n",
      "2025-09-10 12:04:16,440 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:16,446 : INFO : topic #0 (0.379): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:16,447 : INFO : topic #1 (0.073): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:04:16,448 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:04:16,448 : INFO : topic #3 (0.328): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.004*\"still\"\n",
      "2025-09-10 12:04:16,449 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:04:16,450 : INFO : topic diff=0.077953, rho=0.133136\n",
      "2025-09-10 12:04:17,649 : INFO : -8.493 per-word bound, 360.3 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:04:17,649 : INFO : PROGRESS: pass 48, at document #14000/14833\n",
      "2025-09-10 12:04:18,317 : INFO : optimized alpha [0.36900413, 0.07468387, 0.068388104, 0.34098375, 0.07350357]\n",
      "2025-09-10 12:04:18,322 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:18,328 : INFO : topic #0 (0.369): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:18,329 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:04:18,330 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:04:18,331 : INFO : topic #3 (0.341): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:18,331 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:04:18,332 : INFO : topic diff=0.146853, rho=0.133136\n",
      "2025-09-10 12:04:18,836 : INFO : -8.390 per-word bound, 335.5 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:04:18,837 : INFO : PROGRESS: pass 48, at document #14833/14833\n",
      "2025-09-10 12:04:19,100 : INFO : optimized alpha [0.34068027, 0.07560455, 0.069715865, 0.35243586, 0.07318893]\n",
      "2025-09-10 12:04:19,105 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:19,111 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:19,112 : INFO : topic #1 (0.076): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"people\" + 0.004*\"like\"\n",
      "2025-09-10 12:04:19,112 : INFO : topic #2 (0.070): 0.020*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:04:19,113 : INFO : topic #3 (0.352): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:19,114 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:04:19,115 : INFO : topic diff=0.129434, rho=0.133136\n",
      "2025-09-10 12:04:20,497 : INFO : -7.947 per-word bound, 246.8 perplexity estimate based on a held-out corpus of 2000 documents with 288390 words\n",
      "2025-09-10 12:04:20,497 : INFO : PROGRESS: pass 49, at document #2000/14833\n",
      "2025-09-10 12:04:21,170 : INFO : optimized alpha [0.3579979, 0.07479459, 0.06918679, 0.32411206, 0.073846295]\n",
      "2025-09-10 12:04:21,175 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:21,181 : INFO : topic #0 (0.358): 0.014*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:04:21,182 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"company\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"sleep\" + 0.004*\"like\" + 0.004*\"also\"\n",
      "2025-09-10 12:04:21,183 : INFO : topic #2 (0.069): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:21,184 : INFO : topic #3 (0.324): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"said\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:21,185 : INFO : topic #4 (0.074): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"god\" + 0.006*\"way\"\n",
      "2025-09-10 12:04:21,185 : INFO : topic diff=0.112840, rho=0.131972\n",
      "2025-09-10 12:04:22,468 : INFO : -7.857 per-word bound, 231.9 perplexity estimate based on a held-out corpus of 2000 documents with 238012 words\n",
      "2025-09-10 12:04:22,469 : INFO : PROGRESS: pass 49, at document #4000/14833\n",
      "2025-09-10 12:04:23,150 : INFO : optimized alpha [0.3759383, 0.07431893, 0.06834838, 0.32295588, 0.07422149]\n",
      "2025-09-10 12:04:23,156 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:23,161 : INFO : topic #0 (0.376): 0.014*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.008*\"one\" + 0.008*\"someone\" + 0.007*\"something\" + 0.007*\"might\" + 0.007*\"even\" + 0.007*\"life\" + 0.006*\"things\"\n",
      "2025-09-10 12:04:23,162 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"business\" + 0.006*\"language\" + 0.006*\"jobs\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"teeth\" + 0.005*\"apple\" + 0.005*\"financial\" + 0.005*\"like\"\n",
      "2025-09-10 12:04:23,163 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.004*\"back\" + 0.003*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:23,164 : INFO : topic #3 (0.323): 0.011*\"one\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:23,165 : INFO : topic #4 (0.074): 0.013*\"like\" + 0.011*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.006*\"even\" + 0.006*\"death\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:23,165 : INFO : topic diff=0.072974, rho=0.131972\n",
      "2025-09-10 12:04:24,368 : INFO : -7.763 per-word bound, 217.2 perplexity estimate based on a held-out corpus of 2000 documents with 210539 words\n",
      "2025-09-10 12:04:24,369 : INFO : PROGRESS: pass 49, at document #6000/14833\n",
      "2025-09-10 12:04:25,034 : INFO : optimized alpha [0.38997737, 0.07387382, 0.0669554, 0.3320123, 0.07255198]\n",
      "2025-09-10 12:04:25,040 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:25,046 : INFO : topic #0 (0.390): 0.014*\"like\" + 0.014*\"people\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.007*\"really\"\n",
      "2025-09-10 12:04:25,047 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"sleep\" + 0.006*\"company\" + 0.006*\"apple\" + 0.006*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"also\"\n",
      "2025-09-10 12:04:25,048 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"internet\" + 0.003*\"human\"\n",
      "2025-09-10 12:04:25,049 : INFO : topic #3 (0.332): 0.011*\"like\" + 0.011*\"one\" + 0.008*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"even\" + 0.005*\"got\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"said\"\n",
      "2025-09-10 12:04:25,049 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:25,050 : INFO : topic diff=0.072926, rho=0.131972\n",
      "2025-09-10 12:04:26,200 : INFO : -7.998 per-word bound, 255.6 perplexity estimate based on a held-out corpus of 2000 documents with 184846 words\n",
      "2025-09-10 12:04:26,201 : INFO : PROGRESS: pass 49, at document #8000/14833\n",
      "2025-09-10 12:04:26,852 : INFO : optimized alpha [0.3901898, 0.07379601, 0.06692927, 0.33096135, 0.07284607]\n",
      "2025-09-10 12:04:26,857 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:26,862 : INFO : topic #0 (0.390): 0.014*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"really\"\n",
      "2025-09-10 12:04:26,863 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"apple\" + 0.006*\"sleep\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"teeth\" + 0.005*\"also\" + 0.005*\"like\"\n",
      "2025-09-10 12:04:26,864 : INFO : topic #2 (0.067): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"students\" + 0.005*\"could\" + 0.004*\"even\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"math\" + 0.003*\"human\" + 0.003*\"internet\"\n",
      "2025-09-10 12:04:26,865 : INFO : topic #3 (0.331): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"said\" + 0.005*\"years\"\n",
      "2025-09-10 12:04:26,866 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.010*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.006*\"death\" + 0.006*\"even\" + 0.006*\"maybe\" + 0.006*\"way\" + 0.005*\"life\"\n",
      "2025-09-10 12:04:26,866 : INFO : topic diff=0.068293, rho=0.131972\n",
      "2025-09-10 12:04:27,992 : INFO : -8.058 per-word bound, 266.6 perplexity estimate based on a held-out corpus of 2000 documents with 184247 words\n",
      "2025-09-10 12:04:27,992 : INFO : PROGRESS: pass 49, at document #10000/14833\n",
      "2025-09-10 12:04:28,619 : INFO : optimized alpha [0.3835505, 0.07378983, 0.066357486, 0.32877058, 0.073259875]\n",
      "2025-09-10 12:04:28,624 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:28,630 : INFO : topic #0 (0.384): 0.015*\"people\" + 0.014*\"like\" + 0.012*\"think\" + 0.008*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.007*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:28,631 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.006*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.006*\"language\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.004*\"financial\"\n",
      "2025-09-10 12:04:28,632 : INFO : topic #2 (0.066): 0.023*\"would\" + 0.007*\"one\" + 0.005*\"could\" + 0.005*\"even\" + 0.004*\"students\" + 0.004*\"war\" + 0.004*\"back\" + 0.004*\"internet\" + 0.003*\"human\" + 0.003*\"math\"\n",
      "2025-09-10 12:04:28,633 : INFO : topic #3 (0.329): 0.011*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"back\" + 0.006*\"day\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:28,634 : INFO : topic #4 (0.073): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.007*\"even\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"question\"\n",
      "2025-09-10 12:04:28,634 : INFO : topic diff=0.070967, rho=0.131972\n",
      "2025-09-10 12:04:29,722 : INFO : -8.097 per-word bound, 273.7 perplexity estimate based on a held-out corpus of 2000 documents with 183021 words\n",
      "2025-09-10 12:04:29,722 : INFO : PROGRESS: pass 49, at document #12000/14833\n",
      "2025-09-10 12:04:30,314 : INFO : optimized alpha [0.37893343, 0.07350842, 0.06587994, 0.32725972, 0.071821235]\n",
      "2025-09-10 12:04:30,319 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:30,325 : INFO : topic #0 (0.379): 0.015*\"people\" + 0.013*\"like\" + 0.011*\"think\" + 0.009*\"someone\" + 0.008*\"something\" + 0.008*\"one\" + 0.008*\"even\" + 0.007*\"might\" + 0.007*\"things\" + 0.006*\"life\"\n",
      "2025-09-10 12:04:30,326 : INFO : topic #1 (0.074): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"jobs\" + 0.006*\"business\" + 0.005*\"apple\" + 0.005*\"company\" + 0.005*\"language\" + 0.005*\"teeth\" + 0.005*\"like\" + 0.005*\"even\"\n",
      "2025-09-10 12:04:30,327 : INFO : topic #2 (0.066): 0.021*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.005*\"students\" + 0.004*\"could\" + 0.004*\"war\" + 0.004*\"math\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"like\"\n",
      "2025-09-10 12:04:30,328 : INFO : topic #3 (0.327): 0.012*\"like\" + 0.011*\"one\" + 0.007*\"time\" + 0.006*\"even\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"day\" + 0.005*\"said\" + 0.005*\"years\" + 0.004*\"still\"\n",
      "2025-09-10 12:04:30,329 : INFO : topic #4 (0.072): 0.013*\"like\" + 0.010*\"time\" + 0.009*\"think\" + 0.009*\"something\" + 0.007*\"even\" + 0.007*\"one\" + 0.007*\"maybe\" + 0.006*\"way\" + 0.006*\"death\" + 0.005*\"idea\"\n",
      "2025-09-10 12:04:30,329 : INFO : topic diff=0.077195, rho=0.131972\n",
      "2025-09-10 12:04:31,534 : INFO : -8.492 per-word bound, 360.0 perplexity estimate based on a held-out corpus of 2000 documents with 245086 words\n",
      "2025-09-10 12:04:31,534 : INFO : PROGRESS: pass 49, at document #14000/14833\n",
      "2025-09-10 12:04:32,204 : INFO : optimized alpha [0.36878952, 0.07472173, 0.06834307, 0.34058818, 0.07348538]\n",
      "2025-09-10 12:04:32,209 : INFO : merging changes from 2000 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:32,215 : INFO : topic #0 (0.369): 0.015*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.008*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:32,216 : INFO : topic #1 (0.075): 0.009*\"money\" + 0.007*\"sleep\" + 0.006*\"language\" + 0.006*\"business\" + 0.006*\"jobs\" + 0.005*\"company\" + 0.005*\"apple\" + 0.005*\"like\" + 0.004*\"teeth\" + 0.004*\"even\"\n",
      "2025-09-10 12:04:32,217 : INFO : topic #2 (0.068): 0.022*\"would\" + 0.007*\"one\" + 0.005*\"even\" + 0.004*\"could\" + 0.004*\"students\" + 0.004*\"war\" + 0.003*\"internet\" + 0.003*\"math\" + 0.003*\"back\" + 0.003*\"human\"\n",
      "2025-09-10 12:04:32,218 : INFO : topic #3 (0.341): 0.011*\"one\" + 0.011*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"said\" + 0.005*\"years\" + 0.005*\"get\"\n",
      "2025-09-10 12:04:32,219 : INFO : topic #4 (0.073): 0.012*\"like\" + 0.011*\"time\" + 0.008*\"think\" + 0.008*\"something\" + 0.007*\"one\" + 0.007*\"death\" + 0.006*\"even\" + 0.006*\"god\" + 0.006*\"black\" + 0.006*\"way\"\n",
      "2025-09-10 12:04:32,219 : INFO : topic diff=0.145260, rho=0.131972\n",
      "2025-09-10 12:04:32,721 : INFO : -8.390 per-word bound, 335.4 perplexity estimate based on a held-out corpus of 833 documents with 106140 words\n",
      "2025-09-10 12:04:32,721 : INFO : PROGRESS: pass 49, at document #14833/14833\n",
      "2025-09-10 12:04:32,984 : INFO : optimized alpha [0.34068316, 0.075632535, 0.069658875, 0.35193533, 0.073173106]\n",
      "2025-09-10 12:04:32,989 : INFO : merging changes from 833 documents into a model of 14833 documents\n",
      "2025-09-10 12:04:32,995 : INFO : topic #0 (0.341): 0.016*\"people\" + 0.013*\"like\" + 0.010*\"think\" + 0.008*\"someone\" + 0.008*\"one\" + 0.007*\"something\" + 0.007*\"even\" + 0.007*\"things\" + 0.006*\"life\" + 0.006*\"might\"\n",
      "2025-09-10 12:04:32,996 : INFO : topic #1 (0.076): 0.010*\"money\" + 0.007*\"jobs\" + 0.006*\"business\" + 0.006*\"teeth\" + 0.006*\"company\" + 0.006*\"sleep\" + 0.005*\"apple\" + 0.005*\"language\" + 0.004*\"like\" + 0.004*\"people\"\n",
      "2025-09-10 12:04:32,997 : INFO : topic #2 (0.070): 0.021*\"would\" + 0.007*\"one\" + 0.004*\"students\" + 0.004*\"even\" + 0.004*\"could\" + 0.004*\"war\" + 0.003*\"back\" + 0.003*\"human\" + 0.003*\"internet\" + 0.003*\"two\"\n",
      "2025-09-10 12:04:32,997 : INFO : topic #3 (0.352): 0.011*\"one\" + 0.009*\"like\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"back\" + 0.005*\"said\" + 0.005*\"got\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\"\n",
      "2025-09-10 12:04:32,998 : INFO : topic #4 (0.073): 0.011*\"like\" + 0.011*\"time\" + 0.007*\"think\" + 0.007*\"something\" + 0.007*\"death\" + 0.007*\"one\" + 0.006*\"god\" + 0.006*\"even\" + 0.006*\"life\" + 0.006*\"black\"\n",
      "2025-09-10 12:04:32,999 : INFO : topic diff=0.128306, rho=0.131972\n",
      "2025-09-10 12:04:32,999 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=34546, num_topics=5, decay=0.5, chunksize=2000> in 715.48s', 'datetime': '2025-09-10T12:04:32.999683', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from kneed import KneeLocator\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Warning 이상의 문제 발생 시 출력\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## nltk set download\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except:\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def extract_english_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text) ## 구두점 전처리\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def select_optimal_topic(coherence_scores, topic_range):\n",
    "    knee = KneeLocator(topic_range, coherence_scores, curve = \"concave\", direction = \"decreasing\")\n",
    "    \n",
    "    if knee.knee is not None:\n",
    "        print(f\"KneeLocator detected optimal topic number: {knee.knee}\")\n",
    "        return knee.knee\n",
    "    \n",
    "    best_topic = topic_range[np.argmax(coherence_scores)]\n",
    "    print(f\"Using highest coherence: {best_topic}\")\n",
    "    return best_topic\n",
    "\n",
    "def main():\n",
    "    file_path = \"combined_data_NLP.xlsx\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.dropna(subset = [\"answer\"])\n",
    "    df[\"text\"] = df[\"answer\"]\n",
    "\n",
    "    ## 불용어 및 토크나이즈 설정\n",
    "    df[\"tokens\"] = df[\"text\"].apply(extract_english_tokens)\n",
    "    tokens_list = df[\"tokens\"].tolist()\n",
    "\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "    ## 최적 토픽 수 계산\n",
    "    topic_range = range(2, 6)\n",
    "    coherence_scores = []\n",
    "\n",
    "    print(\"-----토픽 개수 산정을 위한 coherence score 계산 중-----\")\n",
    "\n",
    "    for n in topic_range:\n",
    "        lda_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = n, passes = 10, random_state = 42, alpha = \"auto\")\n",
    "        cm = CoherenceModel(model = lda_model, texts = tokens_list, dictionary = dictionary, coherence = \"c_v\")\n",
    "        coherence_scores.append(cm.get_coherence())\n",
    "        print(f\"Topic {n} - Coherence: {coherence_scores[-1]:.4f}\")\n",
    "\n",
    "    optimal_topics = select_optimal_topic(coherence_scores, topic_range)\n",
    "    print(f\"최적 토픽 수: {optimal_topics}\")\n",
    "\n",
    "    ## 메인 토픽 모델 학습\n",
    "    main_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = optimal_topics,\n",
    "                          passes = 50,\n",
    "                          random_state = 42,\n",
    "                          update_every = 1,\n",
    "                          eval_every = 1,\n",
    "                          alpha = \"auto\")\n",
    "    \n",
    "    ## 서브토픽 분석\n",
    "    subtopic_records = []\n",
    "    subtopic_labels = []\n",
    "    subtopic_words = []\n",
    "\n",
    "    ## 각 최적의 토픽별로 서브토픽 추출\n",
    "    for topic_id in range(optimal_topics):\n",
    "        print(f\"\\n[Subtopic Analysis for Topic {topic_id}]\")\n",
    "        topic_docs = []\n",
    "\n",
    "        ## 메인 토픽과 관련된 문서 추출\n",
    "        for i, dist in enumerate(main_model[corpus]):\n",
    "            for t_id, prob in dist:\n",
    "                if t_id == topic_id:\n",
    "                    topic_docs.append((i, prob))\n",
    "\n",
    "        topic_docs = sorted(topic_docs, key=lambda x: x[1], reverse=True)[:20]  ## 토픽을 대표하는 상위 20개 문서 추출\n",
    "        doc_indices = [idx for idx, _ in topic_docs]\n",
    "        filtered_texts = [tokens_list[i] for i in doc_indices]\n",
    "\n",
    "        ## 메인 토픽에 해당하는 문서가 하나도 없으면 다음 반복문으로 이동... 방어용 코드\n",
    "        if len(filtered_texts) == 0:\n",
    "            print(f\"⚠️ Topic {topic_id} has no representative documents.\")\n",
    "            continue\n",
    "\n",
    "        sub_dict = Dictionary(filtered_texts)\n",
    "        sub_corpus = [sub_dict.doc2bow(text) for text in tokens_list]  ## 전체 문서를 sub_dict 공간으로 사영?\n",
    "\n",
    "        # 최적 서브토픽 수 선택\n",
    "        scores = []\n",
    "        for n in range(2, 10):\n",
    "            try:\n",
    "                temp_model = LdaModel(corpus=[sub_dict.doc2bow(t) for t in filtered_texts], id2word=sub_dict, num_topics=n, passes=15, random_state=42)\n",
    "                cm = CoherenceModel(model=temp_model, texts=filtered_texts, dictionary=sub_dict, coherence='c_v')\n",
    "                scores.append(cm.get_coherence())\n",
    "            except:\n",
    "                scores.append(0)\n",
    "        best_n = range(2, 10)[np.argmax(scores)]\n",
    "\n",
    "        # 서브모델 학습\n",
    "        sub_model = LdaModel(corpus=sub_corpus, id2word=sub_dict, num_topics=best_n,\n",
    "                             passes=50,\n",
    "                             random_state=42,\n",
    "                             update_every=1,\n",
    "                             eval_every=1,\n",
    "                             alpha='auto')\n",
    "\n",
    "        # 키워드 저장\n",
    "        for t_id in range(best_n):\n",
    "            terms = sub_model.show_topic(t_id, topn=20)\n",
    "            keywords = \", \".join([f\"{word} ({weight:.3f})\" for word, weight in terms])\n",
    "            subtopic_words.append({\n",
    "                \"Model\": f\"LDA {topic_id}\",\n",
    "                \"Subtopic\": f\"{topic_id}-{t_id+1}\",\n",
    "                \"Keywords\": keywords\n",
    "            })\n",
    "\n",
    "        # 전체 문서에 대한 분포 계산\n",
    "        for i, doc in enumerate(tokens_list):\n",
    "            bow = sub_dict.doc2bow(doc)\n",
    "            dist = sub_model.get_document_topics(bow, minimum_probability=0) #해당 문서가 서브토픽들에 대해 가지는 확률 분포\n",
    "            record = {'doc_id': i}\n",
    "            for t_id, prob in dist:\n",
    "                label = f\"{topic_id}-{t_id+1}\"\n",
    "                record[label] = prob\n",
    "                if label not in subtopic_labels:\n",
    "                    subtopic_labels.append(label)\n",
    "            subtopic_records.append(record)\n",
    "\n",
    "    # 5. 결과 저장\n",
    "    result_df = pd.DataFrame(subtopic_records)\n",
    "    result_df = result_df.groupby(\"doc_id\").mean().reset_index()\n",
    "    result_df[\"llm_name\"] = df[\"model\"]\n",
    "\n",
    "    # 누락된 서브토픽 컬럼 0으로 채우기\n",
    "    for col in subtopic_labels:\n",
    "        if col not in result_df.columns:\n",
    "            result_df[col] = 0.0\n",
    "\n",
    "    result_df.to_excel(\"C:/Users/User/Desktop/soloplay/01. nlp/stopic_final/llm_subtopic_distribution_v2.xlsx\", index=False)\n",
    "    print(\"저장 완료: llm_subtopic_distribution.xlsx\")\n",
    "\n",
    "    words_df = pd.DataFrame(subtopic_words)\n",
    "    words_df.to_excel(\"C:/Users/User/Desktop/soloplay/01. nlp/stopic_final/subtopic_keywords_v2.xlsx\", index=False)\n",
    "    print(\"저장 완료: subtopic_keywords.xlsx\")\n",
    "\n",
    "# 6. 실행 시작\n",
    "if __name__ == '__main__':\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c7b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 12:04:42,818 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-09-10 12:04:43,710 : INFO : adding document #10000 to Dictionary<23014 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...>\n",
      "2025-09-10 12:04:44,131 : INFO : built Dictionary<34546 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...> from 14833 documents (total 1640281 corpus positions)\n",
      "2025-09-10 12:04:44,132 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<34546 unique tokens: ['academia', 'academic', 'achieve', 'actually', 'advanced']...> from 14833 documents (total 1640281 corpus positions)\", 'datetime': '2025-09-10T12:04:44.131996', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]', 'platform': 'Linux-5.15.0-1048-nvidia-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"combined_data_NLP.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "df = df.dropna(subset = [\"answer\"])\n",
    "df[\"text\"] = df[\"answer\"]\n",
    "\n",
    "## 불용어 및 토크나이즈 설정\n",
    "df[\"tokens\"] = df[\"text\"].apply(extract_english_tokens)\n",
    "tokens_list = df[\"tokens\"].tolist()\n",
    "\n",
    "dictionary = Dictionary(tokens_list)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad13d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame([dist for dist in main_model[corpus]])\n",
    "df_temp.to_csv(\"temp_dist.csv\", index = False)\n",
    "dup = df_temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e73b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_temp = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemp_dist.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m dup = deepcopy(df_temp)\n",
      "\u001b[31mTypeError\u001b[39m: read_csv() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "df_temp = pd.read_csv(\"temp_dist.csv\")\n",
    "dup = deepcopy(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad9b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0\n",
      "1 0\n",
      "1\n",
      "2 0\n",
      "2\n",
      "3 0\n",
      "3\n",
      "4 0\n",
      "4\n",
      "5 0\n",
      "5\n",
      "6 0\n",
      "6\n",
      "7 0\n",
      "7\n",
      "8 0\n",
      "8\n",
      "9 0\n",
      "9\n",
      "10 0\n",
      "10\n",
      "11 0\n",
      "11\n",
      "12 0\n",
      "12\n",
      "13 0\n",
      "13\n",
      "14 0\n",
      "14\n",
      "15 0\n",
      "15\n",
      "16 0\n",
      "16\n",
      "17 0\n",
      "17\n",
      "18 0\n",
      "18\n",
      "19 0\n",
      "19\n",
      "20 0\n",
      "20\n",
      "21 0\n",
      "21\n",
      "22 0\n",
      "22\n",
      "23 0\n",
      "23\n",
      "24 0\n",
      "24\n",
      "25 0\n",
      "25\n",
      "26 0\n",
      "26\n",
      "27 0\n",
      "27\n",
      "28 0\n",
      "28\n",
      "29 0\n",
      "29\n",
      "30 0\n",
      "30\n",
      "31 0\n",
      "31\n",
      "32 0\n",
      "32\n",
      "33 0\n",
      "33\n",
      "34 0\n",
      "34\n",
      "35 0\n",
      "35\n",
      "36 0\n",
      "36\n",
      "37 0\n",
      "37\n",
      "38 0\n",
      "38\n",
      "39 0\n",
      "39\n",
      "40 0\n",
      "40\n",
      "41 0\n",
      "41\n",
      "42 0\n",
      "42\n",
      "43 0\n",
      "43\n",
      "44 0\n",
      "44\n",
      "45 0\n",
      "45\n",
      "46 0\n",
      "46\n",
      "47 0\n",
      "47\n",
      "48 0\n",
      "48\n",
      "49 0\n",
      "49\n",
      "50 0\n",
      "50\n",
      "51 0\n",
      "51\n",
      "52 0\n",
      "52\n",
      "53 0\n",
      "53\n",
      "54 0\n",
      "54\n",
      "55 0\n",
      "55\n",
      "56 0\n",
      "56\n",
      "57 0\n",
      "57\n",
      "58 0\n",
      "58\n",
      "59 0\n",
      "59\n",
      "60 0\n",
      "60\n",
      "61 0\n",
      "61\n",
      "62 0\n",
      "62\n",
      "63 0\n",
      "63\n",
      "64 0\n",
      "64\n",
      "65 0\n",
      "65\n",
      "66 0\n",
      "66\n",
      "67 0\n",
      "67\n",
      "68 0\n",
      "68\n",
      "69 0\n",
      "69\n",
      "70 0\n",
      "70\n",
      "71 0\n",
      "71\n",
      "72 0\n",
      "72\n",
      "73 0\n",
      "73\n",
      "74 0\n",
      "74\n",
      "75 0\n",
      "75\n",
      "76 0\n",
      "76\n",
      "77 0\n",
      "77\n",
      "78 0\n",
      "78\n",
      "79 0\n",
      "79\n",
      "80 0\n",
      "80\n",
      "81 0\n",
      "81\n",
      "82 0\n",
      "82\n",
      "83 0\n",
      "83\n",
      "84 0\n",
      "84\n",
      "85 0\n",
      "85\n",
      "86 0\n",
      "86\n",
      "87 0\n",
      "87\n",
      "88 0\n",
      "88\n",
      "89 0\n",
      "89\n",
      "90 0\n",
      "90\n",
      "91 0\n",
      "91\n",
      "92 0\n",
      "92\n",
      "93 0\n",
      "93\n",
      "94 0\n",
      "94\n",
      "95 0\n",
      "95\n",
      "96 0\n",
      "96\n",
      "97 0\n",
      "97\n",
      "98 0\n",
      "98\n",
      "99 0\n",
      "99\n",
      "100 0\n",
      "100\n",
      "101 0\n",
      "101\n",
      "102 0\n",
      "102\n",
      "103 0\n",
      "103\n",
      "104 0\n",
      "104\n",
      "105 0\n",
      "105\n",
      "106 0\n",
      "106\n",
      "107 0\n",
      "107\n",
      "108 0\n",
      "108\n",
      "109 0\n",
      "109\n",
      "110 0\n",
      "110\n",
      "111 0\n",
      "111\n",
      "112 0\n",
      "112\n",
      "113 0\n",
      "113\n",
      "114 0\n",
      "114\n",
      "115 0\n",
      "115\n",
      "116 0\n",
      "116\n",
      "117 0\n",
      "117\n",
      "118 0\n",
      "118\n",
      "119 0\n",
      "119\n",
      "120 0\n",
      "120\n",
      "121 0\n",
      "121\n",
      "122 0\n",
      "122\n",
      "123 0\n",
      "123\n",
      "124 0\n",
      "124\n",
      "125 0\n",
      "125\n",
      "126 0\n",
      "126\n",
      "127 0\n",
      "127\n",
      "128 0\n",
      "128\n",
      "129 0\n",
      "129\n",
      "130 0\n",
      "130\n",
      "131 0\n",
      "131\n",
      "132 0\n",
      "132\n",
      "133 0\n",
      "133\n",
      "134 0\n",
      "134\n",
      "135 0\n",
      "135\n",
      "136 0\n",
      "136\n",
      "137 0\n",
      "137\n",
      "138 0\n",
      "138\n",
      "139 0\n",
      "139\n",
      "140 0\n",
      "140\n",
      "141 0\n",
      "141\n",
      "142 0\n",
      "142\n",
      "143 0\n",
      "143\n",
      "144 0\n",
      "144\n",
      "145 0\n",
      "145\n",
      "146 0\n",
      "146\n",
      "147 0\n",
      "147\n",
      "148 0\n",
      "148\n",
      "149 0\n",
      "149\n",
      "150 0\n",
      "150\n",
      "151 0\n",
      "151\n",
      "152 0\n",
      "152\n",
      "153 0\n",
      "153\n",
      "154 0\n",
      "154\n",
      "155 0\n",
      "155\n",
      "156 0\n",
      "156\n",
      "157 0\n",
      "157\n",
      "158 0\n",
      "158\n",
      "159 0\n",
      "159\n",
      "160 0\n",
      "160\n",
      "161 0\n",
      "161\n",
      "162 0\n",
      "162\n",
      "163 0\n",
      "163\n",
      "164 0\n",
      "164\n",
      "165 0\n",
      "165\n",
      "166 0\n",
      "166\n",
      "167 0\n",
      "167\n",
      "168 0\n",
      "168\n",
      "169 0\n",
      "169\n",
      "170 0\n",
      "170\n",
      "171 0\n",
      "171\n",
      "172 0\n",
      "172\n",
      "173 0\n",
      "173\n",
      "174 0\n",
      "174\n",
      "175 0\n",
      "175\n",
      "176 0\n",
      "176\n",
      "177 0\n",
      "177\n",
      "178 0\n",
      "178\n",
      "179 0\n",
      "179\n",
      "180 0\n",
      "180\n",
      "181 0\n",
      "181\n",
      "182 0\n",
      "182\n",
      "183 0\n",
      "183\n",
      "184 0\n",
      "184\n",
      "185 0\n",
      "185\n",
      "186 0\n",
      "186\n",
      "187 0\n",
      "187\n",
      "188 0\n",
      "188\n",
      "189 0\n",
      "189\n",
      "190 0\n",
      "190\n",
      "191 0\n",
      "191\n",
      "192 0\n",
      "192\n",
      "193 0\n",
      "193\n",
      "194 0\n",
      "194\n",
      "195 0\n",
      "195\n",
      "196 0\n",
      "196\n",
      "197 0\n",
      "197\n",
      "198 0\n",
      "198\n",
      "199 0\n",
      "199\n",
      "200 0\n",
      "200\n",
      "201 0\n",
      "201\n",
      "202 0\n",
      "202\n",
      "203 0\n",
      "203\n",
      "204 0\n",
      "204\n",
      "205 0\n",
      "205\n",
      "206 0\n",
      "206\n",
      "207 0\n",
      "207\n",
      "208 0\n",
      "208\n",
      "209 0\n",
      "209\n",
      "210 0\n",
      "210\n",
      "211 0\n",
      "211\n",
      "212 0\n",
      "212\n",
      "213 0\n",
      "213\n",
      "214 0\n",
      "214\n",
      "215 0\n",
      "215\n",
      "216 0\n",
      "216\n",
      "217 0\n",
      "217\n",
      "218 0\n",
      "218\n",
      "219 0\n",
      "219\n",
      "220 0\n",
      "220\n",
      "221 0\n",
      "221\n",
      "222 0\n",
      "222\n",
      "223 0\n",
      "223\n",
      "224 0\n",
      "224\n",
      "225 0\n",
      "225\n",
      "226 0\n",
      "226\n",
      "227 0\n",
      "227\n",
      "228 0\n",
      "228\n",
      "229 0\n",
      "229\n",
      "230 0\n",
      "230\n",
      "231 0\n",
      "231\n",
      "232 0\n",
      "232\n",
      "233 0\n",
      "233\n",
      "234 0\n",
      "234\n",
      "235 0\n",
      "235\n",
      "236 0\n",
      "236\n",
      "237 0\n",
      "237\n",
      "238 0\n",
      "238\n",
      "239 0\n",
      "239\n",
      "240 0\n",
      "240\n",
      "241 0\n",
      "241\n",
      "242 0\n",
      "242\n",
      "243 0\n",
      "243\n",
      "244 0\n",
      "244\n",
      "245 0\n",
      "245\n",
      "246 0\n",
      "246\n",
      "247 0\n",
      "247\n",
      "248 0\n",
      "248\n",
      "249 0\n",
      "249\n",
      "250 0\n",
      "250\n",
      "251 0\n",
      "251\n",
      "252 0\n",
      "252\n",
      "253 0\n",
      "253\n",
      "254 0\n",
      "254\n",
      "255 0\n",
      "255\n",
      "256 0\n",
      "256\n",
      "257 0\n",
      "257\n",
      "258 0\n",
      "258\n",
      "259 0\n",
      "259\n",
      "260 0\n",
      "260\n",
      "261 0\n",
      "261\n",
      "262 0\n",
      "262\n",
      "263 0\n",
      "263\n",
      "264 0\n",
      "264\n",
      "265 0\n",
      "265\n",
      "266 0\n",
      "266\n",
      "267 0\n",
      "267\n",
      "268 0\n",
      "268\n",
      "269 0\n",
      "269\n",
      "270 0\n",
      "270\n",
      "271 0\n",
      "271\n",
      "272 0\n",
      "272\n",
      "273 0\n",
      "273\n",
      "274 0\n",
      "274\n",
      "275 0\n",
      "275\n",
      "276 0\n",
      "276\n",
      "277 0\n",
      "277\n",
      "278 0\n",
      "278\n",
      "279 0\n",
      "279\n",
      "280 0\n",
      "280\n",
      "281 0\n",
      "281\n",
      "282 0\n",
      "282\n",
      "283 0\n",
      "283\n",
      "284 0\n",
      "284\n",
      "285 0\n",
      "285\n",
      "286 0\n",
      "286\n",
      "287 0\n",
      "287\n",
      "288 0\n",
      "288\n",
      "289 0\n",
      "289\n",
      "290 0\n",
      "290\n",
      "291 0\n",
      "291\n",
      "292 0\n",
      "292\n",
      "293 0\n",
      "293\n",
      "294 0\n",
      "294\n",
      "295 0\n",
      "295\n",
      "296 0\n",
      "296\n",
      "297 0\n",
      "297\n",
      "298 0\n",
      "298\n",
      "299 0\n",
      "299\n",
      "300 0\n",
      "300\n",
      "301 0\n",
      "301\n",
      "302 0\n",
      "302\n",
      "303 0\n",
      "303\n",
      "304 0\n",
      "304\n",
      "305 0\n",
      "305\n",
      "306 0\n",
      "306\n",
      "307 0\n",
      "307\n",
      "308 0\n",
      "308\n",
      "309 0\n",
      "309\n",
      "310 0\n",
      "310\n",
      "311 0\n",
      "311\n",
      "312 0\n",
      "312\n",
      "313 0\n",
      "313\n",
      "314 0\n",
      "314\n",
      "315 0\n",
      "315\n",
      "316 0\n",
      "316\n",
      "317 0\n",
      "317\n",
      "318 0\n",
      "318\n",
      "319 0\n",
      "319\n",
      "320 0\n",
      "320\n",
      "321 0\n",
      "321\n",
      "322 0\n",
      "322\n",
      "323 0\n",
      "323\n",
      "324 0\n",
      "324\n",
      "325 0\n",
      "325\n",
      "326 0\n",
      "326\n",
      "327 0\n",
      "327\n",
      "328 0\n",
      "328\n",
      "329 0\n",
      "329\n",
      "330 0\n",
      "330\n",
      "331 0\n",
      "331\n",
      "332 0\n",
      "332\n",
      "333 0\n",
      "333\n",
      "334 0\n",
      "334\n",
      "335 0\n",
      "335\n",
      "336 0\n",
      "336\n",
      "337 0\n",
      "337\n",
      "338 0\n",
      "338\n",
      "339 0\n",
      "339\n",
      "340 0\n",
      "340\n",
      "341 0\n",
      "341\n",
      "342 0\n",
      "342\n",
      "343 0\n",
      "343\n",
      "344 0\n",
      "344\n",
      "345 0\n",
      "345\n",
      "346 0\n",
      "346\n",
      "347 0\n",
      "347\n",
      "348 0\n",
      "348\n",
      "349 0\n",
      "349\n",
      "350 0\n",
      "350\n",
      "351 0\n",
      "351\n",
      "352 0\n",
      "352\n",
      "353 0\n",
      "353\n",
      "354 0\n",
      "354\n",
      "355 0\n",
      "355\n",
      "356 0\n",
      "356\n",
      "357 0\n",
      "357\n",
      "358 0\n",
      "358\n",
      "359 0\n",
      "359\n",
      "360 0\n",
      "360\n",
      "361 0\n",
      "361\n",
      "362 0\n",
      "362\n",
      "363 0\n",
      "363\n",
      "364 0\n",
      "364\n",
      "365 0\n",
      "365\n",
      "366 0\n",
      "366\n",
      "367 0\n",
      "367\n",
      "368 0\n",
      "368\n",
      "369 0\n",
      "369\n",
      "370 0\n",
      "370\n",
      "371 0\n",
      "371\n",
      "372 0\n",
      "372\n",
      "373 0\n",
      "373\n",
      "374 0\n",
      "374\n",
      "375 0\n",
      "375\n",
      "376 0\n",
      "376\n",
      "377 0\n",
      "377\n",
      "378 0\n",
      "378\n",
      "379 0\n",
      "379\n",
      "380 0\n",
      "380\n",
      "381 0\n",
      "381\n",
      "382 0\n",
      "382\n",
      "383 0\n",
      "383\n",
      "384 0\n",
      "384\n",
      "385 0\n",
      "385\n",
      "386 0\n",
      "386\n",
      "387 0\n",
      "387\n",
      "388 0\n",
      "388\n",
      "389 0\n",
      "389\n",
      "390 0\n",
      "390\n",
      "391 0\n",
      "391\n",
      "392 0\n",
      "392\n",
      "393 0\n",
      "393\n",
      "394 0\n",
      "394\n",
      "395 0\n",
      "395\n",
      "396 0\n",
      "396\n",
      "397 0\n",
      "397\n",
      "398 0\n",
      "398\n",
      "399 0\n",
      "399\n",
      "400 0\n",
      "400\n",
      "401 0\n",
      "401\n",
      "402 0\n",
      "402\n",
      "403 0\n",
      "403\n",
      "404 0\n",
      "404\n",
      "405 0\n",
      "405\n",
      "406 0\n",
      "406\n",
      "407 0\n",
      "407\n",
      "408 0\n",
      "408\n",
      "409 0\n",
      "409\n",
      "410 0\n",
      "410\n",
      "411 0\n",
      "411\n",
      "412 0\n",
      "412\n",
      "413 0\n",
      "413\n",
      "414 0\n",
      "414\n",
      "415 0\n",
      "415\n",
      "416 0\n",
      "416\n",
      "417 0\n",
      "417\n",
      "418 0\n",
      "418\n",
      "419 0\n",
      "419\n",
      "420 0\n",
      "420\n",
      "421 0\n",
      "421\n",
      "422 0\n",
      "422\n",
      "423 0\n",
      "423\n",
      "424 0\n",
      "424\n",
      "425 0\n",
      "425\n",
      "426 0\n",
      "426\n",
      "427 0\n",
      "427\n",
      "428 0\n",
      "428\n",
      "429 0\n",
      "429\n",
      "430 0\n",
      "430\n",
      "431 0\n",
      "431\n",
      "432 0\n",
      "432\n",
      "433 0\n",
      "433\n",
      "434 0\n",
      "434\n",
      "435 0\n",
      "435\n",
      "436 0\n",
      "436\n",
      "437 0\n",
      "437\n",
      "438 0\n",
      "438\n",
      "439 0\n",
      "439\n",
      "440 0\n",
      "440\n",
      "441 0\n",
      "441\n",
      "442 0\n",
      "442\n",
      "443 0\n",
      "443\n",
      "444 0\n",
      "444\n",
      "445 0\n",
      "445\n",
      "446 0\n",
      "446\n",
      "447 0\n",
      "447\n",
      "448 0\n",
      "448\n",
      "449 0\n",
      "449\n",
      "450 0\n",
      "450\n",
      "451 0\n",
      "451\n",
      "452 0\n",
      "452\n",
      "453 0\n",
      "453\n",
      "454 0\n",
      "454\n",
      "455 0\n",
      "455\n",
      "456 0\n",
      "456\n",
      "457 0\n",
      "457\n",
      "458 0\n",
      "458\n",
      "459 0\n",
      "459\n",
      "460 0\n",
      "460\n",
      "461 0\n",
      "461\n",
      "462 0\n",
      "462\n",
      "463 0\n",
      "463\n",
      "464 0\n",
      "464\n",
      "465 0\n",
      "465\n",
      "466 0\n",
      "466\n",
      "467 0\n",
      "467\n",
      "468 0\n",
      "468\n",
      "469 0\n",
      "469\n",
      "470 0\n",
      "470\n",
      "471 0\n",
      "471\n",
      "472 0\n",
      "472\n",
      "473 0\n",
      "473\n",
      "474 0\n",
      "474\n",
      "475 0\n",
      "475\n",
      "476 0\n",
      "476\n",
      "477 0\n",
      "477\n",
      "478 0\n",
      "478\n",
      "479 0\n",
      "479\n",
      "480 0\n",
      "480\n",
      "481 0\n",
      "481\n",
      "482 0\n",
      "482\n",
      "483 0\n",
      "483\n",
      "484 0\n",
      "484\n",
      "485 0\n",
      "485\n",
      "486 0\n",
      "486\n",
      "487 0\n",
      "487\n",
      "488 0\n",
      "488\n",
      "489 0\n",
      "489\n",
      "490 0\n",
      "490\n",
      "491 0\n",
      "491\n",
      "492 0\n",
      "492\n",
      "493 0\n",
      "493\n",
      "494 0\n",
      "494\n",
      "495 0\n",
      "495\n",
      "496 0\n",
      "496\n",
      "497 0\n",
      "497\n",
      "498 0\n",
      "498\n",
      "499 0\n",
      "499\n",
      "500 0\n",
      "500\n",
      "501 0\n",
      "501\n",
      "502 0\n",
      "502\n",
      "503 0\n",
      "503\n",
      "504 0\n",
      "504\n",
      "505 0\n",
      "505\n",
      "506 0\n",
      "506\n",
      "507 0\n",
      "507\n",
      "508 0\n",
      "508\n",
      "509 0\n",
      "509\n",
      "510 0\n",
      "510\n",
      "511 0\n",
      "511\n",
      "512 0\n",
      "512\n",
      "513 0\n",
      "513\n",
      "514 0\n",
      "514\n",
      "515 0\n",
      "515\n",
      "516 0\n",
      "516\n",
      "517 0\n",
      "517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 0\n",
      "518\n",
      "519 0\n",
      "519\n",
      "520 0\n",
      "520\n",
      "521 0\n",
      "521\n",
      "522 0\n",
      "522\n",
      "523 0\n",
      "523\n",
      "524 0\n",
      "524\n",
      "525 0\n",
      "525\n",
      "526 0\n",
      "526\n",
      "527 0\n",
      "527\n",
      "528 0\n",
      "528\n",
      "529 0\n",
      "529\n",
      "530 0\n",
      "530\n",
      "531 0\n",
      "531\n",
      "532 0\n",
      "532\n",
      "533 0\n",
      "533\n",
      "534 0\n",
      "534\n",
      "535 0\n",
      "535\n",
      "536 0\n",
      "536\n",
      "537 0\n",
      "537\n",
      "538 0\n",
      "538\n",
      "539 0\n",
      "539\n",
      "540 0\n",
      "540\n",
      "541 0\n",
      "541\n",
      "542 0\n",
      "542\n",
      "543 0\n",
      "543\n",
      "544 0\n",
      "544\n",
      "545 0\n",
      "545\n",
      "546 0\n",
      "546\n",
      "547 0\n",
      "547\n",
      "548 0\n",
      "548\n",
      "549 0\n",
      "549\n",
      "550 0\n",
      "550\n",
      "551 0\n",
      "551\n",
      "552 0\n",
      "552\n",
      "553 0\n",
      "553\n",
      "554 0\n",
      "554\n",
      "555 0\n",
      "555\n",
      "556 0\n",
      "556\n",
      "557 0\n",
      "557\n",
      "558 0\n",
      "558\n",
      "559 0\n",
      "559\n",
      "560 0\n",
      "560\n",
      "561 0\n",
      "561\n",
      "562 0\n",
      "562\n",
      "563 0\n",
      "563\n",
      "564 0\n",
      "564\n",
      "565 0\n",
      "565\n",
      "566 0\n",
      "566\n",
      "567 0\n",
      "567\n",
      "568 0\n",
      "568\n",
      "569 0\n",
      "569\n",
      "570 0\n",
      "570\n",
      "571 0\n",
      "571\n",
      "572 0\n",
      "572\n",
      "573 0\n",
      "573\n",
      "574 0\n",
      "574\n",
      "575 0\n",
      "575\n",
      "576 0\n",
      "576\n",
      "577 0\n",
      "577\n",
      "578 0\n",
      "578\n",
      "579 0\n",
      "579\n",
      "580 0\n",
      "580\n",
      "581 0\n",
      "581\n",
      "582 0\n",
      "582\n",
      "583 0\n",
      "583\n",
      "584 0\n",
      "584\n",
      "585 0\n",
      "585\n",
      "586 0\n",
      "586\n",
      "587 0\n",
      "587\n",
      "588 0\n",
      "588\n",
      "589 0\n",
      "589\n",
      "590 0\n",
      "590\n",
      "591 0\n",
      "591\n",
      "592 0\n",
      "592\n",
      "593 0\n",
      "593\n",
      "594 0\n",
      "594\n",
      "595 0\n",
      "595\n",
      "596 0\n",
      "596\n",
      "597 0\n",
      "597\n",
      "598 0\n",
      "598\n",
      "599 0\n",
      "599\n",
      "600 0\n",
      "600\n",
      "601 0\n",
      "601\n",
      "602 0\n",
      "602\n",
      "603 0\n",
      "603\n",
      "604 0\n",
      "604\n",
      "605 0\n",
      "605\n",
      "606 0\n",
      "606\n",
      "607 0\n",
      "607\n",
      "608 0\n",
      "608\n",
      "609 0\n",
      "609\n",
      "610 0\n",
      "610\n",
      "611 0\n",
      "611\n",
      "612 0\n",
      "612\n",
      "613 0\n",
      "613\n",
      "614 0\n",
      "614\n",
      "615 0\n",
      "615\n",
      "616 0\n",
      "616\n",
      "617 0\n",
      "617\n",
      "618 0\n",
      "618\n",
      "619 0\n",
      "619\n",
      "620 0\n",
      "620\n",
      "621 0\n",
      "621\n",
      "622 0\n",
      "622\n",
      "623 0\n",
      "623\n",
      "624 0\n",
      "624\n",
      "625 0\n",
      "625\n",
      "626 0\n",
      "626\n",
      "627 0\n",
      "627\n",
      "628 0\n",
      "628\n",
      "629 0\n",
      "629\n",
      "630 0\n",
      "630\n",
      "631 0\n",
      "631\n",
      "632 0\n",
      "632\n",
      "633 0\n",
      "633\n",
      "634 0\n",
      "634\n",
      "635 0\n",
      "635\n",
      "636 0\n",
      "636\n",
      "637 0\n",
      "637\n",
      "638 0\n",
      "638\n",
      "639 0\n",
      "639\n",
      "640 0\n",
      "640\n",
      "641 0\n",
      "641\n",
      "642 0\n",
      "642\n",
      "643 0\n",
      "643\n",
      "644 0\n",
      "644\n",
      "645 0\n",
      "645\n",
      "646 0\n",
      "646\n",
      "647 0\n",
      "647\n",
      "648 0\n",
      "648\n",
      "649 0\n",
      "649\n",
      "650 0\n",
      "650\n",
      "651 0\n",
      "651\n",
      "652 0\n",
      "652\n",
      "653 0\n",
      "653\n",
      "654 0\n",
      "654\n",
      "655 0\n",
      "655\n",
      "656 0\n",
      "656\n",
      "657 0\n",
      "657\n",
      "658 0\n",
      "658\n",
      "659 0\n",
      "659\n",
      "660 0\n",
      "660\n",
      "661 0\n",
      "661\n",
      "662 0\n",
      "662\n",
      "663 0\n",
      "663\n",
      "664 0\n",
      "664\n",
      "665 0\n",
      "665\n",
      "666 0\n",
      "666\n",
      "667 0\n",
      "667\n",
      "668 0\n",
      "668\n",
      "669 0\n",
      "669\n",
      "670 0\n",
      "670\n",
      "671 0\n",
      "671\n",
      "672 0\n",
      "672\n",
      "673 0\n",
      "673\n",
      "674 0\n",
      "674\n",
      "675 0\n",
      "675\n",
      "676 0\n",
      "676\n",
      "677 0\n",
      "677\n",
      "678 0\n",
      "678\n",
      "679 0\n",
      "679\n",
      "680 0\n",
      "680\n",
      "681 0\n",
      "681\n",
      "682 0\n",
      "682\n",
      "683 0\n",
      "683\n",
      "684 0\n",
      "684\n",
      "685 0\n",
      "685\n",
      "686 0\n",
      "686\n",
      "687 0\n",
      "687\n",
      "688 0\n",
      "688\n",
      "689 0\n",
      "689\n",
      "690 0\n",
      "690\n",
      "691 0\n",
      "691\n",
      "692 0\n",
      "692\n",
      "693 0\n",
      "693\n",
      "694 0\n",
      "694\n",
      "695 0\n",
      "695\n",
      "696 0\n",
      "696\n",
      "697 0\n",
      "697\n",
      "698 0\n",
      "698\n",
      "699 0\n",
      "699\n",
      "700 0\n",
      "700\n",
      "701 0\n",
      "701\n",
      "702 0\n",
      "702\n",
      "703 0\n",
      "703\n",
      "704 0\n",
      "704\n",
      "705 0\n",
      "705\n",
      "706 0\n",
      "706\n",
      "707 0\n",
      "707\n",
      "708 0\n",
      "708\n",
      "709 0\n",
      "709\n",
      "710 0\n",
      "710\n",
      "711 0\n",
      "711\n",
      "712 0\n",
      "712\n",
      "713 0\n",
      "713\n",
      "714 0\n",
      "714\n",
      "715 0\n",
      "715\n",
      "716 0\n",
      "716\n",
      "717 0\n",
      "717\n",
      "718 0\n",
      "718\n",
      "719 0\n",
      "719\n",
      "720 0\n",
      "720\n",
      "721 0\n",
      "721\n",
      "722 0\n",
      "722\n",
      "723 0\n",
      "723\n",
      "724 0\n",
      "724\n",
      "725 0\n",
      "725\n",
      "726 0\n",
      "726\n",
      "727 0\n",
      "727\n",
      "728 0\n",
      "728\n",
      "729 0\n",
      "729\n",
      "730 0\n",
      "730\n",
      "731 0\n",
      "731\n",
      "732 0\n",
      "732\n",
      "733 0\n",
      "733\n",
      "734 0\n",
      "734\n",
      "735 0\n",
      "735\n",
      "736 0\n",
      "736\n",
      "737 0\n",
      "737\n",
      "738 0\n",
      "738\n",
      "739 0\n",
      "739\n",
      "740 0\n",
      "740\n",
      "741 0\n",
      "741\n",
      "742 0\n",
      "742\n",
      "743 0\n",
      "743\n",
      "744 0\n",
      "744\n",
      "745 0\n",
      "745\n",
      "746 0\n",
      "746\n",
      "747 0\n",
      "747\n",
      "748 0\n",
      "748\n",
      "749 0\n",
      "749\n",
      "750 0\n",
      "750\n",
      "751 0\n",
      "751\n",
      "752 0\n",
      "752\n",
      "753 0\n",
      "753\n",
      "754 0\n",
      "754\n",
      "755 0\n",
      "755\n",
      "756 0\n",
      "756\n",
      "757 0\n",
      "757\n",
      "758 0\n",
      "758\n",
      "759 0\n",
      "759\n",
      "760 0\n",
      "760\n",
      "761 0\n",
      "761\n",
      "762 0\n",
      "762\n",
      "763 0\n",
      "763\n",
      "764 0\n",
      "764\n",
      "765 0\n",
      "765\n",
      "766 0\n",
      "766\n",
      "767 0\n",
      "767\n",
      "768 0\n",
      "768\n",
      "769 0\n",
      "769\n",
      "770 0\n",
      "770\n",
      "771 0\n",
      "771\n",
      "772 0\n",
      "772\n",
      "773 0\n",
      "773\n",
      "774 0\n",
      "774\n",
      "775 0\n",
      "775\n",
      "776 0\n",
      "776\n",
      "777 0\n",
      "777\n",
      "778 0\n",
      "778\n",
      "779 0\n",
      "779\n",
      "780 0\n",
      "780\n",
      "781 0\n",
      "781\n",
      "782 0\n",
      "782\n",
      "783 0\n",
      "783\n",
      "784 0\n",
      "784\n",
      "785 0\n",
      "785\n",
      "786 0\n",
      "786\n",
      "787 0\n",
      "787\n",
      "788 0\n",
      "788\n",
      "789 0\n",
      "789\n",
      "790 0\n",
      "790\n",
      "791 0\n",
      "791\n",
      "792 0\n",
      "792\n",
      "793 0\n",
      "793\n",
      "794 0\n",
      "794\n",
      "795 0\n",
      "795\n",
      "796 0\n",
      "796\n",
      "797 0\n",
      "797\n",
      "798 0\n",
      "798\n",
      "799 0\n",
      "799\n",
      "800 0\n",
      "800\n",
      "801 0\n",
      "801\n",
      "802 0\n",
      "802\n",
      "803 0\n",
      "803\n",
      "804 0\n",
      "804\n",
      "805 0\n",
      "805\n",
      "806 0\n",
      "806\n",
      "807 0\n",
      "807\n",
      "808 0\n",
      "808\n",
      "809 0\n",
      "809\n",
      "810 0\n",
      "810\n",
      "811 0\n",
      "811\n",
      "812 0\n",
      "812\n",
      "813 0\n",
      "813\n",
      "814 0\n",
      "814\n",
      "815 0\n",
      "815\n",
      "816 0\n",
      "816\n",
      "817 0\n",
      "817\n",
      "818 0\n",
      "818\n",
      "819 0\n",
      "819\n",
      "820 0\n",
      "820\n",
      "821 0\n",
      "821\n",
      "822 0\n",
      "822\n",
      "823 0\n",
      "823\n",
      "824 0\n",
      "824\n",
      "825 0\n",
      "825\n",
      "826 0\n",
      "826\n",
      "827 0\n",
      "827\n",
      "828 0\n",
      "828\n",
      "829 0\n",
      "829\n",
      "830 0\n",
      "830\n",
      "831 0\n",
      "831\n",
      "832 0\n",
      "832\n",
      "833 0\n",
      "833\n",
      "834 0\n",
      "834\n",
      "835 0\n",
      "835\n",
      "836 0\n",
      "836\n",
      "837 0\n",
      "837\n",
      "838 0\n",
      "838\n",
      "839 0\n",
      "839\n",
      "840 0\n",
      "840\n",
      "841 0\n",
      "841\n",
      "842 0\n",
      "842\n",
      "843 0\n",
      "843\n",
      "844 0\n",
      "844\n",
      "845 0\n",
      "845\n",
      "846 0\n",
      "846\n",
      "847 0\n",
      "847\n",
      "848 0\n",
      "848\n",
      "849 0\n",
      "849\n",
      "850 0\n",
      "850\n",
      "851 0\n",
      "851\n",
      "852 0\n",
      "852\n",
      "853 0\n",
      "853\n",
      "854 0\n",
      "854\n",
      "855 0\n",
      "855\n",
      "856 0\n",
      "856\n",
      "857 0\n",
      "857\n",
      "858 0\n",
      "858\n",
      "859 0\n",
      "859\n",
      "860 0\n",
      "860\n",
      "861 0\n",
      "861\n",
      "862 0\n",
      "862\n",
      "863 0\n",
      "863\n",
      "864 0\n",
      "864\n",
      "865 0\n",
      "865\n",
      "866 0\n",
      "866\n",
      "867 0\n",
      "867\n",
      "868 0\n",
      "868\n",
      "869 0\n",
      "869\n",
      "870 0\n",
      "870\n",
      "871 0\n",
      "871\n",
      "872 0\n",
      "872\n",
      "873 0\n",
      "873\n",
      "874 0\n",
      "874\n",
      "875 0\n",
      "875\n",
      "876 0\n",
      "876\n",
      "877 0\n",
      "877\n",
      "878 0\n",
      "878\n",
      "879 0\n",
      "879\n",
      "880 0\n",
      "880\n",
      "881 0\n",
      "881\n",
      "882 0\n",
      "882\n",
      "883 0\n",
      "883\n",
      "884 0\n",
      "884\n",
      "885 0\n",
      "885\n",
      "886 0\n",
      "886\n",
      "887 0\n",
      "887\n",
      "888 0\n",
      "888\n",
      "889 0\n",
      "889\n",
      "890 0\n",
      "890\n",
      "891 0\n",
      "891\n",
      "892 0\n",
      "892\n",
      "893 0\n",
      "893\n",
      "894 0\n",
      "894\n",
      "895 0\n",
      "895\n",
      "896 0\n",
      "896\n",
      "897 0\n",
      "897\n",
      "898 0\n",
      "898\n",
      "899 0\n",
      "899\n",
      "900 0\n",
      "900\n",
      "901 0\n",
      "901\n",
      "902 0\n",
      "902\n",
      "903 0\n",
      "903\n",
      "904 0\n",
      "904\n",
      "905 0\n",
      "905\n",
      "906 0\n",
      "906\n",
      "907 0\n",
      "907\n",
      "908 0\n",
      "908\n",
      "909 0\n",
      "909\n",
      "910 0\n",
      "910\n",
      "911 0\n",
      "911\n",
      "912 0\n",
      "912\n",
      "913 0\n",
      "913\n",
      "914 0\n",
      "914\n",
      "915 0\n",
      "915\n",
      "916 0\n",
      "916\n",
      "917 0\n",
      "917\n",
      "918 0\n",
      "918\n",
      "919 0\n",
      "919\n",
      "920 0\n",
      "920\n",
      "921 0\n",
      "921\n",
      "922 0\n",
      "922\n",
      "923 0\n",
      "923\n",
      "924 0\n",
      "924\n",
      "925 0\n",
      "925\n",
      "926 0\n",
      "926\n",
      "927 0\n",
      "927\n",
      "928 0\n",
      "928\n",
      "929 0\n",
      "929\n",
      "930 0\n",
      "930\n",
      "931 0\n",
      "931\n",
      "932 0\n",
      "932\n",
      "933 0\n",
      "933\n",
      "934 0\n",
      "934\n",
      "935 0\n",
      "935\n",
      "936 0\n",
      "936\n",
      "937 0\n",
      "937\n",
      "938 0\n",
      "938\n",
      "939 0\n",
      "939\n",
      "940 0\n",
      "940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941 0\n",
      "941\n",
      "942 0\n",
      "942\n",
      "943 0\n",
      "943\n",
      "944 0\n",
      "944\n",
      "945 0\n",
      "945\n",
      "946 0\n",
      "946\n",
      "947 0\n",
      "947\n",
      "948 0\n",
      "948\n",
      "949 0\n",
      "949\n",
      "950 0\n",
      "950\n",
      "951 0\n",
      "951\n",
      "952 0\n",
      "952\n",
      "953 0\n",
      "953\n",
      "954 0\n",
      "954\n",
      "955 0\n",
      "955\n",
      "956 0\n",
      "956\n",
      "957 0\n",
      "957\n",
      "958 0\n",
      "958\n",
      "959 0\n",
      "959\n",
      "960 0\n",
      "960\n",
      "961 0\n",
      "961\n",
      "962 0\n",
      "962\n",
      "963 0\n",
      "963\n",
      "964 0\n",
      "964\n",
      "965 0\n",
      "965\n",
      "966 0\n",
      "966\n",
      "967 0\n",
      "967\n",
      "968 0\n",
      "968\n",
      "969 0\n",
      "969\n",
      "970 0\n",
      "970\n",
      "971 0\n",
      "971\n",
      "972 0\n",
      "972\n",
      "973 0\n",
      "973\n",
      "974 0\n",
      "974\n",
      "975 0\n",
      "975\n",
      "976 0\n",
      "976\n",
      "977 0\n",
      "977\n",
      "978 0\n",
      "978\n",
      "979 0\n",
      "979\n",
      "980 0\n",
      "980\n",
      "981 0\n",
      "981\n",
      "982 0\n",
      "982\n",
      "983 0\n",
      "983\n",
      "984 0\n",
      "984\n",
      "985 0\n",
      "985\n",
      "986 0\n",
      "986\n",
      "987 0\n",
      "987\n",
      "988 0\n",
      "988\n",
      "989 0\n",
      "989\n",
      "990 0\n",
      "990\n",
      "991 0\n",
      "991\n",
      "992 0\n",
      "992\n",
      "993 0\n",
      "993\n",
      "994 0\n",
      "994\n",
      "995 0\n",
      "995\n",
      "996 0\n",
      "996\n",
      "997 0\n",
      "997\n",
      "998 0\n",
      "998\n",
      "999 0\n",
      "999\n",
      "1000 0\n",
      "1000\n",
      "1001 0\n",
      "1001\n",
      "1002 0\n",
      "1002\n",
      "1003 0\n",
      "1003\n",
      "1004 0\n",
      "1004\n",
      "1005 0\n",
      "1005\n",
      "1006 0\n",
      "1006\n",
      "1007 0\n",
      "1007\n",
      "1008 0\n",
      "1008\n",
      "1009 0\n",
      "1009\n",
      "1010 0\n",
      "1010\n",
      "1011 0\n",
      "1011\n",
      "1012 0\n",
      "1012\n",
      "1013 0\n",
      "1013\n",
      "1014 0\n",
      "1014\n",
      "1015 0\n",
      "1015\n",
      "1016 0\n",
      "1016\n",
      "1017 0\n",
      "1017\n",
      "1018 0\n",
      "1018\n",
      "1019 0\n",
      "1019\n",
      "1020 0\n",
      "1020\n",
      "1021 0\n",
      "1021\n",
      "1022 0\n",
      "1022\n",
      "1023 0\n",
      "1023\n",
      "1024 0\n",
      "1024\n",
      "1025 0\n",
      "1025\n",
      "1026 0\n",
      "1026\n",
      "1027 0\n",
      "1027\n",
      "1028 0\n",
      "1028\n",
      "1029 0\n",
      "1029\n",
      "1030 0\n",
      "1030\n",
      "1031 0\n",
      "1031\n",
      "1032 0\n",
      "1032\n",
      "1033 0\n",
      "1033\n",
      "1034 0\n",
      "1034\n",
      "1035 0\n",
      "1035\n",
      "1036 0\n",
      "1036\n",
      "1037 0\n",
      "1037\n",
      "1038 0\n",
      "1038\n",
      "1039 0\n",
      "1039\n",
      "1040 0\n",
      "1040\n",
      "1041 0\n",
      "1041\n",
      "1042 0\n",
      "1042\n",
      "1043 0\n",
      "1043\n",
      "1044 0\n",
      "1044\n",
      "1045 0\n",
      "1045\n",
      "1046 0\n",
      "1046\n",
      "1047 0\n",
      "1047\n",
      "1048 0\n",
      "1048\n",
      "1049 0\n",
      "1049\n",
      "1050 0\n",
      "1050\n",
      "1051 0\n",
      "1051\n",
      "1052 0\n",
      "1052\n",
      "1053 0\n",
      "1053\n",
      "1054 0\n",
      "1054\n",
      "1055 0\n",
      "1055\n",
      "1056 0\n",
      "1056\n",
      "1057 0\n",
      "1057\n",
      "1058 0\n",
      "1058\n",
      "1059 0\n",
      "1059\n",
      "1060 0\n",
      "1060\n",
      "1061 0\n",
      "1061\n",
      "1062 0\n",
      "1062\n",
      "1063 0\n",
      "1063\n",
      "1064 0\n",
      "1064\n",
      "1065 0\n",
      "1065\n",
      "1066 0\n",
      "1066\n",
      "1067 0\n",
      "1067\n",
      "1068 0\n",
      "1068\n",
      "1069 0\n",
      "1069\n",
      "1070 0\n",
      "1070\n",
      "1071 0\n",
      "1071\n",
      "1072 0\n",
      "1072\n",
      "1073 0\n",
      "1073\n",
      "1074 0\n",
      "1074\n",
      "1075 0\n",
      "1075\n",
      "1076 0\n",
      "1076\n",
      "1077 0\n",
      "1077\n",
      "1078 0\n",
      "1078\n",
      "1079 0\n",
      "1079\n",
      "1080 0\n",
      "1080\n",
      "1081 0\n",
      "1081\n",
      "1082 0\n",
      "1082\n",
      "1083 0\n",
      "1083\n",
      "1084 0\n",
      "1084\n",
      "1085 0\n",
      "1085\n",
      "1086 0\n",
      "1086\n",
      "1087 0\n",
      "1087\n",
      "1088 0\n",
      "1088\n",
      "1089 0\n",
      "1089\n",
      "1090 0\n",
      "1090\n",
      "1091 0\n",
      "1091\n",
      "1092 0\n",
      "1092\n",
      "1093 0\n",
      "1093\n",
      "1094 0\n",
      "1094\n",
      "1095 0\n",
      "1095\n",
      "1096 0\n",
      "1096\n",
      "1097 0\n",
      "1097\n",
      "1098 0\n",
      "1098\n",
      "1099 0\n",
      "1099\n",
      "1100 0\n",
      "1100\n",
      "1101 0\n",
      "1101\n",
      "1102 0\n",
      "1102\n",
      "1103 0\n",
      "1103\n",
      "1104 0\n",
      "1104\n",
      "1105 0\n",
      "1105\n",
      "1106 0\n",
      "1106\n",
      "1107 0\n",
      "1107\n",
      "1108 0\n",
      "1108\n",
      "1109 0\n",
      "1109\n",
      "1110 0\n",
      "1110\n",
      "1111 0\n",
      "1111\n",
      "1112 0\n",
      "1112\n",
      "1113 0\n",
      "1113\n",
      "1114 0\n",
      "1114\n",
      "1115 0\n",
      "1115\n",
      "1116 0\n",
      "1116\n",
      "1117 0\n",
      "1117\n",
      "1118 0\n",
      "1118\n",
      "1119 0\n",
      "1119\n",
      "1120 0\n",
      "1120\n",
      "1121 0\n",
      "1121\n",
      "1122 0\n",
      "1122\n",
      "1123 0\n",
      "1123\n",
      "1124 0\n",
      "1124\n",
      "1125 0\n",
      "1125\n",
      "1126 0\n",
      "1126\n",
      "1127 0\n",
      "1127\n",
      "1128 0\n",
      "1128\n",
      "1129 0\n",
      "1129\n",
      "1130 0\n",
      "1130\n",
      "1131 0\n",
      "1131\n",
      "1132 0\n",
      "1132\n",
      "1133 0\n",
      "1133\n",
      "1134 0\n",
      "1134\n",
      "1135 0\n",
      "1135\n",
      "1136 0\n",
      "1136\n",
      "1137 0\n",
      "1137\n",
      "1138 0\n",
      "1138\n",
      "1139 0\n",
      "1139\n",
      "1140 0\n",
      "1140\n",
      "1141 0\n",
      "1141\n",
      "1142 0\n",
      "1142\n",
      "1143 0\n",
      "1143\n",
      "1144 0\n",
      "1144\n",
      "1145 0\n",
      "1145\n",
      "1146 0\n",
      "1146\n",
      "1147 0\n",
      "1147\n",
      "1148 0\n",
      "1148\n",
      "1149 0\n",
      "1149\n",
      "1150 0\n",
      "1150\n",
      "1151 0\n",
      "1151\n",
      "1152 0\n",
      "1152\n",
      "1153 0\n",
      "1153\n",
      "1154 0\n",
      "1154\n",
      "1155 0\n",
      "1155\n",
      "1156 0\n",
      "1156\n",
      "1157 0\n",
      "1157\n",
      "1158 0\n",
      "1158\n",
      "1159 0\n",
      "1159\n",
      "1160 0\n",
      "1160\n",
      "1161 0\n",
      "1161\n",
      "1162 0\n",
      "1162\n",
      "1163 0\n",
      "1163\n",
      "1164 0\n",
      "1164\n",
      "1165 0\n",
      "1165\n",
      "1166 0\n",
      "1166\n",
      "1167 0\n",
      "1167\n",
      "1168 0\n",
      "1168\n",
      "1169 0\n",
      "1169\n",
      "1170 0\n",
      "1170\n",
      "1171 0\n",
      "1171\n",
      "1172 0\n",
      "1172\n",
      "1173 0\n",
      "1173\n",
      "1174 0\n",
      "1174\n",
      "1175 0\n",
      "1175\n",
      "1176 0\n",
      "1176\n",
      "1177 0\n",
      "1177\n",
      "1178 0\n",
      "1178\n",
      "1179 0\n",
      "1179\n",
      "1180 0\n",
      "1180\n",
      "1181 0\n",
      "1181\n",
      "1182 0\n",
      "1182\n",
      "1183 0\n",
      "1183\n",
      "1184 0\n",
      "1184\n",
      "1185 0\n",
      "1185\n",
      "1186 0\n",
      "1186\n",
      "1187 0\n",
      "1187\n",
      "1188 0\n",
      "1188\n",
      "1189 0\n",
      "1189\n",
      "1190 0\n",
      "1190\n",
      "1191 0\n",
      "1191\n",
      "1192 0\n",
      "1192\n",
      "1193 0\n",
      "1193\n",
      "1194 0\n",
      "1194\n",
      "1195 0\n",
      "1195\n",
      "1196 0\n",
      "1196\n",
      "1197 0\n",
      "1197\n",
      "1198 0\n",
      "1198\n",
      "1199 0\n",
      "1199\n",
      "1200 0\n",
      "1200\n",
      "1201 0\n",
      "1201\n",
      "1202 0\n",
      "1202\n",
      "1203 0\n",
      "1203\n",
      "1204 0\n",
      "1204\n",
      "1205 0\n",
      "1205\n",
      "1206 0\n",
      "1206\n",
      "1207 0\n",
      "1207\n",
      "1208 0\n",
      "1208\n",
      "1209 0\n",
      "1209\n",
      "1210 0\n",
      "1210\n",
      "1211 0\n",
      "1211\n",
      "1212 0\n",
      "1212\n",
      "1213 0\n",
      "1213\n",
      "1214 0\n",
      "1214\n",
      "1215 0\n",
      "1215\n",
      "1216 0\n",
      "1216\n",
      "1217 0\n",
      "1217\n",
      "1218 0\n",
      "1218\n",
      "1219 0\n",
      "1219\n",
      "1220 0\n",
      "1220\n",
      "1221 0\n",
      "1221\n",
      "1222 0\n",
      "1222\n",
      "1223 0\n",
      "1223\n",
      "1224 0\n",
      "1224\n",
      "1225 0\n",
      "1225\n",
      "1226 0\n",
      "1226\n",
      "1227 0\n",
      "1227\n",
      "1228 0\n",
      "1228\n",
      "1229 0\n",
      "1229\n",
      "1230 0\n",
      "1230\n",
      "1231 0\n",
      "1231\n",
      "1232 0\n",
      "1232\n",
      "1233 0\n",
      "1233\n",
      "1234 0\n",
      "1234\n",
      "1235 0\n",
      "1235\n",
      "1236 0\n",
      "1236\n",
      "1237 0\n",
      "1237\n",
      "1238 0\n",
      "1238\n",
      "1239 0\n",
      "1239\n",
      "1240 0\n",
      "1240\n",
      "1241 0\n",
      "1241\n",
      "1242 0\n",
      "1242\n",
      "1243 0\n",
      "1243\n",
      "1244 0\n",
      "1244\n",
      "1245 0\n",
      "1245\n",
      "1246 0\n",
      "1246\n",
      "1247 0\n",
      "1247\n",
      "1248 0\n",
      "1248\n",
      "1249 0\n",
      "1249\n",
      "1250 0\n",
      "1250\n",
      "1251 0\n",
      "1251\n",
      "1252 0\n",
      "1252\n",
      "1253 0\n",
      "1253\n",
      "1254 0\n",
      "1254\n",
      "1255 0\n",
      "1255\n",
      "1256 0\n",
      "1256\n",
      "1257 0\n",
      "1257\n",
      "1258 0\n",
      "1258\n",
      "1259 0\n",
      "1259\n",
      "1260 0\n",
      "1260\n",
      "1261 0\n",
      "1261\n",
      "1262 0\n",
      "1262\n",
      "1263 0\n",
      "1263\n",
      "1264 0\n",
      "1264\n",
      "1265 0\n",
      "1265\n",
      "1266 0\n",
      "1266\n",
      "1267 0\n",
      "1267\n",
      "1268 0\n",
      "1268\n",
      "1269 0\n",
      "1269\n",
      "1270 0\n",
      "1270\n",
      "1271 0\n",
      "1271\n",
      "1272 0\n",
      "1272\n",
      "1273 0\n",
      "1273\n",
      "1274 0\n",
      "1274\n",
      "1275 0\n",
      "1275\n",
      "1276 0\n",
      "1276\n",
      "1277 0\n",
      "1277\n",
      "1278 0\n",
      "1278\n",
      "1279 0\n",
      "1279\n",
      "1280 0\n",
      "1280\n",
      "1281 0\n",
      "1281\n",
      "1282 0\n",
      "1282\n",
      "1283 0\n",
      "1283\n",
      "1284 0\n",
      "1284\n",
      "1285 0\n",
      "1285\n",
      "1286 0\n",
      "1286\n",
      "1287 0\n",
      "1287\n",
      "1288 0\n",
      "1288\n",
      "1289 0\n",
      "1289\n",
      "1290 0\n",
      "1290\n",
      "1291 0\n",
      "1291\n",
      "1292 0\n",
      "1292\n",
      "1293 0\n",
      "1293\n",
      "1294 0\n",
      "1294\n",
      "1295 0\n",
      "1295\n",
      "1296 0\n",
      "1296\n",
      "1297 0\n",
      "1297\n",
      "1298 0\n",
      "1298\n",
      "1299 0\n",
      "1299\n",
      "1300 0\n",
      "1300\n",
      "1301 0\n",
      "1301\n",
      "1302 0\n",
      "1302\n",
      "1303 0\n",
      "1303\n",
      "1304 0\n",
      "1304\n",
      "1305 0\n",
      "1305\n",
      "1306 0\n",
      "1306\n",
      "1307 0\n",
      "1307\n",
      "1308 0\n",
      "1308\n",
      "1309 0\n",
      "1309\n",
      "1310 0\n",
      "1310\n",
      "1311 0\n",
      "1311\n",
      "1312 0\n",
      "1312\n",
      "1313 0\n",
      "1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314 0\n",
      "1314\n",
      "1315 0\n",
      "1315\n",
      "1316 0\n",
      "1316\n",
      "1317 0\n",
      "1317\n",
      "1318 0\n",
      "1318\n",
      "1319 0\n",
      "1319\n",
      "1320 0\n",
      "1320\n",
      "1321 0\n",
      "1321\n",
      "1322 0\n",
      "1322\n",
      "1323 0\n",
      "1323\n",
      "1324 0\n",
      "1324\n",
      "1325 0\n",
      "1325\n",
      "1326 0\n",
      "1326\n",
      "1327 0\n",
      "1327\n",
      "1328 0\n",
      "1328\n",
      "1329 0\n",
      "1329\n",
      "1330 0\n",
      "1330\n",
      "1331 0\n",
      "1331\n",
      "1332 0\n",
      "1332\n",
      "1333 0\n",
      "1333\n",
      "1334 0\n",
      "1334\n",
      "1335 0\n",
      "1335\n",
      "1336 0\n",
      "1336\n",
      "1337 0\n",
      "1337\n",
      "1338 0\n",
      "1338\n",
      "1339 0\n",
      "1339\n",
      "1340 0\n",
      "1340\n",
      "1341 0\n",
      "1341\n",
      "1342 0\n",
      "1342\n",
      "1343 0\n",
      "1343\n",
      "1344 0\n",
      "1344\n",
      "1345 0\n",
      "1345\n",
      "1346 0\n",
      "1346\n",
      "1347 0\n",
      "1347\n",
      "1348 0\n",
      "1348\n",
      "1349 0\n",
      "1349\n",
      "1350 0\n",
      "1350\n",
      "1351 0\n",
      "1351\n",
      "1352 0\n",
      "1352\n",
      "1353 0\n",
      "1353\n",
      "1354 0\n",
      "1354\n",
      "1355 0\n",
      "1355\n",
      "1356 0\n",
      "1356\n",
      "1357 0\n",
      "1357\n",
      "1358 0\n",
      "1358\n",
      "1359 0\n",
      "1359\n",
      "1360 0\n",
      "1360\n",
      "1361 0\n",
      "1361\n",
      "1362 0\n",
      "1362\n",
      "1363 0\n",
      "1363\n",
      "1364 0\n",
      "1364\n",
      "1365 0\n",
      "1365\n",
      "1366 0\n",
      "1366\n",
      "1367 0\n",
      "1367\n",
      "1368 0\n",
      "1368\n",
      "1369 0\n",
      "1369\n",
      "1370 0\n",
      "1370\n",
      "1371 0\n",
      "1371\n",
      "1372 0\n",
      "1372\n",
      "1373 0\n",
      "1373\n",
      "1374 0\n",
      "1374\n",
      "1375 0\n",
      "1375\n",
      "1376 0\n",
      "1376\n",
      "1377 0\n",
      "1377\n",
      "1378 0\n",
      "1378\n",
      "1379 0\n",
      "1379\n",
      "1380 0\n",
      "1380\n",
      "1381 0\n",
      "1381\n",
      "1382 0\n",
      "1382\n",
      "1383 0\n",
      "1383\n",
      "1384 0\n",
      "1384\n",
      "1385 0\n",
      "1385\n",
      "1386 0\n",
      "1386\n",
      "1387 0\n",
      "1387\n",
      "1388 0\n",
      "1388\n",
      "1389 0\n",
      "1389\n",
      "1390 0\n",
      "1390\n",
      "1391 0\n",
      "1391\n",
      "1392 0\n",
      "1392\n",
      "1393 0\n",
      "1393\n",
      "1394 0\n",
      "1394\n",
      "1395 0\n",
      "1395\n",
      "1396 0\n",
      "1396\n",
      "1397 0\n",
      "1397\n",
      "1398 0\n",
      "1398\n",
      "1399 0\n",
      "1399\n",
      "1400 0\n",
      "1400\n",
      "1401 0\n",
      "1401\n",
      "1402 0\n",
      "1402\n",
      "1403 0\n",
      "1403\n",
      "1404 0\n",
      "1404\n",
      "1405 0\n",
      "1405\n",
      "1406 0\n",
      "1406\n",
      "1407 0\n",
      "1407\n",
      "1408 0\n",
      "1408\n",
      "1409 0\n",
      "1409\n",
      "1410 0\n",
      "1410\n",
      "1411 0\n",
      "1411\n",
      "1412 0\n",
      "1412\n",
      "1413 0\n",
      "1413\n",
      "1414 0\n",
      "1414\n",
      "1415 0\n",
      "1415\n",
      "1416 0\n",
      "1416\n",
      "1417 0\n",
      "1417\n",
      "1418 0\n",
      "1418\n",
      "1419 0\n",
      "1419\n",
      "1420 0\n",
      "1420\n",
      "1421 0\n",
      "1421\n",
      "1422 0\n",
      "1422\n",
      "1423 0\n",
      "1423\n",
      "1424 0\n",
      "1424\n",
      "1425 0\n",
      "1425\n",
      "1426 0\n",
      "1426\n",
      "1427 0\n",
      "1427\n",
      "1428 0\n",
      "1428\n",
      "1429 0\n",
      "1429\n",
      "1430 0\n",
      "1430\n",
      "1431 0\n",
      "1431\n",
      "1432 0\n",
      "1432\n",
      "1433 0\n",
      "1433\n",
      "1434 0\n",
      "1434\n",
      "1435 0\n",
      "1435\n",
      "1436 0\n",
      "1436\n",
      "1437 0\n",
      "1437\n",
      "1438 0\n",
      "1438\n",
      "1439 0\n",
      "1439\n",
      "1440 0\n",
      "1440\n",
      "1441 0\n",
      "1441\n",
      "1442 0\n",
      "1442\n",
      "1443 0\n",
      "1443\n",
      "1444 0\n",
      "1444\n",
      "1445 0\n",
      "1445\n",
      "1446 0\n",
      "1446\n",
      "1447 0\n",
      "1447\n",
      "1448 0\n",
      "1448\n",
      "1449 0\n",
      "1449\n",
      "1450 0\n",
      "1450\n",
      "1451 0\n",
      "1451\n",
      "1452 0\n",
      "1452\n",
      "1453 0\n",
      "1453\n",
      "1454 0\n",
      "1454\n",
      "1455 0\n",
      "1455\n",
      "1456 0\n",
      "1456\n",
      "1457 0\n",
      "1457\n",
      "1458 0\n",
      "1458\n",
      "1459 0\n",
      "1459\n",
      "1460 0\n",
      "1460\n",
      "1461 0\n",
      "1461\n",
      "1462 0\n",
      "1462\n",
      "1463 0\n",
      "1463\n",
      "1464 0\n",
      "1464\n",
      "1465 0\n",
      "1465\n",
      "1466 0\n",
      "1466\n",
      "1467 0\n",
      "1467\n",
      "1468 0\n",
      "1468\n",
      "1469 0\n",
      "1469\n",
      "1470 0\n",
      "1470\n",
      "1471 0\n",
      "1471\n",
      "1472 0\n",
      "1472\n",
      "1473 0\n",
      "1473\n",
      "1474 0\n",
      "1474\n",
      "1475 0\n",
      "1475\n",
      "1476 0\n",
      "1476\n",
      "1477 0\n",
      "1477\n",
      "1478 0\n",
      "1478\n",
      "1479 0\n",
      "1479\n",
      "1480 0\n",
      "1480\n",
      "1481 0\n",
      "1481\n",
      "1482 0\n",
      "1482\n",
      "1483 0\n",
      "1483\n",
      "1484 0\n",
      "1484\n",
      "1485 0\n",
      "1485\n",
      "1486 0\n",
      "1486\n",
      "1487 0\n",
      "1487\n",
      "1488 0\n",
      "1488\n",
      "1489 0\n",
      "1489\n",
      "1490 0\n",
      "1490\n",
      "1491 0\n",
      "1491\n",
      "1492 0\n",
      "1492\n",
      "1493 0\n",
      "1493\n",
      "1494 0\n",
      "1494\n",
      "1495 0\n",
      "1495\n",
      "1496 0\n",
      "1496\n",
      "1497 0\n",
      "1497\n",
      "1498 0\n",
      "1498\n",
      "1499 0\n",
      "1499\n",
      "1500 0\n",
      "1500\n",
      "1501 0\n",
      "1501\n",
      "1502 0\n",
      "1502\n",
      "1503 0\n",
      "1503\n",
      "1504 0\n",
      "1504\n",
      "1505 0\n",
      "1505\n",
      "1506 0\n",
      "1506\n",
      "1507 0\n",
      "1507\n",
      "1508 0\n",
      "1508\n",
      "1509 0\n",
      "1509\n",
      "1510 0\n",
      "1510\n",
      "1511 0\n",
      "1511\n",
      "1512 0\n",
      "1512\n",
      "1513 0\n",
      "1513\n",
      "1514 0\n",
      "1514\n",
      "1515 0\n",
      "1515\n",
      "1516 0\n",
      "1516\n",
      "1517 0\n",
      "1517\n",
      "1518 0\n",
      "1518\n",
      "1519 0\n",
      "1519\n",
      "1520 0\n",
      "1520\n",
      "1521 0\n",
      "1521\n",
      "1522 0\n",
      "1522\n",
      "1523 0\n",
      "1523\n",
      "1524 0\n",
      "1524\n",
      "1525 0\n",
      "1525\n",
      "1526 0\n",
      "1526\n",
      "1527 0\n",
      "1527\n",
      "1528 0\n",
      "1528\n",
      "1529 0\n",
      "1529\n",
      "1530 0\n",
      "1530\n",
      "1531 0\n",
      "1531\n",
      "1532 0\n",
      "1532\n",
      "1533 0\n",
      "1533\n",
      "1534 0\n",
      "1534\n",
      "1535 0\n",
      "1535\n",
      "1536 0\n",
      "1536\n",
      "1537 0\n",
      "1537\n",
      "1538 0\n",
      "1538\n",
      "1539 0\n",
      "1539\n",
      "1540 0\n",
      "1540\n",
      "1541 0\n",
      "1541\n",
      "1542 0\n",
      "1542\n",
      "1543 0\n",
      "1543\n",
      "1544 0\n",
      "1544\n",
      "1545 0\n",
      "1545\n",
      "1546 0\n",
      "1546\n",
      "1547 0\n",
      "1547\n",
      "1548 0\n",
      "1548\n",
      "1549 0\n",
      "1549\n",
      "1550 0\n",
      "1550\n",
      "1551 0\n",
      "1551\n",
      "1552 0\n",
      "1552\n",
      "1553 0\n",
      "1553\n",
      "1554 0\n",
      "1554\n",
      "1555 0\n",
      "1555\n",
      "1556 0\n",
      "1556\n",
      "1557 0\n",
      "1557\n",
      "1558 0\n",
      "1558\n",
      "1559 0\n",
      "1559\n",
      "1560 0\n",
      "1560\n",
      "1561 0\n",
      "1561\n",
      "1562 0\n",
      "1562\n",
      "1563 0\n",
      "1563\n",
      "1564 0\n",
      "1564\n",
      "1565 0\n",
      "1565\n",
      "1566 0\n",
      "1566\n",
      "1567 0\n",
      "1567\n",
      "1568 0\n",
      "1568\n",
      "1569 0\n",
      "1569\n",
      "1570 0\n",
      "1570\n",
      "1571 0\n",
      "1571\n",
      "1572 0\n",
      "1572\n",
      "1573 0\n",
      "1573\n",
      "1574 0\n",
      "1574\n",
      "1575 0\n",
      "1575\n",
      "1576 0\n",
      "1576\n",
      "1577 0\n",
      "1577\n",
      "1578 0\n",
      "1578\n",
      "1579 0\n",
      "1579\n",
      "1580 0\n",
      "1580\n",
      "1581 0\n",
      "1581\n",
      "1582 0\n",
      "1582\n",
      "1583 0\n",
      "1583\n",
      "1584 0\n",
      "1584\n",
      "1585 0\n",
      "1585\n",
      "1586 0\n",
      "1586\n",
      "1587 0\n",
      "1587\n",
      "1588 0\n",
      "1588\n",
      "1589 0\n",
      "1589\n",
      "1590 0\n",
      "1590\n",
      "1591 0\n",
      "1591\n",
      "1592 0\n",
      "1592\n",
      "1593 0\n",
      "1593\n",
      "1594 0\n",
      "1594\n",
      "1595 0\n",
      "1595\n",
      "1596 0\n",
      "1596\n",
      "1597 0\n",
      "1597\n",
      "1598 0\n",
      "1598\n",
      "1599 0\n",
      "1599\n",
      "1600 0\n",
      "1600\n",
      "1601 0\n",
      "1601\n",
      "1602 0\n",
      "1602\n",
      "1603 0\n",
      "1603\n",
      "1604 0\n",
      "1604\n",
      "1605 0\n",
      "1605\n",
      "1606 0\n",
      "1606\n",
      "1607 0\n",
      "1607\n",
      "1608 0\n",
      "1608\n",
      "1609 0\n",
      "1609\n",
      "1610 0\n",
      "1610\n",
      "1611 0\n",
      "1611\n",
      "1612 0\n",
      "1612\n",
      "1613 0\n",
      "1613\n",
      "1614 0\n",
      "1614\n",
      "1615 0\n",
      "1615\n",
      "1616 0\n",
      "1616\n",
      "1617 0\n",
      "1617\n",
      "1618 0\n",
      "1618\n",
      "1619 0\n",
      "1619\n",
      "1620 0\n",
      "1620\n",
      "1621 0\n",
      "1621\n",
      "1622 0\n",
      "1622\n",
      "1623 0\n",
      "1623\n",
      "1624 0\n",
      "1624\n",
      "1625 0\n",
      "1625\n",
      "1626 0\n",
      "1626\n",
      "1627 0\n",
      "1627\n",
      "1628 0\n",
      "1628\n",
      "1629 0\n",
      "1629\n",
      "1630 0\n",
      "1630\n",
      "1631 0\n",
      "1631\n",
      "1632 0\n",
      "1632\n",
      "1633 0\n",
      "1633\n",
      "1634 0\n",
      "1634\n",
      "1635 0\n",
      "1635\n",
      "1636 0\n",
      "1636\n",
      "1637 0\n",
      "1637\n",
      "1638 0\n",
      "1638\n",
      "1639 0\n",
      "1639\n",
      "1640 0\n",
      "1640\n",
      "1641 0\n",
      "1641\n",
      "1642 0\n",
      "1642\n",
      "1643 0\n",
      "1643\n",
      "1644 0\n",
      "1644\n",
      "1645 0\n",
      "1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646 0\n",
      "1646\n",
      "1647 0\n",
      "1647\n",
      "1648 0\n",
      "1648\n",
      "1649 0\n",
      "1649\n",
      "1650 0\n",
      "1650\n",
      "1651 0\n",
      "1651\n",
      "1652 0\n",
      "1652\n",
      "1653 0\n",
      "1653\n",
      "1654 0\n",
      "1654\n",
      "1655 0\n",
      "1655\n",
      "1656 0\n",
      "1656\n",
      "1657 0\n",
      "1657\n",
      "1658 0\n",
      "1658\n",
      "1659 0\n",
      "1659\n",
      "1660 0\n",
      "1660\n",
      "1661 0\n",
      "1661\n",
      "1662 0\n",
      "1662\n",
      "1663 0\n",
      "1663\n",
      "1664 0\n",
      "1664\n",
      "1665 0\n",
      "1665\n",
      "1666 0\n",
      "1666\n",
      "1667 0\n",
      "1667\n",
      "1668 0\n",
      "1668\n",
      "1669 0\n",
      "1669\n",
      "1670 0\n",
      "1670\n",
      "1671 0\n",
      "1671\n",
      "1672 0\n",
      "1672\n",
      "1673 0\n",
      "1673\n",
      "1674 0\n",
      "1674\n",
      "1675 0\n",
      "1675\n",
      "1676 0\n",
      "1676\n",
      "1677 0\n",
      "1677\n",
      "1678 0\n",
      "1678\n",
      "1679 0\n",
      "1679\n",
      "1680 0\n",
      "1680\n",
      "1681 0\n",
      "1681\n",
      "1682 0\n",
      "1682\n",
      "1683 0\n",
      "1683\n",
      "1684 0\n",
      "1684\n",
      "1685 0\n",
      "1685\n",
      "1686 0\n",
      "1686\n",
      "1687 0\n",
      "1687\n",
      "1688 0\n",
      "1688\n",
      "1689 0\n",
      "1689\n",
      "1690 0\n",
      "1690\n",
      "1691 0\n",
      "1691\n",
      "1692 0\n",
      "1692\n",
      "1693 0\n",
      "1693\n",
      "1694 0\n",
      "1694\n",
      "1695 0\n",
      "1695\n",
      "1696 0\n",
      "1696\n",
      "1697 0\n",
      "1697\n",
      "1698 0\n",
      "1698\n",
      "1699 0\n",
      "1699\n",
      "1700 0\n",
      "1700\n",
      "1701 0\n",
      "1701\n",
      "1702 0\n",
      "1702\n",
      "1703 0\n",
      "1703\n",
      "1704 0\n",
      "1704\n",
      "1705 0\n",
      "1705\n",
      "1706 0\n",
      "1706\n",
      "1707 0\n",
      "1707\n",
      "1708 0\n",
      "1708\n",
      "1709 0\n",
      "1709\n",
      "1710 0\n",
      "1710\n",
      "1711 0\n",
      "1711\n",
      "1712 0\n",
      "1712\n",
      "1713 0\n",
      "1713\n",
      "1714 0\n",
      "1714\n",
      "1715 0\n",
      "1715\n",
      "1716 0\n",
      "1716\n",
      "1717 0\n",
      "1717\n",
      "1718 0\n",
      "1718\n",
      "1719 0\n",
      "1719\n",
      "1720 0\n",
      "1720\n",
      "1721 0\n",
      "1721\n",
      "1722 0\n",
      "1722\n",
      "1723 0\n",
      "1723\n",
      "1724 0\n",
      "1724\n",
      "1725 0\n",
      "1725\n",
      "1726 0\n",
      "1726\n",
      "1727 0\n",
      "1727\n",
      "1728 0\n",
      "1728\n",
      "1729 0\n",
      "1729\n",
      "1730 0\n",
      "1730\n",
      "1731 0\n",
      "1731\n",
      "1732 0\n",
      "1732\n",
      "1733 0\n",
      "1733\n",
      "1734 0\n",
      "1734\n",
      "1735 0\n",
      "1735\n",
      "1736 0\n",
      "1736\n",
      "1737 0\n",
      "1737\n",
      "1738 0\n",
      "1738\n",
      "1739 0\n",
      "1739\n",
      "1740 0\n",
      "1740\n",
      "1741 0\n",
      "1741\n",
      "1742 0\n",
      "1742\n",
      "1743 0\n",
      "1743\n",
      "1744 0\n",
      "1744\n",
      "1745 0\n",
      "1745\n",
      "1746 0\n",
      "1746\n",
      "1747 0\n",
      "1747\n",
      "1748 0\n",
      "1748\n",
      "1749 0\n",
      "1749\n",
      "1750 0\n",
      "1750\n",
      "1751 0\n",
      "1751\n",
      "1752 0\n",
      "1752\n",
      "1753 0\n",
      "1753\n",
      "1754 0\n",
      "1754\n",
      "1755 0\n",
      "1755\n",
      "1756 0\n",
      "1756\n",
      "1757 0\n",
      "1757\n",
      "1758 0\n",
      "1758\n",
      "1759 0\n",
      "1759\n",
      "1760 0\n",
      "1760\n",
      "1761 0\n",
      "1761\n",
      "1762 0\n",
      "1762\n",
      "1763 0\n",
      "1763\n",
      "1764 0\n",
      "1764\n",
      "1765 0\n",
      "1765\n",
      "1766 0\n",
      "1766\n",
      "1767 0\n",
      "1767\n",
      "1768 0\n",
      "1768\n",
      "1769 0\n",
      "1769\n",
      "1770 0\n",
      "1770\n",
      "1771 0\n",
      "1771\n",
      "1772 0\n",
      "1772\n",
      "1773 0\n",
      "1773\n",
      "1774 0\n",
      "1774\n",
      "1775 0\n",
      "1775\n",
      "1776 0\n",
      "1776\n",
      "1777 0\n",
      "1777\n",
      "1778 0\n",
      "1778\n",
      "1779 0\n",
      "1779\n",
      "1780 0\n",
      "1780\n",
      "1781 0\n",
      "1781\n",
      "1782 0\n",
      "1782\n",
      "1783 0\n",
      "1783\n",
      "1784 0\n",
      "1784\n",
      "1785 0\n",
      "1785\n",
      "1786 0\n",
      "1786\n",
      "1787 0\n",
      "1787\n",
      "1788 0\n",
      "1788\n",
      "1789 0\n",
      "1789\n",
      "1790 0\n",
      "1790\n",
      "1791 0\n",
      "1791\n",
      "1792 0\n",
      "1792\n",
      "1793 0\n",
      "1793\n",
      "1794 0\n",
      "1794\n",
      "1795 0\n",
      "1795\n",
      "1796 0\n",
      "1796\n",
      "1797 0\n",
      "1797\n",
      "1798 0\n",
      "1798\n",
      "1799 0\n",
      "1799\n",
      "1800 0\n",
      "1800\n",
      "1801 0\n",
      "1801\n",
      "1802 0\n",
      "1802\n",
      "1803 0\n",
      "1803\n",
      "1804 0\n",
      "1804\n",
      "1805 0\n",
      "1805\n",
      "1806 0\n",
      "1806\n",
      "1807 0\n",
      "1807\n",
      "1808 0\n",
      "1808\n",
      "1809 0\n",
      "1809\n",
      "1810 0\n",
      "1810\n",
      "1811 0\n",
      "1811\n",
      "1812 0\n",
      "1812\n",
      "1813 0\n",
      "1813\n",
      "1814 0\n",
      "1814\n",
      "1815 0\n",
      "1815\n",
      "1816 0\n",
      "1816\n",
      "1817 0\n",
      "1817\n",
      "1818 0\n",
      "1818\n",
      "1819 0\n",
      "1819\n",
      "1820 0\n",
      "1820\n",
      "1821 0\n",
      "1821\n",
      "1822 0\n",
      "1822\n",
      "1823 0\n",
      "1823\n",
      "1824 0\n",
      "1824\n",
      "1825 0\n",
      "1825\n",
      "1826 0\n",
      "1826\n",
      "1827 0\n",
      "1827\n",
      "1828 0\n",
      "1828\n",
      "1829 0\n",
      "1829\n",
      "1830 0\n",
      "1830\n",
      "1831 0\n",
      "1831\n",
      "1832 0\n",
      "1832\n",
      "1833 0\n",
      "1833\n",
      "1834 0\n",
      "1834\n",
      "1835 0\n",
      "1835\n",
      "1836 0\n",
      "1836\n",
      "1837 0\n",
      "1837\n",
      "1838 0\n",
      "1838\n",
      "1839 0\n",
      "1839\n",
      "1840 0\n",
      "1840\n",
      "1841 0\n",
      "1841\n",
      "1842 0\n",
      "1842\n",
      "1843 0\n",
      "1843\n",
      "1844 0\n",
      "1844\n",
      "1845 0\n",
      "1845\n",
      "1846 0\n",
      "1846\n",
      "1847 0\n",
      "1847\n",
      "1848 0\n",
      "1848\n",
      "1849 0\n",
      "1849\n",
      "1850 0\n",
      "1850\n",
      "1851 0\n",
      "1851\n",
      "1852 0\n",
      "1852\n",
      "1853 0\n",
      "1853\n",
      "1854 0\n",
      "1854\n",
      "1855 0\n",
      "1855\n",
      "1856 0\n",
      "1856\n",
      "1857 0\n",
      "1857\n",
      "1858 0\n",
      "1858\n",
      "1859 0\n",
      "1859\n",
      "1860 0\n",
      "1860\n",
      "1861 0\n",
      "1861\n",
      "1862 0\n",
      "1862\n",
      "1863 0\n",
      "1863\n",
      "1864 0\n",
      "1864\n",
      "1865 0\n",
      "1865\n",
      "1866 0\n",
      "1866\n",
      "1867 0\n",
      "1867\n",
      "1868 0\n",
      "1868\n",
      "1869 0\n",
      "1869\n",
      "1870 0\n",
      "1870\n",
      "1871 0\n",
      "1871\n",
      "1872 0\n",
      "1872\n",
      "1873 0\n",
      "1873\n",
      "1874 0\n",
      "1874\n",
      "1875 0\n",
      "1875\n",
      "1876 0\n",
      "1876\n",
      "1877 0\n",
      "1877\n",
      "1878 0\n",
      "1878\n",
      "1879 0\n",
      "1879\n",
      "1880 0\n",
      "1880\n",
      "1881 0\n",
      "1881\n",
      "1882 0\n",
      "1882\n",
      "1883 0\n",
      "1883\n",
      "1884 0\n",
      "1884\n",
      "1885 0\n",
      "1885\n",
      "1886 0\n",
      "1886\n",
      "1887 0\n",
      "1887\n",
      "1888 0\n",
      "1888\n",
      "1889 0\n",
      "1889\n",
      "1890 0\n",
      "1890\n",
      "1891 0\n",
      "1891\n",
      "1892 0\n",
      "1892\n",
      "1893 0\n",
      "1893\n",
      "1894 0\n",
      "1894\n",
      "1895 0\n",
      "1895\n",
      "1896 0\n",
      "1896\n",
      "1897 0\n",
      "1897\n",
      "1898 0\n",
      "1898\n",
      "1899 0\n",
      "1899\n",
      "1900 0\n",
      "1900\n",
      "1901 0\n",
      "1901\n",
      "1902 0\n",
      "1902\n",
      "1903 0\n",
      "1903\n",
      "1904 0\n",
      "1904\n",
      "1905 0\n",
      "1905\n",
      "1906 0\n",
      "1906\n",
      "1907 0\n",
      "1907\n",
      "1908 0\n",
      "1908\n",
      "1909 0\n",
      "1909\n",
      "1910 0\n",
      "1910\n",
      "1911 0\n",
      "1911\n",
      "1912 0\n",
      "1912\n",
      "1913 0\n",
      "1913\n",
      "1914 0\n",
      "1914\n",
      "1915 0\n",
      "1915\n",
      "1916 0\n",
      "1916\n",
      "1917 0\n",
      "1917\n",
      "1918 0\n",
      "1918\n",
      "1919 0\n",
      "1919\n",
      "1920 0\n",
      "1920\n",
      "1921 0\n",
      "1921\n",
      "1922 0\n",
      "1922\n",
      "1923 0\n",
      "1923\n",
      "1924 0\n",
      "1924\n",
      "1925 0\n",
      "1925\n",
      "1926 0\n",
      "1926\n",
      "1927 0\n",
      "1927\n",
      "1928 0\n",
      "1928\n",
      "1929 0\n",
      "1929\n",
      "1930 0\n",
      "1930\n",
      "1931 0\n",
      "1931\n",
      "1932 0\n",
      "1932\n",
      "1933 0\n",
      "1933\n",
      "1934 0\n",
      "1934\n",
      "1935 0\n",
      "1935\n",
      "1936 0\n",
      "1936\n",
      "1937 0\n",
      "1937\n",
      "1938 0\n",
      "1938\n",
      "1939 0\n",
      "1939\n",
      "1940 0\n",
      "1940\n",
      "1941 0\n",
      "1941\n",
      "1942 0\n",
      "1942\n",
      "1943 0\n",
      "1943\n",
      "1944 0\n",
      "1944\n",
      "1945 0\n",
      "1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946 0\n",
      "1946\n",
      "1947 0\n",
      "1947\n",
      "1948 0\n",
      "1948\n",
      "1949 0\n",
      "1949\n",
      "1950 0\n",
      "1950\n",
      "1951 0\n",
      "1951\n",
      "1952 0\n",
      "1952\n",
      "1953 0\n",
      "1953\n",
      "1954 0\n",
      "1954\n",
      "1955 0\n",
      "1955\n",
      "1956 0\n",
      "1956\n",
      "1957 0\n",
      "1957\n",
      "1958 0\n",
      "1958\n",
      "1959 0\n",
      "1959\n",
      "1960 0\n",
      "1960\n",
      "1961 0\n",
      "1961\n",
      "1962 0\n",
      "1962\n",
      "1963 0\n",
      "1963\n",
      "1964 0\n",
      "1964\n",
      "1965 0\n",
      "1965\n",
      "1966 0\n",
      "1966\n",
      "1967 0\n",
      "1967\n",
      "1968 0\n",
      "1968\n",
      "1969 0\n",
      "1969\n",
      "1970 0\n",
      "1970\n",
      "1971 0\n",
      "1971\n",
      "1972 0\n",
      "1972\n",
      "1973 0\n",
      "1973\n",
      "1974 0\n",
      "1974\n",
      "1975 0\n",
      "1975\n",
      "1976 0\n",
      "1976\n",
      "1977 0\n",
      "1977\n",
      "1978 0\n",
      "1978\n",
      "1979 0\n",
      "1979\n",
      "1980 0\n",
      "1980\n",
      "1981 0\n",
      "1981\n",
      "1982 0\n",
      "1982\n",
      "1983 0\n",
      "1983\n",
      "1984 0\n",
      "1984\n",
      "1985 0\n",
      "1985\n",
      "1986 0\n",
      "1986\n",
      "1987 0\n",
      "1987\n",
      "1988 0\n",
      "1988\n",
      "1989 0\n",
      "1989\n",
      "1990 0\n",
      "1990\n",
      "1991 0\n",
      "1991\n",
      "1992 0\n",
      "1992\n",
      "1993 0\n",
      "1993\n",
      "1994 0\n",
      "1994\n",
      "1995 0\n",
      "1995\n",
      "1996 0\n",
      "1996\n",
      "1997 0\n",
      "1997\n",
      "1998 0\n",
      "1998\n",
      "1999 0\n",
      "1999\n",
      "2000 0\n",
      "2000\n",
      "2001 0\n",
      "2001\n",
      "2002 0\n",
      "2002\n",
      "2003 0\n",
      "2003\n",
      "2004 0\n",
      "2004\n",
      "2005 0\n",
      "2005\n",
      "2006 0\n",
      "2006\n",
      "2007 0\n",
      "2007\n",
      "2008 0\n",
      "2008\n",
      "2009 0\n",
      "2009\n",
      "2010 0\n",
      "2010\n",
      "2011 0\n",
      "2011\n",
      "2012 0\n",
      "2012\n",
      "2013 0\n",
      "2013\n",
      "2014 0\n",
      "2014\n",
      "2015 0\n",
      "2015\n",
      "2016 0\n",
      "2016\n",
      "2017 0\n",
      "2017\n",
      "2018 0\n",
      "2018\n",
      "2019 0\n",
      "2019\n",
      "2020 0\n",
      "2020\n",
      "2021 0\n",
      "2021\n",
      "2022 0\n",
      "2022\n",
      "2023 0\n",
      "2023\n",
      "2024 0\n",
      "2024\n",
      "2025 0\n",
      "2025\n",
      "2026 0\n",
      "2026\n",
      "2027 0\n",
      "2027\n",
      "2028 0\n",
      "2028\n",
      "2029 0\n",
      "2029\n",
      "2030 0\n",
      "2030\n",
      "2031 0\n",
      "2031\n",
      "2032 0\n",
      "2032\n",
      "2033 0\n",
      "2033\n",
      "2034 0\n",
      "2034\n",
      "2035 0\n",
      "2035\n",
      "2036 0\n",
      "2036\n",
      "2037 0\n",
      "2037\n",
      "2038 0\n",
      "2038\n",
      "2039 0\n",
      "2039\n",
      "2040 0\n",
      "2040\n",
      "2041 0\n",
      "2041\n",
      "2042 0\n",
      "2042\n",
      "2043 0\n",
      "2043\n",
      "2044 0\n",
      "2044\n",
      "2045 0\n",
      "2045\n",
      "2046 0\n",
      "2046\n",
      "2047 0\n",
      "2047\n",
      "2048 0\n",
      "2048\n",
      "2049 0\n",
      "2049\n",
      "2050 0\n",
      "2050\n",
      "2051 0\n",
      "2051\n",
      "2052 0\n",
      "2052\n",
      "2053 0\n",
      "2053\n",
      "2054 0\n",
      "2054\n",
      "2055 0\n",
      "2055\n",
      "2056 0\n",
      "2056\n",
      "2057 0\n",
      "2057\n",
      "2058 0\n",
      "2058\n",
      "2059 0\n",
      "2059\n",
      "2060 0\n",
      "2060\n",
      "2061 0\n",
      "2061\n",
      "2062 0\n",
      "2062\n",
      "2063 0\n",
      "2063\n",
      "2064 0\n",
      "2064\n",
      "2065 0\n",
      "2065\n",
      "2066 0\n",
      "2066\n",
      "2067 0\n",
      "2067\n",
      "2068 0\n",
      "2068\n",
      "2069 0\n",
      "2069\n",
      "2070 0\n",
      "2070\n",
      "2071 0\n",
      "2071\n",
      "2072 0\n",
      "2072\n",
      "2073 0\n",
      "2073\n",
      "2074 0\n",
      "2074\n",
      "2075 0\n",
      "2075\n",
      "2076 0\n",
      "2076\n",
      "2077 0\n",
      "2077\n",
      "2078 0\n",
      "2078\n",
      "2079 0\n",
      "2079\n",
      "2080 0\n",
      "2080\n",
      "2081 0\n",
      "2081\n",
      "2082 0\n",
      "2082\n",
      "2083 0\n",
      "2083\n",
      "2084 0\n",
      "2084\n",
      "2085 0\n",
      "2085\n",
      "2086 0\n",
      "2086\n",
      "2087 0\n",
      "2087\n",
      "2088 0\n",
      "2088\n",
      "2089 0\n",
      "2089\n",
      "2090 0\n",
      "2090\n",
      "2091 0\n",
      "2091\n",
      "2092 0\n",
      "2092\n",
      "2093 0\n",
      "2093\n",
      "2094 0\n",
      "2094\n",
      "2095 0\n",
      "2095\n",
      "2096 0\n",
      "2096\n",
      "2097 0\n",
      "2097\n",
      "2098 0\n",
      "2098\n",
      "2099 0\n",
      "2099\n",
      "2100 0\n",
      "2100\n",
      "2101 0\n",
      "2101\n",
      "2102 0\n",
      "2102\n",
      "2103 0\n",
      "2103\n",
      "2104 0\n",
      "2104\n",
      "2105 0\n",
      "2105\n",
      "2106 0\n",
      "2106\n",
      "2107 0\n",
      "2107\n",
      "2108 0\n",
      "2108\n",
      "2109 0\n",
      "2109\n",
      "2110 0\n",
      "2110\n",
      "2111 0\n",
      "2111\n",
      "2112 0\n",
      "2112\n",
      "2113 0\n",
      "2113\n",
      "2114 0\n",
      "2114\n",
      "2115 0\n",
      "2115\n",
      "2116 0\n",
      "2116\n",
      "2117 0\n",
      "2117\n",
      "2118 0\n",
      "2118\n",
      "2119 0\n",
      "2119\n",
      "2120 0\n",
      "2120\n",
      "2121 0\n",
      "2121\n",
      "2122 0\n",
      "2122\n",
      "2123 0\n",
      "2123\n",
      "2124 0\n",
      "2124\n",
      "2125 0\n",
      "2125\n",
      "2126 0\n",
      "2126\n",
      "2127 0\n",
      "2127\n",
      "2128 0\n",
      "2128\n",
      "2129 0\n",
      "2129\n",
      "2130 0\n",
      "2130\n",
      "2131 0\n",
      "2131\n",
      "2132 0\n",
      "2132\n",
      "2133 0\n",
      "2133\n",
      "2134 0\n",
      "2134\n",
      "2135 0\n",
      "2135\n",
      "2136 0\n",
      "2136\n",
      "2137 0\n",
      "2137\n",
      "2138 0\n",
      "2138\n",
      "2139 0\n",
      "2139\n",
      "2140 0\n",
      "2140\n",
      "2141 0\n",
      "2141\n",
      "2142 0\n",
      "2142\n",
      "2143 0\n",
      "2143\n",
      "2144 0\n",
      "2144\n",
      "2145 0\n",
      "2145\n",
      "2146 0\n",
      "2146\n",
      "2147 0\n",
      "2147\n",
      "2148 0\n",
      "2148\n",
      "2149 0\n",
      "2149\n",
      "2150 0\n",
      "2150\n",
      "2151 0\n",
      "2151\n",
      "2152 0\n",
      "2152\n",
      "2153 0\n",
      "2153\n",
      "2154 0\n",
      "2154\n",
      "2155 0\n",
      "2155\n",
      "2156 0\n",
      "2156\n",
      "2157 0\n",
      "2157\n",
      "2158 0\n",
      "2158\n",
      "2159 0\n",
      "2159\n",
      "2160 0\n",
      "2160\n",
      "2161 0\n",
      "2161\n",
      "2162 0\n",
      "2162\n",
      "2163 0\n",
      "2163\n",
      "2164 0\n",
      "2164\n",
      "2165 0\n",
      "2165\n",
      "2166 0\n",
      "2166\n",
      "2167 0\n",
      "2167\n",
      "2168 0\n",
      "2168\n",
      "2169 0\n",
      "2169\n",
      "2170 0\n",
      "2170\n",
      "2171 0\n",
      "2171\n",
      "2172 0\n",
      "2172\n",
      "2173 0\n",
      "2173\n",
      "2174 0\n",
      "2174\n",
      "2175 0\n",
      "2175\n",
      "2176 0\n",
      "2176\n",
      "2177 0\n",
      "2177\n",
      "2178 0\n",
      "2178\n",
      "2179 0\n",
      "2179\n",
      "2180 0\n",
      "2180\n",
      "2181 0\n",
      "2181\n",
      "2182 0\n",
      "2182\n",
      "2183 0\n",
      "2183\n",
      "2184 0\n",
      "2184\n",
      "2185 0\n",
      "2185\n",
      "2186 0\n",
      "2186\n",
      "2187 0\n",
      "2187\n",
      "2188 0\n",
      "2188\n",
      "2189 0\n",
      "2189\n",
      "2190 0\n",
      "2190\n",
      "2191 0\n",
      "2191\n",
      "2192 0\n",
      "2192\n",
      "2193 0\n",
      "2193\n",
      "2194 0\n",
      "2194\n",
      "2195 0\n",
      "2195\n",
      "2196 0\n",
      "2196\n",
      "2197 0\n",
      "2197\n",
      "2198 0\n",
      "2198\n",
      "2199 0\n",
      "2199\n",
      "2200 0\n",
      "2200\n",
      "2201 0\n",
      "2201\n",
      "2202 0\n",
      "2202\n",
      "2203 0\n",
      "2203\n",
      "2204 0\n",
      "2204\n",
      "2205 0\n",
      "2205\n",
      "2206 0\n",
      "2206\n",
      "2207 0\n",
      "2207\n",
      "2208 0\n",
      "2208\n",
      "2209 0\n",
      "2209\n",
      "2210 0\n",
      "2210\n",
      "2211 0\n",
      "2211\n",
      "2212 0\n",
      "2212\n",
      "2213 0\n",
      "2213\n",
      "2214 0\n",
      "2214\n",
      "2215 0\n",
      "2215\n",
      "2216 0\n",
      "2216\n",
      "2217 0\n",
      "2217\n",
      "2218 0\n",
      "2218\n",
      "2219 0\n",
      "2219\n",
      "2220 0\n",
      "2220\n",
      "2221 0\n",
      "2221\n",
      "2222 0\n",
      "2222\n",
      "2223 0\n",
      "2223\n",
      "2224 0\n",
      "2224\n",
      "2225 0\n",
      "2225\n",
      "2226 0\n",
      "2226\n",
      "2227 0\n",
      "2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2228 0\n",
      "2228\n",
      "2229 0\n",
      "2229\n",
      "2230 0\n",
      "2230\n",
      "2231 0\n",
      "2231\n",
      "2232 0\n",
      "2232\n",
      "2233 0\n",
      "2233\n",
      "2234 0\n",
      "2234\n",
      "2235 0\n",
      "2235\n",
      "2236 0\n",
      "2236\n",
      "2237 0\n",
      "2237\n",
      "2238 0\n",
      "2238\n",
      "2239 0\n",
      "2239\n",
      "2240 0\n",
      "2240\n",
      "2241 0\n",
      "2241\n",
      "2242 0\n",
      "2242\n",
      "2243 0\n",
      "2243\n",
      "2244 0\n",
      "2244\n",
      "2245 0\n",
      "2245\n",
      "2246 0\n",
      "2246\n",
      "2247 0\n",
      "2247\n",
      "2248 0\n",
      "2248\n",
      "2249 0\n",
      "2249\n",
      "2250 0\n",
      "2250\n",
      "2251 0\n",
      "2251\n",
      "2252 0\n",
      "2252\n",
      "2253 0\n",
      "2253\n",
      "2254 0\n",
      "2254\n",
      "2255 0\n",
      "2255\n",
      "2256 0\n",
      "2256\n",
      "2257 0\n",
      "2257\n",
      "2258 0\n",
      "2258\n",
      "2259 0\n",
      "2259\n",
      "2260 0\n",
      "2260\n",
      "2261 0\n",
      "2261\n",
      "2262 0\n",
      "2262\n",
      "2263 0\n",
      "2263\n",
      "2264 0\n",
      "2264\n",
      "2265 0\n",
      "2265\n",
      "2266 0\n",
      "2266\n",
      "2267 0\n",
      "2267\n",
      "2268 0\n",
      "2268\n",
      "2269 0\n",
      "2269\n",
      "2270 0\n",
      "2270\n",
      "2271 0\n",
      "2271\n",
      "2272 0\n",
      "2272\n",
      "2273 0\n",
      "2273\n",
      "2274 0\n",
      "2274\n",
      "2275 0\n",
      "2275\n",
      "2276 0\n",
      "2276\n",
      "2277 0\n",
      "2277\n",
      "2278 0\n",
      "2278\n",
      "2279 0\n",
      "2279\n",
      "2280 0\n",
      "2280\n",
      "2281 0\n",
      "2281\n",
      "2282 0\n",
      "2282\n",
      "2283 0\n",
      "2283\n",
      "2284 0\n",
      "2284\n",
      "2285 0\n",
      "2285\n",
      "2286 0\n",
      "2286\n",
      "2287 0\n",
      "2287\n",
      "2288 0\n",
      "2288\n",
      "2289 0\n",
      "2289\n",
      "2290 0\n",
      "2290\n",
      "2291 0\n",
      "2291\n",
      "2292 0\n",
      "2292\n",
      "2293 0\n",
      "2293\n",
      "2294 0\n",
      "2294\n",
      "2295 0\n",
      "2295\n",
      "2296 0\n",
      "2296\n",
      "2297 0\n",
      "2297\n",
      "2298 0\n",
      "2298\n",
      "2299 0\n",
      "2299\n",
      "2300 0\n",
      "2300\n",
      "2301 0\n",
      "2301\n",
      "2302 0\n",
      "2302\n",
      "2303 0\n",
      "2303\n",
      "2304 0\n",
      "2304\n",
      "2305 0\n",
      "2305\n",
      "2306 0\n",
      "2306\n",
      "2307 0\n",
      "2307\n",
      "2308 0\n",
      "2308\n",
      "2309 0\n",
      "2309\n",
      "2310 0\n",
      "2310\n",
      "2311 0\n",
      "2311\n",
      "2312 0\n",
      "2312\n",
      "2313 0\n",
      "2313\n",
      "2314 0\n",
      "2314\n",
      "2315 0\n",
      "2315\n",
      "2316 0\n",
      "2316\n",
      "2317 0\n",
      "2317\n",
      "2318 0\n",
      "2318\n",
      "2319 0\n",
      "2319\n",
      "2320 0\n",
      "2320\n",
      "2321 0\n",
      "2321\n",
      "2322 0\n",
      "2322\n",
      "2323 0\n",
      "2323\n",
      "2324 0\n",
      "2324\n",
      "2325 0\n",
      "2325\n",
      "2326 0\n",
      "2326\n",
      "2327 0\n",
      "2327\n",
      "2328 0\n",
      "2328\n",
      "2329 0\n",
      "2329\n",
      "2330 0\n",
      "2330\n",
      "2331 0\n",
      "2331\n",
      "2332 0\n",
      "2332\n",
      "2333 0\n",
      "2333\n",
      "2334 0\n",
      "2334\n",
      "2335 0\n",
      "2335\n",
      "2336 0\n",
      "2336\n",
      "2337 0\n",
      "2337\n",
      "2338 0\n",
      "2338\n",
      "2339 0\n",
      "2339\n",
      "2340 0\n",
      "2340\n",
      "2341 0\n",
      "2341\n",
      "2342 0\n",
      "2342\n",
      "2343 0\n",
      "2343\n",
      "2344 0\n",
      "2344\n",
      "2345 0\n",
      "2345\n",
      "2346 0\n",
      "2346\n",
      "2347 0\n",
      "2347\n",
      "2348 0\n",
      "2348\n",
      "2349 0\n",
      "2349\n",
      "2350 0\n",
      "2350\n",
      "2351 0\n",
      "2351\n",
      "2352 0\n",
      "2352\n",
      "2353 0\n",
      "2353\n",
      "2354 0\n",
      "2354\n",
      "2355 0\n",
      "2355\n",
      "2356 0\n",
      "2356\n",
      "2357 0\n",
      "2357\n",
      "2358 0\n",
      "2358\n",
      "2359 0\n",
      "2359\n",
      "2360 0\n",
      "2360\n",
      "2361 0\n",
      "2361\n",
      "2362 0\n",
      "2362\n",
      "2363 0\n",
      "2363\n",
      "2364 0\n",
      "2364\n",
      "2365 0\n",
      "2365\n",
      "2366 0\n",
      "2366\n",
      "2367 0\n",
      "2367\n",
      "2368 0\n",
      "2368\n",
      "2369 0\n",
      "2369\n",
      "2370 0\n",
      "2370\n",
      "2371 0\n",
      "2371\n",
      "2372 0\n",
      "2372\n",
      "2373 0\n",
      "2373\n",
      "2374 0\n",
      "2374\n",
      "2375 0\n",
      "2375\n",
      "2376 0\n",
      "2376\n",
      "2377 0\n",
      "2377\n",
      "2378 0\n",
      "2378\n",
      "2379 0\n",
      "2379\n",
      "2380 0\n",
      "2380\n",
      "2381 0\n",
      "2381\n",
      "2382 0\n",
      "2382\n",
      "2383 0\n",
      "2383\n",
      "2384 0\n",
      "2384\n",
      "2385 0\n",
      "2385\n",
      "2386 0\n",
      "2386\n",
      "2387 0\n",
      "2387\n",
      "2388 0\n",
      "2388\n",
      "2389 0\n",
      "2389\n",
      "2390 0\n",
      "2390\n",
      "2391 0\n",
      "2391\n",
      "2392 0\n",
      "2392\n",
      "2393 0\n",
      "2393\n",
      "2394 0\n",
      "2394\n",
      "2395 0\n",
      "2395\n",
      "2396 0\n",
      "2396\n",
      "2397 0\n",
      "2397\n",
      "2398 0\n",
      "2398\n",
      "2399 0\n",
      "2399\n",
      "2400 0\n",
      "2400\n",
      "2401 0\n",
      "2401\n",
      "2402 0\n",
      "2402\n",
      "2403 0\n",
      "2403\n",
      "2404 0\n",
      "2404\n",
      "2405 0\n",
      "2405\n",
      "2406 0\n",
      "2406\n",
      "2407 0\n",
      "2407\n",
      "2408 0\n",
      "2408\n",
      "2409 0\n",
      "2409\n",
      "2410 0\n",
      "2410\n",
      "2411 0\n",
      "2411\n",
      "2412 0\n",
      "2412\n",
      "2413 0\n",
      "2413\n",
      "2414 0\n",
      "2414\n",
      "2415 0\n",
      "2415\n",
      "2416 0\n",
      "2416\n",
      "2417 0\n",
      "2417\n",
      "2418 0\n",
      "2418\n",
      "2419 0\n",
      "2419\n",
      "2420 0\n",
      "2420\n",
      "2421 0\n",
      "2421\n",
      "2422 0\n",
      "2422\n",
      "2423 0\n",
      "2423\n",
      "2424 0\n",
      "2424\n",
      "2425 0\n",
      "2425\n",
      "2426 0\n",
      "2426\n",
      "2427 0\n",
      "2427\n",
      "2428 0\n",
      "2428\n",
      "2429 0\n",
      "2429\n",
      "2430 0\n",
      "2430\n",
      "2431 0\n",
      "2431\n",
      "2432 0\n",
      "2432\n",
      "2433 0\n",
      "2433\n",
      "2434 0\n",
      "2434\n",
      "2435 0\n",
      "2435\n",
      "2436 0\n",
      "2436\n",
      "2437 0\n",
      "2437\n",
      "2438 0\n",
      "2438\n",
      "2439 0\n",
      "2439\n",
      "2440 0\n",
      "2440\n",
      "2441 0\n",
      "2441\n",
      "2442 0\n",
      "2442\n",
      "2443 0\n",
      "2443\n",
      "2444 0\n",
      "2444\n",
      "2445 0\n",
      "2445\n",
      "2446 0\n",
      "2446\n",
      "2447 0\n",
      "2447\n",
      "2448 0\n",
      "2448\n",
      "2449 0\n",
      "2449\n",
      "2450 0\n",
      "2450\n",
      "2451 0\n",
      "2451\n",
      "2452 0\n",
      "2452\n",
      "2453 0\n",
      "2453\n",
      "2454 0\n",
      "2454\n",
      "2455 0\n",
      "2455\n",
      "2456 0\n",
      "2456\n",
      "2457 0\n",
      "2457\n",
      "2458 0\n",
      "2458\n",
      "2459 0\n",
      "2459\n",
      "2460 0\n",
      "2460\n",
      "2461 0\n",
      "2461\n",
      "2462 0\n",
      "2462\n",
      "2463 0\n",
      "2463\n",
      "2464 0\n",
      "2464\n",
      "2465 0\n",
      "2465\n",
      "2466 0\n",
      "2466\n",
      "2467 0\n",
      "2467\n",
      "2468 0\n",
      "2468\n",
      "2469 0\n",
      "2469\n",
      "2470 0\n",
      "2470\n",
      "2471 0\n",
      "2471\n",
      "2472 0\n",
      "2472\n",
      "2473 0\n",
      "2473\n",
      "2474 0\n",
      "2474\n",
      "2475 0\n",
      "2475\n",
      "2476 0\n",
      "2476\n",
      "2477 0\n",
      "2477\n",
      "2478 0\n",
      "2478\n",
      "2479 0\n",
      "2479\n",
      "2480 0\n",
      "2480\n",
      "2481 0\n",
      "2481\n",
      "2482 0\n",
      "2482\n",
      "2483 0\n",
      "2483\n",
      "2484 0\n",
      "2484\n",
      "2485 0\n",
      "2485\n",
      "2486 0\n",
      "2486\n",
      "2487 0\n",
      "2487\n",
      "2488 0\n",
      "2488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489 0\n",
      "2489\n",
      "2490 0\n",
      "2490\n",
      "2491 0\n",
      "2491\n",
      "2492 0\n",
      "2492\n",
      "2493 0\n",
      "2493\n",
      "2494 0\n",
      "2494\n",
      "2495 0\n",
      "2495\n",
      "2496 0\n",
      "2496\n",
      "2497 0\n",
      "2497\n",
      "2498 0\n",
      "2498\n",
      "2499 0\n",
      "2499\n",
      "2500 0\n",
      "2500\n",
      "2501 0\n",
      "2501\n",
      "2502 0\n",
      "2502\n",
      "2503 0\n",
      "2503\n",
      "2504 0\n",
      "2504\n",
      "2505 0\n",
      "2505\n",
      "2506 0\n",
      "2506\n",
      "2507 0\n",
      "2507\n",
      "2508 0\n",
      "2508\n",
      "2509 0\n",
      "2509\n",
      "2510 0\n",
      "2510\n",
      "2511 0\n",
      "2511\n",
      "2512 0\n",
      "2512\n",
      "2513 0\n",
      "2513\n",
      "2514 0\n",
      "2514\n",
      "2515 0\n",
      "2515\n",
      "2516 0\n",
      "2516\n",
      "2517 0\n",
      "2517\n",
      "2518 0\n",
      "2518\n",
      "2519 0\n",
      "2519\n",
      "2520 0\n",
      "2520\n",
      "2521 0\n",
      "2521\n",
      "2522 0\n",
      "2522\n",
      "2523 0\n",
      "2523\n",
      "2524 0\n",
      "2524\n",
      "2525 0\n",
      "2525\n",
      "2526 0\n",
      "2526\n",
      "2527 0\n",
      "2527\n",
      "2528 0\n",
      "2528\n",
      "2529 0\n",
      "2529\n",
      "2530 0\n",
      "2530\n",
      "2531 0\n",
      "2531\n",
      "2532 0\n",
      "2532\n",
      "2533 0\n",
      "2533\n",
      "2534 0\n",
      "2534\n",
      "2535 0\n",
      "2535\n",
      "2536 0\n",
      "2536\n",
      "2537 0\n",
      "2537\n",
      "2538 0\n",
      "2538\n",
      "2539 0\n",
      "2539\n",
      "2540 0\n",
      "2540\n",
      "2541 0\n",
      "2541\n",
      "2542 0\n",
      "2542\n",
      "2543 0\n",
      "2543\n",
      "2544 0\n",
      "2544\n",
      "2545 0\n",
      "2545\n",
      "2546 0\n",
      "2546\n",
      "2547 0\n",
      "2547\n",
      "2548 0\n",
      "2548\n",
      "2549 0\n",
      "2549\n",
      "2550 0\n",
      "2550\n",
      "2551 0\n",
      "2551\n",
      "2552 0\n",
      "2552\n",
      "2553 0\n",
      "2553\n",
      "2554 0\n",
      "2554\n",
      "2555 0\n",
      "2555\n",
      "2556 0\n",
      "2556\n",
      "2557 0\n",
      "2557\n",
      "2558 0\n",
      "2558\n",
      "2559 0\n",
      "2559\n",
      "2560 0\n",
      "2560\n",
      "2561 0\n",
      "2561\n",
      "2562 0\n",
      "2562\n",
      "2563 0\n",
      "2563\n",
      "2564 0\n",
      "2564\n",
      "2565 0\n",
      "2565\n",
      "2566 0\n",
      "2566\n",
      "2567 0\n",
      "2567\n",
      "2568 0\n",
      "2568\n",
      "2569 0\n",
      "2569\n",
      "2570 0\n",
      "2570\n",
      "2571 0\n",
      "2571\n",
      "2572 0\n",
      "2572\n",
      "2573 0\n",
      "2573\n",
      "2574 0\n",
      "2574\n",
      "2575 0\n",
      "2575\n",
      "2576 0\n",
      "2576\n",
      "2577 0\n",
      "2577\n",
      "2578 0\n",
      "2578\n",
      "2579 0\n",
      "2579\n",
      "2580 0\n",
      "2580\n",
      "2581 0\n",
      "2581\n",
      "2582 0\n",
      "2582\n",
      "2583 0\n",
      "2583\n",
      "2584 0\n",
      "2584\n",
      "2585 0\n",
      "2585\n",
      "2586 0\n",
      "2586\n",
      "2587 0\n",
      "2587\n",
      "2588 0\n",
      "2588\n",
      "2589 0\n",
      "2589\n",
      "2590 0\n",
      "2590\n",
      "2591 0\n",
      "2591\n",
      "2592 0\n",
      "2592\n",
      "2593 0\n",
      "2593\n",
      "2594 0\n",
      "2594\n",
      "2595 0\n",
      "2595\n",
      "2596 0\n",
      "2596\n",
      "2597 0\n",
      "2597\n",
      "2598 0\n",
      "2598\n",
      "2599 0\n",
      "2599\n",
      "2600 0\n",
      "2600\n",
      "2601 0\n",
      "2601\n",
      "2602 0\n",
      "2602\n",
      "2603 0\n",
      "2603\n",
      "2604 0\n",
      "2604\n",
      "2605 0\n",
      "2605\n",
      "2606 0\n",
      "2606\n",
      "2607 0\n",
      "2607\n",
      "2608 0\n",
      "2608\n",
      "2609 0\n",
      "2609\n",
      "2610 0\n",
      "2610\n",
      "2611 0\n",
      "2611\n",
      "2612 0\n",
      "2612\n",
      "2613 0\n",
      "2613\n",
      "2614 0\n",
      "2614\n",
      "2615 0\n",
      "2615\n",
      "2616 0\n",
      "2616\n",
      "2617 0\n",
      "2617\n",
      "2618 0\n",
      "2618\n",
      "2619 0\n",
      "2619\n",
      "2620 0\n",
      "2620\n",
      "2621 0\n",
      "2621\n",
      "2622 0\n",
      "2622\n",
      "2623 0\n",
      "2623\n",
      "2624 0\n",
      "2624\n",
      "2625 0\n",
      "2625\n",
      "2626 0\n",
      "2626\n",
      "2627 0\n",
      "2627\n",
      "2628 0\n",
      "2628\n",
      "2629 0\n",
      "2629\n",
      "2630 0\n",
      "2630\n",
      "2631 0\n",
      "2631\n",
      "2632 0\n",
      "2632\n",
      "2633 0\n",
      "2633\n",
      "2634 0\n",
      "2634\n",
      "2635 0\n",
      "2635\n",
      "2636 0\n",
      "2636\n",
      "2637 0\n",
      "2637\n",
      "2638 0\n",
      "2638\n",
      "2639 0\n",
      "2639\n",
      "2640 0\n",
      "2640\n",
      "2641 0\n",
      "2641\n",
      "2642 0\n",
      "2642\n",
      "2643 0\n",
      "2643\n",
      "2644 0\n",
      "2644\n",
      "2645 0\n",
      "2645\n",
      "2646 0\n",
      "2646\n",
      "2647 0\n",
      "2647\n",
      "2648 0\n",
      "2648\n",
      "2649 0\n",
      "2649\n",
      "2650 0\n",
      "2650\n",
      "2651 0\n",
      "2651\n",
      "2652 0\n",
      "2652\n",
      "2653 0\n",
      "2653\n",
      "2654 0\n",
      "2654\n",
      "2655 0\n",
      "2655\n",
      "2656 0\n",
      "2656\n",
      "2657 0\n",
      "2657\n",
      "2658 0\n",
      "2658\n",
      "2659 0\n",
      "2659\n",
      "2660 0\n",
      "2660\n",
      "2661 0\n",
      "2661\n",
      "2662 0\n",
      "2662\n",
      "2663 0\n",
      "2663\n",
      "2664 0\n",
      "2664\n",
      "2665 0\n",
      "2665\n",
      "2666 0\n",
      "2666\n",
      "2667 0\n",
      "2667\n",
      "2668 0\n",
      "2668\n",
      "2669 0\n",
      "2669\n",
      "2670 0\n",
      "2670\n",
      "2671 0\n",
      "2671\n",
      "2672 0\n",
      "2672\n",
      "2673 0\n",
      "2673\n",
      "2674 0\n",
      "2674\n",
      "2675 0\n",
      "2675\n",
      "2676 0\n",
      "2676\n",
      "2677 0\n",
      "2677\n",
      "2678 0\n",
      "2678\n",
      "2679 0\n",
      "2679\n",
      "2680 0\n",
      "2680\n",
      "2681 0\n",
      "2681\n",
      "2682 0\n",
      "2682\n",
      "2683 0\n",
      "2683\n",
      "2684 0\n",
      "2684\n",
      "2685 0\n",
      "2685\n",
      "2686 0\n",
      "2686\n",
      "2687 0\n",
      "2687\n",
      "2688 0\n",
      "2688\n",
      "2689 0\n",
      "2689\n",
      "2690 0\n",
      "2690\n",
      "2691 0\n",
      "2691\n",
      "2692 0\n",
      "2692\n",
      "2693 0\n",
      "2693\n",
      "2694 0\n",
      "2694\n",
      "2695 0\n",
      "2695\n",
      "2696 0\n",
      "2696\n",
      "2697 0\n",
      "2697\n",
      "2698 0\n",
      "2698\n",
      "2699 0\n",
      "2699\n",
      "2700 0\n",
      "2700\n",
      "2701 0\n",
      "2701\n",
      "2702 0\n",
      "2702\n",
      "2703 0\n",
      "2703\n",
      "2704 0\n",
      "2704\n",
      "2705 0\n",
      "2705\n",
      "2706 0\n",
      "2706\n",
      "2707 0\n",
      "2707\n",
      "2708 0\n",
      "2708\n",
      "2709 0\n",
      "2709\n",
      "2710 0\n",
      "2710\n",
      "2711 0\n",
      "2711\n",
      "2712 0\n",
      "2712\n",
      "2713 0\n",
      "2713\n",
      "2714 0\n",
      "2714\n",
      "2715 0\n",
      "2715\n",
      "2716 0\n",
      "2716\n",
      "2717 0\n",
      "2717\n",
      "2718 0\n",
      "2718\n",
      "2719 0\n",
      "2719\n",
      "2720 0\n",
      "2720\n",
      "2721 0\n",
      "2721\n",
      "2722 0\n",
      "2722\n",
      "2723 0\n",
      "2723\n",
      "2724 0\n",
      "2724\n",
      "2725 0\n",
      "2725\n",
      "2726 0\n",
      "2726\n",
      "2727 0\n",
      "2727\n",
      "2728 0\n",
      "2728\n",
      "2729 0\n",
      "2729\n",
      "2730 0\n",
      "2730\n",
      "2731 0\n",
      "2731\n",
      "2732 0\n",
      "2732\n",
      "2733 0\n",
      "2733\n",
      "2734 0\n",
      "2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2735 0\n",
      "2735\n",
      "2736 0\n",
      "2736\n",
      "2737 0\n",
      "2737\n",
      "2738 0\n",
      "2738\n",
      "2739 0\n",
      "2739\n",
      "2740 0\n",
      "2740\n",
      "2741 0\n",
      "2741\n",
      "2742 0\n",
      "2742\n",
      "2743 0\n",
      "2743\n",
      "2744 0\n",
      "2744\n",
      "2745 0\n",
      "2745\n",
      "2746 0\n",
      "2746\n",
      "2747 0\n",
      "2747\n",
      "2748 0\n",
      "2748\n",
      "2749 0\n",
      "2749\n",
      "2750 0\n",
      "2750\n",
      "2751 0\n",
      "2751\n",
      "2752 0\n",
      "2752\n",
      "2753 0\n",
      "2753\n",
      "2754 0\n",
      "2754\n",
      "2755 0\n",
      "2755\n",
      "2756 0\n",
      "2756\n",
      "2757 0\n",
      "2757\n",
      "2758 0\n",
      "2758\n",
      "2759 0\n",
      "2759\n",
      "2760 0\n",
      "2760\n",
      "2761 0\n",
      "2761\n",
      "2762 0\n",
      "2762\n",
      "2763 0\n",
      "2763\n",
      "2764 0\n",
      "2764\n",
      "2765 0\n",
      "2765\n",
      "2766 0\n",
      "2766\n",
      "2767 0\n",
      "2767\n",
      "2768 0\n",
      "2768\n",
      "2769 0\n",
      "2769\n",
      "2770 0\n",
      "2770\n",
      "2771 0\n",
      "2771\n",
      "2772 0\n",
      "2772\n",
      "2773 0\n",
      "2773\n",
      "2774 0\n",
      "2774\n",
      "2775 0\n",
      "2775\n",
      "2776 0\n",
      "2776\n",
      "2777 0\n",
      "2777\n",
      "2778 0\n",
      "2778\n",
      "2779 0\n",
      "2779\n",
      "2780 0\n",
      "2780\n",
      "2781 0\n",
      "2781\n",
      "2782 0\n",
      "2782\n",
      "2783 0\n",
      "2783\n",
      "2784 0\n",
      "2784\n",
      "2785 0\n",
      "2785\n",
      "2786 0\n",
      "2786\n",
      "2787 0\n",
      "2787\n",
      "2788 0\n",
      "2788\n",
      "2789 0\n",
      "2789\n",
      "2790 0\n",
      "2790\n",
      "2791 0\n",
      "2791\n",
      "2792 0\n",
      "2792\n",
      "2793 0\n",
      "2793\n",
      "2794 0\n",
      "2794\n",
      "2795 0\n",
      "2795\n",
      "2796 0\n",
      "2796\n",
      "2797 0\n",
      "2797\n",
      "2798 0\n",
      "2798\n",
      "2799 0\n",
      "2799\n",
      "2800 0\n",
      "2800\n",
      "2801 0\n",
      "2801\n",
      "2802 0\n",
      "2802\n",
      "2803 0\n",
      "2803\n",
      "2804 0\n",
      "2804\n",
      "2805 0\n",
      "2805\n",
      "2806 0\n",
      "2806\n",
      "2807 0\n",
      "2807\n",
      "2808 0\n",
      "2808\n",
      "2809 0\n",
      "2809\n",
      "2810 0\n",
      "2810\n",
      "2811 0\n",
      "2811\n",
      "2812 0\n",
      "2812\n",
      "2813 0\n",
      "2813\n",
      "2814 0\n",
      "2814\n",
      "2815 0\n",
      "2815\n",
      "2816 0\n",
      "2816\n",
      "2817 0\n",
      "2817\n",
      "2818 0\n",
      "2818\n",
      "2819 0\n",
      "2819\n",
      "2820 0\n",
      "2820\n",
      "2821 0\n",
      "2821\n",
      "2822 0\n",
      "2822\n",
      "2823 0\n",
      "2823\n",
      "2824 0\n",
      "2824\n",
      "2825 0\n",
      "2825\n",
      "2826 0\n",
      "2826\n",
      "2827 0\n",
      "2827\n",
      "2828 0\n",
      "2828\n",
      "2829 0\n",
      "2829\n",
      "2830 0\n",
      "2830\n",
      "2831 0\n",
      "2831\n",
      "2832 0\n",
      "2832\n",
      "2833 0\n",
      "2833\n",
      "2834 0\n",
      "2834\n",
      "2835 0\n",
      "2835\n",
      "2836 0\n",
      "2836\n",
      "2837 0\n",
      "2837\n",
      "2838 0\n",
      "2838\n",
      "2839 0\n",
      "2839\n",
      "2840 0\n",
      "2840\n",
      "2841 0\n",
      "2841\n",
      "2842 0\n",
      "2842\n",
      "2843 0\n",
      "2843\n",
      "2844 0\n",
      "2844\n",
      "2845 0\n",
      "2845\n",
      "2846 0\n",
      "2846\n",
      "2847 0\n",
      "2847\n",
      "2848 0\n",
      "2848\n",
      "2849 0\n",
      "2849\n",
      "2850 0\n",
      "2850\n",
      "2851 0\n",
      "2851\n",
      "2852 0\n",
      "2852\n",
      "2853 0\n",
      "2853\n",
      "2854 0\n",
      "2854\n",
      "2855 0\n",
      "2855\n",
      "2856 0\n",
      "2856\n",
      "2857 0\n",
      "2857\n",
      "2858 0\n",
      "2858\n",
      "2859 0\n",
      "2859\n",
      "2860 0\n",
      "2860\n",
      "2861 0\n",
      "2861\n",
      "2862 0\n",
      "2862\n",
      "2863 0\n",
      "2863\n",
      "2864 0\n",
      "2864\n",
      "2865 0\n",
      "2865\n",
      "2866 0\n",
      "2866\n",
      "2867 0\n",
      "2867\n",
      "2868 0\n",
      "2868\n",
      "2869 0\n",
      "2869\n",
      "2870 0\n",
      "2870\n",
      "2871 0\n",
      "2871\n",
      "2872 0\n",
      "2872\n",
      "2873 0\n",
      "2873\n",
      "2874 0\n",
      "2874\n",
      "2875 0\n",
      "2875\n",
      "2876 0\n",
      "2876\n",
      "2877 0\n",
      "2877\n",
      "2878 0\n",
      "2878\n",
      "2879 0\n",
      "2879\n",
      "2880 0\n",
      "2880\n",
      "2881 0\n",
      "2881\n",
      "2882 0\n",
      "2882\n",
      "2883 0\n",
      "2883\n",
      "2884 0\n",
      "2884\n",
      "2885 0\n",
      "2885\n",
      "2886 0\n",
      "2886\n",
      "2887 0\n",
      "2887\n",
      "2888 0\n",
      "2888\n",
      "2889 0\n",
      "2889\n",
      "2890 0\n",
      "2890\n",
      "2891 0\n",
      "2891\n",
      "2892 0\n",
      "2892\n",
      "2893 0\n",
      "2893\n",
      "2894 0\n",
      "2894\n",
      "2895 0\n",
      "2895\n",
      "2896 0\n",
      "2896\n",
      "2897 0\n",
      "2897\n",
      "2898 0\n",
      "2898\n",
      "2899 0\n",
      "2899\n",
      "2900 0\n",
      "2900\n",
      "2901 0\n",
      "2901\n",
      "2902 0\n",
      "2902\n",
      "2903 0\n",
      "2903\n",
      "2904 0\n",
      "2904\n",
      "2905 0\n",
      "2905\n",
      "2906 0\n",
      "2906\n",
      "2907 0\n",
      "2907\n",
      "2908 0\n",
      "2908\n",
      "2909 0\n",
      "2909\n",
      "2910 0\n",
      "2910\n",
      "2911 0\n",
      "2911\n",
      "2912 0\n",
      "2912\n",
      "2913 0\n",
      "2913\n",
      "2914 0\n",
      "2914\n",
      "2915 0\n",
      "2915\n",
      "2916 0\n",
      "2916\n",
      "2917 0\n",
      "2917\n",
      "2918 0\n",
      "2918\n",
      "2919 0\n",
      "2919\n",
      "2920 0\n",
      "2920\n",
      "2921 0\n",
      "2921\n",
      "2922 0\n",
      "2922\n",
      "2923 0\n",
      "2923\n",
      "2924 0\n",
      "2924\n",
      "2925 0\n",
      "2925\n",
      "2926 0\n",
      "2926\n",
      "2927 0\n",
      "2927\n",
      "2928 0\n",
      "2928\n",
      "2929 0\n",
      "2929\n",
      "2930 0\n",
      "2930\n",
      "2931 0\n",
      "2931\n",
      "2932 0\n",
      "2932\n",
      "2933 0\n",
      "2933\n",
      "2934 0\n",
      "2934\n",
      "2935 0\n",
      "2935\n",
      "2936 0\n",
      "2936\n",
      "2937 0\n",
      "2937\n",
      "2938 0\n",
      "2938\n",
      "2939 0\n",
      "2939\n",
      "2940 0\n",
      "2940\n",
      "2941 0\n",
      "2941\n",
      "2942 0\n",
      "2942\n",
      "2943 0\n",
      "2943\n",
      "2944 0\n",
      "2944\n",
      "2945 0\n",
      "2945\n",
      "2946 0\n",
      "2946\n",
      "2947 0\n",
      "2947\n",
      "2948 0\n",
      "2948\n",
      "2949 0\n",
      "2949\n",
      "2950 0\n",
      "2950\n",
      "2951 0\n",
      "2951\n",
      "2952 0\n",
      "2952\n",
      "2953 0\n",
      "2953\n",
      "2954 0\n",
      "2954\n",
      "2955 0\n",
      "2955\n",
      "2956 0\n",
      "2956\n",
      "2957 0\n",
      "2957\n",
      "2958 0\n",
      "2958\n",
      "2959 0\n",
      "2959\n",
      "2960 0\n",
      "2960\n",
      "2961 0\n",
      "2961\n",
      "2962 0\n",
      "2962\n",
      "2963 0\n",
      "2963\n",
      "2964 0\n",
      "2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2965 0\n",
      "2965\n",
      "2966 0\n",
      "2966\n",
      "2967 0\n",
      "2967\n",
      "2968 0\n",
      "2968\n",
      "2969 0\n",
      "2969\n",
      "2970 0\n",
      "2970\n",
      "2971 0\n",
      "2971\n",
      "2972 0\n",
      "2972\n",
      "2973 0\n",
      "2973\n",
      "2974 0\n",
      "2974\n",
      "2975 0\n",
      "2975\n",
      "2976 0\n",
      "2976\n",
      "2977 0\n",
      "2977\n",
      "2978 0\n",
      "2978\n",
      "2979 0\n",
      "2979\n",
      "2980 0\n",
      "2980\n",
      "2981 0\n",
      "2981\n",
      "2982 0\n",
      "2982\n",
      "2983 0\n",
      "2983\n",
      "2984 0\n",
      "2984\n",
      "2985 0\n",
      "2985\n",
      "2986 0\n",
      "2986\n",
      "2987 0\n",
      "2987\n",
      "2988 0\n",
      "2988\n",
      "2989 0\n",
      "2989\n",
      "2990 0\n",
      "2990\n",
      "2991 0\n",
      "2991\n",
      "2992 0\n",
      "2992\n",
      "2993 0\n",
      "2993\n",
      "2994 0\n",
      "2994\n",
      "2995 0\n",
      "2995\n",
      "2996 0\n",
      "2996\n",
      "2997 0\n",
      "2997\n",
      "2998 0\n",
      "2998\n",
      "2999 0\n",
      "2999\n",
      "3000 0\n",
      "3000\n",
      "3001 0\n",
      "3001\n",
      "3002 0\n",
      "3002\n",
      "3003 0\n",
      "3003\n",
      "3004 0\n",
      "3004\n",
      "3005 0\n",
      "3005\n",
      "3006 0\n",
      "3006\n",
      "3007 0\n",
      "3007\n",
      "3008 0\n",
      "3008\n",
      "3009 0\n",
      "3009\n",
      "3010 0\n",
      "3010\n",
      "3011 0\n",
      "3011\n",
      "3012 0\n",
      "3012\n",
      "3013 0\n",
      "3013\n",
      "3014 0\n",
      "3014\n",
      "3015 0\n",
      "3015\n",
      "3016 0\n",
      "3016\n",
      "3017 0\n",
      "3017\n",
      "3018 0\n",
      "3018\n",
      "3019 0\n",
      "3019\n",
      "3020 0\n",
      "3020\n",
      "3021 0\n",
      "3021\n",
      "3022 0\n",
      "3022\n",
      "3023 0\n",
      "3023\n",
      "3024 0\n",
      "3024\n",
      "3025 0\n",
      "3025\n",
      "3026 0\n",
      "3026\n",
      "3027 0\n",
      "3027\n",
      "3028 0\n",
      "3028\n",
      "3029 0\n",
      "3029\n",
      "3030 0\n",
      "3030\n",
      "3031 0\n",
      "3031\n",
      "3032 0\n",
      "3032\n",
      "3033 0\n",
      "3033\n",
      "3034 0\n",
      "3034\n",
      "3035 0\n",
      "3035\n",
      "3036 0\n",
      "3036\n",
      "3037 0\n",
      "3037\n",
      "3038 0\n",
      "3038\n",
      "3039 0\n",
      "3039\n",
      "3040 0\n",
      "3040\n",
      "3041 0\n",
      "3041\n",
      "3042 0\n",
      "3042\n",
      "3043 0\n",
      "3043\n",
      "3044 0\n",
      "3044\n",
      "3045 0\n",
      "3045\n",
      "3046 0\n",
      "3046\n",
      "3047 0\n",
      "3047\n",
      "3048 0\n",
      "3048\n",
      "3049 0\n",
      "3049\n",
      "3050 0\n",
      "3050\n",
      "3051 0\n",
      "3051\n",
      "3052 0\n",
      "3052\n",
      "3053 0\n",
      "3053\n",
      "3054 0\n",
      "3054\n",
      "3055 0\n",
      "3055\n",
      "3056 0\n",
      "3056\n",
      "3057 0\n",
      "3057\n",
      "3058 0\n",
      "3058\n",
      "3059 0\n",
      "3059\n",
      "3060 0\n",
      "3060\n",
      "3061 0\n",
      "3061\n",
      "3062 0\n",
      "3062\n",
      "3063 0\n",
      "3063\n",
      "3064 0\n",
      "3064\n",
      "3065 0\n",
      "3065\n",
      "3066 0\n",
      "3066\n",
      "3067 0\n",
      "3067\n",
      "3068 0\n",
      "3068\n",
      "3069 0\n",
      "3069\n",
      "3070 0\n",
      "3070\n",
      "3071 0\n",
      "3071\n",
      "3072 0\n",
      "3072\n",
      "3073 0\n",
      "3073\n",
      "3074 0\n",
      "3074\n",
      "3075 0\n",
      "3075\n",
      "3076 0\n",
      "3076\n",
      "3077 0\n",
      "3077\n",
      "3078 0\n",
      "3078\n",
      "3079 0\n",
      "3079\n",
      "3080 0\n",
      "3080\n",
      "3081 0\n",
      "3081\n",
      "3082 0\n",
      "3082\n",
      "3083 0\n",
      "3083\n",
      "3084 0\n",
      "3084\n",
      "3085 0\n",
      "3085\n",
      "3086 0\n",
      "3086\n",
      "3087 0\n",
      "3087\n",
      "3088 0\n",
      "3088\n",
      "3089 0\n",
      "3089\n",
      "3090 0\n",
      "3090\n",
      "3091 0\n",
      "3091\n",
      "3092 0\n",
      "3092\n",
      "3093 0\n",
      "3093\n",
      "3094 0\n",
      "3094\n",
      "3095 0\n",
      "3095\n",
      "3096 0\n",
      "3096\n",
      "3097 0\n",
      "3097\n",
      "3098 0\n",
      "3098\n",
      "3099 0\n",
      "3099\n",
      "3100 0\n",
      "3100\n",
      "3101 0\n",
      "3101\n",
      "3102 0\n",
      "3102\n",
      "3103 0\n",
      "3103\n",
      "3104 0\n",
      "3104\n",
      "3105 0\n",
      "3105\n",
      "3106 0\n",
      "3106\n",
      "3107 0\n",
      "3107\n",
      "3108 0\n",
      "3108\n",
      "3109 0\n",
      "3109\n",
      "3110 0\n",
      "3110\n",
      "3111 0\n",
      "3111\n",
      "3112 0\n",
      "3112\n",
      "3113 0\n",
      "3113\n",
      "3114 0\n",
      "3114\n",
      "3115 0\n",
      "3115\n",
      "3116 0\n",
      "3116\n",
      "3117 0\n",
      "3117\n",
      "3118 0\n",
      "3118\n",
      "3119 0\n",
      "3119\n",
      "3120 0\n",
      "3120\n",
      "3121 0\n",
      "3121\n",
      "3122 0\n",
      "3122\n",
      "3123 0\n",
      "3123\n",
      "3124 0\n",
      "3124\n",
      "3125 0\n",
      "3125\n",
      "3126 0\n",
      "3126\n",
      "3127 0\n",
      "3127\n",
      "3128 0\n",
      "3128\n",
      "3129 0\n",
      "3129\n",
      "3130 0\n",
      "3130\n",
      "3131 0\n",
      "3131\n",
      "3132 0\n",
      "3132\n",
      "3133 0\n",
      "3133\n",
      "3134 0\n",
      "3134\n",
      "3135 0\n",
      "3135\n",
      "3136 0\n",
      "3136\n",
      "3137 0\n",
      "3137\n",
      "3138 0\n",
      "3138\n",
      "3139 0\n",
      "3139\n",
      "3140 0\n",
      "3140\n",
      "3141 0\n",
      "3141\n",
      "3142 0\n",
      "3142\n",
      "3143 0\n",
      "3143\n",
      "3144 0\n",
      "3144\n",
      "3145 0\n",
      "3145\n",
      "3146 0\n",
      "3146\n",
      "3147 0\n",
      "3147\n",
      "3148 0\n",
      "3148\n",
      "3149 0\n",
      "3149\n",
      "3150 0\n",
      "3150\n",
      "3151 0\n",
      "3151\n",
      "3152 0\n",
      "3152\n",
      "3153 0\n",
      "3153\n",
      "3154 0\n",
      "3154\n",
      "3155 0\n",
      "3155\n",
      "3156 0\n",
      "3156\n",
      "3157 0\n",
      "3157\n",
      "3158 0\n",
      "3158\n",
      "3159 0\n",
      "3159\n",
      "3160 0\n",
      "3160\n",
      "3161 0\n",
      "3161\n",
      "3162 0\n",
      "3162\n",
      "3163 0\n",
      "3163\n",
      "3164 0\n",
      "3164\n",
      "3165 0\n",
      "3165\n",
      "3166 0\n",
      "3166\n",
      "3167 0\n",
      "3167\n",
      "3168 0\n",
      "3168\n",
      "3169 0\n",
      "3169\n",
      "3170 0\n",
      "3170\n",
      "3171 0\n",
      "3171\n",
      "3172 0\n",
      "3172\n",
      "3173 0\n",
      "3173\n",
      "3174 0\n",
      "3174\n",
      "3175 0\n",
      "3175\n",
      "3176 0\n",
      "3176\n",
      "3177 0\n",
      "3177\n",
      "3178 0\n",
      "3178\n",
      "3179 0\n",
      "3179\n",
      "3180 0\n",
      "3180\n",
      "3181 0\n",
      "3181\n",
      "3182 0\n",
      "3182\n",
      "3183 0\n",
      "3183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184 0\n",
      "3184\n",
      "3185 0\n",
      "3185\n",
      "3186 0\n",
      "3186\n",
      "3187 0\n",
      "3187\n",
      "3188 0\n",
      "3188\n",
      "3189 0\n",
      "3189\n",
      "3190 0\n",
      "3190\n",
      "3191 0\n",
      "3191\n",
      "3192 0\n",
      "3192\n",
      "3193 0\n",
      "3193\n",
      "3194 0\n",
      "3194\n",
      "3195 0\n",
      "3195\n",
      "3196 0\n",
      "3196\n",
      "3197 0\n",
      "3197\n",
      "3198 0\n",
      "3198\n",
      "3199 0\n",
      "3199\n",
      "3200 0\n",
      "3200\n",
      "3201 0\n",
      "3201\n",
      "3202 0\n",
      "3202\n",
      "3203 0\n",
      "3203\n",
      "3204 0\n",
      "3204\n",
      "3205 0\n",
      "3205\n",
      "3206 0\n",
      "3206\n",
      "3207 0\n",
      "3207\n",
      "3208 0\n",
      "3208\n",
      "3209 0\n",
      "3209\n",
      "3210 0\n",
      "3210\n",
      "3211 0\n",
      "3211\n",
      "3212 0\n",
      "3212\n",
      "3213 0\n",
      "3213\n",
      "3214 0\n",
      "3214\n",
      "3215 0\n",
      "3215\n",
      "3216 0\n",
      "3216\n",
      "3217 0\n",
      "3217\n",
      "3218 0\n",
      "3218\n",
      "3219 0\n",
      "3219\n",
      "3220 0\n",
      "3220\n",
      "3221 0\n",
      "3221\n",
      "3222 0\n",
      "3222\n",
      "3223 0\n",
      "3223\n",
      "3224 0\n",
      "3224\n",
      "3225 0\n",
      "3225\n",
      "3226 0\n",
      "3226\n",
      "3227 0\n",
      "3227\n",
      "3228 0\n",
      "3228\n",
      "3229 0\n",
      "3229\n",
      "3230 0\n",
      "3230\n",
      "3231 0\n",
      "3231\n",
      "3232 0\n",
      "3232\n",
      "3233 0\n",
      "3233\n",
      "3234 0\n",
      "3234\n",
      "3235 0\n",
      "3235\n",
      "3236 0\n",
      "3236\n",
      "3237 0\n",
      "3237\n",
      "3238 0\n",
      "3238\n",
      "3239 0\n",
      "3239\n",
      "3240 0\n",
      "3240\n",
      "3241 0\n",
      "3241\n",
      "3242 0\n",
      "3242\n",
      "3243 0\n",
      "3243\n",
      "3244 0\n",
      "3244\n",
      "3245 0\n",
      "3245\n",
      "3246 0\n",
      "3246\n",
      "3247 0\n",
      "3247\n",
      "3248 0\n",
      "3248\n",
      "3249 0\n",
      "3249\n",
      "3250 0\n",
      "3250\n",
      "3251 0\n",
      "3251\n",
      "3252 0\n",
      "3252\n",
      "3253 0\n",
      "3253\n",
      "3254 0\n",
      "3254\n",
      "3255 0\n",
      "3255\n",
      "3256 0\n",
      "3256\n",
      "3257 0\n",
      "3257\n",
      "3258 0\n",
      "3258\n",
      "3259 0\n",
      "3259\n",
      "3260 0\n",
      "3260\n",
      "3261 0\n",
      "3261\n",
      "3262 0\n",
      "3262\n",
      "3263 0\n",
      "3263\n",
      "3264 0\n",
      "3264\n",
      "3265 0\n",
      "3265\n",
      "3266 0\n",
      "3266\n",
      "3267 0\n",
      "3267\n",
      "3268 0\n",
      "3268\n",
      "3269 0\n",
      "3269\n",
      "3270 0\n",
      "3270\n",
      "3271 0\n",
      "3271\n",
      "3272 0\n",
      "3272\n",
      "3273 0\n",
      "3273\n",
      "3274 0\n",
      "3274\n",
      "3275 0\n",
      "3275\n",
      "3276 0\n",
      "3276\n",
      "3277 0\n",
      "3277\n",
      "3278 0\n",
      "3278\n",
      "3279 0\n",
      "3279\n",
      "3280 0\n",
      "3280\n",
      "3281 0\n",
      "3281\n",
      "3282 0\n",
      "3282\n",
      "3283 0\n",
      "3283\n",
      "3284 0\n",
      "3284\n",
      "3285 0\n",
      "3285\n",
      "3286 0\n",
      "3286\n",
      "3287 0\n",
      "3287\n",
      "3288 0\n",
      "3288\n",
      "3289 0\n",
      "3289\n",
      "3290 0\n",
      "3290\n",
      "3291 0\n",
      "3291\n",
      "3292 0\n",
      "3292\n",
      "3293 0\n",
      "3293\n",
      "3294 0\n",
      "3294\n",
      "3295 0\n",
      "3295\n",
      "3296 0\n",
      "3296\n",
      "3297 0\n",
      "3297\n",
      "3298 0\n",
      "3298\n",
      "3299 0\n",
      "3299\n",
      "3300 0\n",
      "3300\n",
      "3301 0\n",
      "3301\n",
      "3302 0\n",
      "3302\n",
      "3303 0\n",
      "3303\n",
      "3304 0\n",
      "3304\n",
      "3305 0\n",
      "3305\n",
      "3306 0\n",
      "3306\n",
      "3307 0\n",
      "3307\n",
      "3308 0\n",
      "3308\n",
      "3309 0\n",
      "3309\n",
      "3310 0\n",
      "3310\n",
      "3311 0\n",
      "3311\n",
      "3312 0\n",
      "3312\n",
      "3313 0\n",
      "3313\n",
      "3314 0\n",
      "3314\n",
      "3315 0\n",
      "3315\n",
      "3316 0\n",
      "3316\n",
      "3317 0\n",
      "3317\n",
      "3318 0\n",
      "3318\n",
      "3319 0\n",
      "3319\n",
      "3320 0\n",
      "3320\n",
      "3321 0\n",
      "3321\n",
      "3322 0\n",
      "3322\n",
      "3323 0\n",
      "3323\n",
      "3324 0\n",
      "3324\n",
      "3325 0\n",
      "3325\n",
      "3326 0\n",
      "3326\n",
      "3327 0\n",
      "3327\n",
      "3328 0\n",
      "3328\n",
      "3329 0\n",
      "3329\n",
      "3330 0\n",
      "3330\n",
      "3331 0\n",
      "3331\n",
      "3332 0\n",
      "3332\n",
      "3333 0\n",
      "3333\n",
      "3334 0\n",
      "3334\n",
      "3335 0\n",
      "3335\n",
      "3336 0\n",
      "3336\n",
      "3337 0\n",
      "3337\n",
      "3338 0\n",
      "3338\n",
      "3339 0\n",
      "3339\n",
      "3340 0\n",
      "3340\n",
      "3341 0\n",
      "3341\n",
      "3342 0\n",
      "3342\n",
      "3343 0\n",
      "3343\n",
      "3344 0\n",
      "3344\n",
      "3345 0\n",
      "3345\n",
      "3346 0\n",
      "3346\n",
      "3347 0\n",
      "3347\n",
      "3348 0\n",
      "3348\n",
      "3349 0\n",
      "3349\n",
      "3350 0\n",
      "3350\n",
      "3351 0\n",
      "3351\n",
      "3352 0\n",
      "3352\n",
      "3353 0\n",
      "3353\n",
      "3354 0\n",
      "3354\n",
      "3355 0\n",
      "3355\n",
      "3356 0\n",
      "3356\n",
      "3357 0\n",
      "3357\n",
      "3358 0\n",
      "3358\n",
      "3359 0\n",
      "3359\n",
      "3360 0\n",
      "3360\n",
      "3361 0\n",
      "3361\n",
      "3362 0\n",
      "3362\n",
      "3363 0\n",
      "3363\n",
      "3364 0\n",
      "3364\n",
      "3365 0\n",
      "3365\n",
      "3366 0\n",
      "3366\n",
      "3367 0\n",
      "3367\n",
      "3368 0\n",
      "3368\n",
      "3369 0\n",
      "3369\n",
      "3370 0\n",
      "3370\n",
      "3371 0\n",
      "3371\n",
      "3372 0\n",
      "3372\n",
      "3373 0\n",
      "3373\n",
      "3374 0\n",
      "3374\n",
      "3375 0\n",
      "3375\n",
      "3376 0\n",
      "3376\n",
      "3377 0\n",
      "3377\n",
      "3378 0\n",
      "3378\n",
      "3379 0\n",
      "3379\n",
      "3380 0\n",
      "3380\n",
      "3381 0\n",
      "3381\n",
      "3382 0\n",
      "3382\n",
      "3383 0\n",
      "3383\n",
      "3384 0\n",
      "3384\n",
      "3385 0\n",
      "3385\n",
      "3386 0\n",
      "3386\n",
      "3387 0\n",
      "3387\n",
      "3388 0\n",
      "3388\n",
      "3389 0\n",
      "3389\n",
      "3390 0\n",
      "3390\n",
      "3391 0\n",
      "3391\n",
      "3392 0\n",
      "3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3393 0\n",
      "3393\n",
      "3394 0\n",
      "3394\n",
      "3395 0\n",
      "3395\n",
      "3396 0\n",
      "3396\n",
      "3397 0\n",
      "3397\n",
      "3398 0\n",
      "3398\n",
      "3399 0\n",
      "3399\n",
      "3400 0\n",
      "3400\n",
      "3401 0\n",
      "3401\n",
      "3402 0\n",
      "3402\n",
      "3403 0\n",
      "3403\n",
      "3404 0\n",
      "3404\n",
      "3405 0\n",
      "3405\n",
      "3406 0\n",
      "3406\n",
      "3407 0\n",
      "3407\n",
      "3408 0\n",
      "3408\n",
      "3409 0\n",
      "3409\n",
      "3410 0\n",
      "3410\n",
      "3411 0\n",
      "3411\n",
      "3412 0\n",
      "3412\n",
      "3413 0\n",
      "3413\n",
      "3414 0\n",
      "3414\n",
      "3415 0\n",
      "3415\n",
      "3416 0\n",
      "3416\n",
      "3417 0\n",
      "3417\n",
      "3418 0\n",
      "3418\n",
      "3419 0\n",
      "3419\n",
      "3420 0\n",
      "3420\n",
      "3421 0\n",
      "3421\n",
      "3422 0\n",
      "3422\n",
      "3423 0\n",
      "3423\n",
      "3424 0\n",
      "3424\n",
      "3425 0\n",
      "3425\n",
      "3426 0\n",
      "3426\n",
      "3427 0\n",
      "3427\n",
      "3428 0\n",
      "3428\n",
      "3429 0\n",
      "3429\n",
      "3430 0\n",
      "3430\n",
      "3431 0\n",
      "3431\n",
      "3432 0\n",
      "3432\n",
      "3433 0\n",
      "3433\n",
      "3434 0\n",
      "3434\n",
      "3435 0\n",
      "3435\n",
      "3436 0\n",
      "3436\n",
      "3437 0\n",
      "3437\n",
      "3438 0\n",
      "3438\n",
      "3439 0\n",
      "3439\n",
      "3440 0\n",
      "3440\n",
      "3441 0\n",
      "3441\n",
      "3442 0\n",
      "3442\n",
      "3443 0\n",
      "3443\n",
      "3444 0\n",
      "3444\n",
      "3445 0\n",
      "3445\n",
      "3446 0\n",
      "3446\n",
      "3447 0\n",
      "3447\n",
      "3448 0\n",
      "3448\n",
      "3449 0\n",
      "3449\n",
      "3450 0\n",
      "3450\n",
      "3451 0\n",
      "3451\n",
      "3452 0\n",
      "3452\n",
      "3453 0\n",
      "3453\n",
      "3454 0\n",
      "3454\n",
      "3455 0\n",
      "3455\n",
      "3456 0\n",
      "3456\n",
      "3457 0\n",
      "3457\n",
      "3458 0\n",
      "3458\n",
      "3459 0\n",
      "3459\n",
      "3460 0\n",
      "3460\n",
      "3461 0\n",
      "3461\n",
      "3462 0\n",
      "3462\n",
      "3463 0\n",
      "3463\n",
      "3464 0\n",
      "3464\n",
      "3465 0\n",
      "3465\n",
      "3466 0\n",
      "3466\n",
      "3467 0\n",
      "3467\n",
      "3468 0\n",
      "3468\n",
      "3469 0\n",
      "3469\n",
      "3470 0\n",
      "3470\n",
      "3471 0\n",
      "3471\n",
      "3472 0\n",
      "3472\n",
      "3473 0\n",
      "3473\n",
      "3474 0\n",
      "3474\n",
      "3475 0\n",
      "3475\n",
      "3476 0\n",
      "3476\n",
      "3477 0\n",
      "3477\n",
      "3478 0\n",
      "3478\n",
      "3479 0\n",
      "3479\n",
      "3480 0\n",
      "3480\n",
      "3481 0\n",
      "3481\n",
      "3482 0\n",
      "3482\n",
      "3483 0\n",
      "3483\n",
      "3484 0\n",
      "3484\n",
      "3485 0\n",
      "3485\n",
      "3486 0\n",
      "3486\n",
      "3487 0\n",
      "3487\n",
      "3488 0\n",
      "3488\n",
      "3489 0\n",
      "3489\n",
      "3490 0\n",
      "3490\n",
      "3491 0\n",
      "3491\n",
      "3492 0\n",
      "3492\n",
      "3493 0\n",
      "3493\n",
      "3494 0\n",
      "3494\n",
      "3495 0\n",
      "3495\n",
      "3496 0\n",
      "3496\n",
      "3497 0\n",
      "3497\n",
      "3498 0\n",
      "3498\n",
      "3499 0\n",
      "3499\n",
      "3500 0\n",
      "3500\n",
      "3501 0\n",
      "3501\n",
      "3502 0\n",
      "3502\n",
      "3503 0\n",
      "3503\n",
      "3504 0\n",
      "3504\n",
      "3505 0\n",
      "3505\n",
      "3506 0\n",
      "3506\n",
      "3507 0\n",
      "3507\n",
      "3508 0\n",
      "3508\n",
      "3509 0\n",
      "3509\n",
      "3510 0\n",
      "3510\n",
      "3511 0\n",
      "3511\n",
      "3512 0\n",
      "3512\n",
      "3513 0\n",
      "3513\n",
      "3514 0\n",
      "3514\n",
      "3515 0\n",
      "3515\n",
      "3516 0\n",
      "3516\n",
      "3517 0\n",
      "3517\n",
      "3518 0\n",
      "3518\n",
      "3519 0\n",
      "3519\n",
      "3520 0\n",
      "3520\n",
      "3521 0\n",
      "3521\n",
      "3522 0\n",
      "3522\n",
      "3523 0\n",
      "3523\n",
      "3524 0\n",
      "3524\n",
      "3525 0\n",
      "3525\n",
      "3526 0\n",
      "3526\n",
      "3527 0\n",
      "3527\n",
      "3528 0\n",
      "3528\n",
      "3529 0\n",
      "3529\n",
      "3530 0\n",
      "3530\n",
      "3531 0\n",
      "3531\n",
      "3532 0\n",
      "3532\n",
      "3533 0\n",
      "3533\n",
      "3534 0\n",
      "3534\n",
      "3535 0\n",
      "3535\n",
      "3536 0\n",
      "3536\n",
      "3537 0\n",
      "3537\n",
      "3538 0\n",
      "3538\n",
      "3539 0\n",
      "3539\n",
      "3540 0\n",
      "3540\n",
      "3541 0\n",
      "3541\n",
      "3542 0\n",
      "3542\n",
      "3543 0\n",
      "3543\n",
      "3544 0\n",
      "3544\n",
      "3545 0\n",
      "3545\n",
      "3546 0\n",
      "3546\n",
      "3547 0\n",
      "3547\n",
      "3548 0\n",
      "3548\n",
      "3549 0\n",
      "3549\n",
      "3550 0\n",
      "3550\n",
      "3551 0\n",
      "3551\n",
      "3552 0\n",
      "3552\n",
      "3553 0\n",
      "3553\n",
      "3554 0\n",
      "3554\n",
      "3555 0\n",
      "3555\n",
      "3556 0\n",
      "3556\n",
      "3557 0\n",
      "3557\n",
      "3558 0\n",
      "3558\n",
      "3559 0\n",
      "3559\n",
      "3560 0\n",
      "3560\n",
      "3561 0\n",
      "3561\n",
      "3562 0\n",
      "3562\n",
      "3563 0\n",
      "3563\n",
      "3564 0\n",
      "3564\n",
      "3565 0\n",
      "3565\n",
      "3566 0\n",
      "3566\n",
      "3567 0\n",
      "3567\n",
      "3568 0\n",
      "3568\n",
      "3569 0\n",
      "3569\n",
      "3570 0\n",
      "3570\n",
      "3571 0\n",
      "3571\n",
      "3572 0\n",
      "3572\n",
      "3573 0\n",
      "3573\n",
      "3574 0\n",
      "3574\n",
      "3575 0\n",
      "3575\n",
      "3576 0\n",
      "3576\n",
      "3577 0\n",
      "3577\n",
      "3578 0\n",
      "3578\n",
      "3579 0\n",
      "3579\n",
      "3580 0\n",
      "3580\n",
      "3581 0\n",
      "3581\n",
      "3582 0\n",
      "3582\n",
      "3583 0\n",
      "3583\n",
      "3584 0\n",
      "3584\n",
      "3585 0\n",
      "3585\n",
      "3586 0\n",
      "3586\n",
      "3587 0\n",
      "3587\n",
      "3588 0\n",
      "3588\n",
      "3589 0\n",
      "3589\n",
      "3590 0\n",
      "3590\n",
      "3591 0\n",
      "3591\n",
      "3592 0\n",
      "3592\n",
      "3593 0\n",
      "3593\n",
      "3594 0\n",
      "3594\n",
      "3595 0\n",
      "3595\n",
      "3596 0\n",
      "3596\n",
      "3597 0\n",
      "3597\n",
      "3598 0\n",
      "3598\n",
      "3599 0\n",
      "3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 0\n",
      "3600\n",
      "3601 0\n",
      "3601\n",
      "3602 0\n",
      "3602\n",
      "3603 0\n",
      "3603\n",
      "3604 0\n",
      "3604\n",
      "3605 0\n",
      "3605\n",
      "3606 0\n",
      "3606\n",
      "3607 0\n",
      "3607\n",
      "3608 0\n",
      "3608\n",
      "3609 0\n",
      "3609\n",
      "3610 0\n",
      "3610\n",
      "3611 0\n",
      "3611\n",
      "3612 0\n",
      "3612\n",
      "3613 0\n",
      "3613\n",
      "3614 0\n",
      "3614\n",
      "3615 0\n",
      "3615\n",
      "3616 0\n",
      "3616\n",
      "3617 0\n",
      "3617\n",
      "3618 0\n",
      "3618\n",
      "3619 0\n",
      "3619\n",
      "3620 0\n",
      "3620\n",
      "3621 0\n",
      "3621\n",
      "3622 0\n",
      "3622\n",
      "3623 0\n",
      "3623\n",
      "3624 0\n",
      "3624\n",
      "3625 0\n",
      "3625\n",
      "3626 0\n",
      "3626\n",
      "3627 0\n",
      "3627\n",
      "3628 0\n",
      "3628\n",
      "3629 0\n",
      "3629\n",
      "3630 0\n",
      "3630\n",
      "3631 0\n",
      "3631\n",
      "3632 0\n",
      "3632\n",
      "3633 0\n",
      "3633\n",
      "3634 0\n",
      "3634\n",
      "3635 0\n",
      "3635\n",
      "3636 0\n",
      "3636\n",
      "3637 0\n",
      "3637\n",
      "3638 0\n",
      "3638\n",
      "3639 0\n",
      "3639\n",
      "3640 0\n",
      "3640\n",
      "3641 0\n",
      "3641\n",
      "3642 0\n",
      "3642\n",
      "3643 0\n",
      "3643\n",
      "3644 0\n",
      "3644\n",
      "3645 0\n",
      "3645\n",
      "3646 0\n",
      "3646\n",
      "3647 0\n",
      "3647\n",
      "3648 0\n",
      "3648\n",
      "3649 0\n",
      "3649\n",
      "3650 0\n",
      "3650\n",
      "3651 0\n",
      "3651\n",
      "3652 0\n",
      "3652\n",
      "3653 0\n",
      "3653\n",
      "3654 0\n",
      "3654\n",
      "3655 0\n",
      "3655\n",
      "3656 0\n",
      "3656\n",
      "3657 0\n",
      "3657\n",
      "3658 0\n",
      "3658\n",
      "3659 0\n",
      "3659\n",
      "3660 0\n",
      "3660\n",
      "3661 0\n",
      "3661\n",
      "3662 0\n",
      "3662\n",
      "3663 0\n",
      "3663\n",
      "3664 0\n",
      "3664\n",
      "3665 0\n",
      "3665\n",
      "3666 0\n",
      "3666\n",
      "3667 0\n",
      "3667\n",
      "3668 0\n",
      "3668\n",
      "3669 0\n",
      "3669\n",
      "3670 0\n",
      "3670\n",
      "3671 0\n",
      "3671\n",
      "3672 0\n",
      "3672\n",
      "3673 0\n",
      "3673\n",
      "3674 0\n",
      "3674\n",
      "3675 0\n",
      "3675\n",
      "3676 0\n",
      "3676\n",
      "3677 0\n",
      "3677\n",
      "3678 0\n",
      "3678\n",
      "3679 0\n",
      "3679\n",
      "3680 0\n",
      "3680\n",
      "3681 0\n",
      "3681\n",
      "3682 0\n",
      "3682\n",
      "3683 0\n",
      "3683\n",
      "3684 0\n",
      "3684\n",
      "3685 0\n",
      "3685\n",
      "3686 0\n",
      "3686\n",
      "3687 0\n",
      "3687\n",
      "3688 0\n",
      "3688\n",
      "3689 0\n",
      "3689\n",
      "3690 0\n",
      "3690\n",
      "3691 0\n",
      "3691\n",
      "3692 0\n",
      "3692\n",
      "3693 0\n",
      "3693\n",
      "3694 0\n",
      "3694\n",
      "3695 0\n",
      "3695\n",
      "3696 0\n",
      "3696\n",
      "3697 0\n",
      "3697\n",
      "3698 0\n",
      "3698\n",
      "3699 0\n",
      "3699\n",
      "3700 0\n",
      "3700\n",
      "3701 0\n",
      "3701\n",
      "3702 0\n",
      "3702\n",
      "3703 0\n",
      "3703\n",
      "3704 0\n",
      "3704\n",
      "3705 0\n",
      "3705\n",
      "3706 0\n",
      "3706\n",
      "3707 0\n",
      "3707\n",
      "3708 0\n",
      "3708\n",
      "3709 0\n",
      "3709\n",
      "3710 0\n",
      "3710\n",
      "3711 0\n",
      "3711\n",
      "3712 0\n",
      "3712\n",
      "3713 0\n",
      "3713\n",
      "3714 0\n",
      "3714\n",
      "3715 0\n",
      "3715\n",
      "3716 0\n",
      "3716\n",
      "3717 0\n",
      "3717\n",
      "3718 0\n",
      "3718\n",
      "3719 0\n",
      "3719\n",
      "3720 0\n",
      "3720\n",
      "3721 0\n",
      "3721\n",
      "3722 0\n",
      "3722\n",
      "3723 0\n",
      "3723\n",
      "3724 0\n",
      "3724\n",
      "3725 0\n",
      "3725\n",
      "3726 0\n",
      "3726\n",
      "3727 0\n",
      "3727\n",
      "3728 0\n",
      "3728\n",
      "3729 0\n",
      "3729\n",
      "3730 0\n",
      "3730\n",
      "3731 0\n",
      "3731\n",
      "3732 0\n",
      "3732\n",
      "3733 0\n",
      "3733\n",
      "3734 0\n",
      "3734\n",
      "3735 0\n",
      "3735\n",
      "3736 0\n",
      "3736\n",
      "3737 0\n",
      "3737\n",
      "3738 0\n",
      "3738\n",
      "3739 0\n",
      "3739\n",
      "3740 0\n",
      "3740\n",
      "3741 0\n",
      "3741\n",
      "3742 0\n",
      "3742\n",
      "3743 0\n",
      "3743\n",
      "3744 0\n",
      "3744\n",
      "3745 0\n",
      "3745\n",
      "3746 0\n",
      "3746\n",
      "3747 0\n",
      "3747\n",
      "3748 0\n",
      "3748\n",
      "3749 0\n",
      "3749\n",
      "3750 0\n",
      "3750\n",
      "3751 0\n",
      "3751\n",
      "3752 0\n",
      "3752\n",
      "3753 0\n",
      "3753\n",
      "3754 0\n",
      "3754\n",
      "3755 0\n",
      "3755\n",
      "3756 0\n",
      "3756\n",
      "3757 0\n",
      "3757\n",
      "3758 0\n",
      "3758\n",
      "3759 0\n",
      "3759\n",
      "3760 0\n",
      "3760\n",
      "3761 0\n",
      "3761\n",
      "3762 0\n",
      "3762\n",
      "3763 0\n",
      "3763\n",
      "3764 0\n",
      "3764\n",
      "3765 0\n",
      "3765\n",
      "3766 0\n",
      "3766\n",
      "3767 0\n",
      "3767\n",
      "3768 0\n",
      "3768\n",
      "3769 0\n",
      "3769\n",
      "3770 0\n",
      "3770\n",
      "3771 0\n",
      "3771\n",
      "3772 0\n",
      "3772\n",
      "3773 0\n",
      "3773\n",
      "3774 0\n",
      "3774\n",
      "3775 0\n",
      "3775\n",
      "3776 0\n",
      "3776\n",
      "3777 0\n",
      "3777\n",
      "3778 0\n",
      "3778\n",
      "3779 0\n",
      "3779\n",
      "3780 0\n",
      "3780\n",
      "3781 0\n",
      "3781\n",
      "3782 0\n",
      "3782\n",
      "3783 0\n",
      "3783\n",
      "3784 0\n",
      "3784\n",
      "3785 0\n",
      "3785\n",
      "3786 0\n",
      "3786\n",
      "3787 0\n",
      "3787\n",
      "3788 0\n",
      "3788\n",
      "3789 0\n",
      "3789\n",
      "3790 0\n",
      "3790\n",
      "3791 0\n",
      "3791\n",
      "3792 0\n",
      "3792\n",
      "3793 0\n",
      "3793\n",
      "3794 0\n",
      "3794\n",
      "3795 0\n",
      "3795\n",
      "3796 0\n",
      "3796\n",
      "3797 0\n",
      "3797\n",
      "3798 0\n",
      "3798\n",
      "3799 0\n",
      "3799\n",
      "3800 0\n",
      "3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3801 0\n",
      "3801\n",
      "3802 0\n",
      "3802\n",
      "3803 0\n",
      "3803\n",
      "3804 0\n",
      "3804\n",
      "3805 0\n",
      "3805\n",
      "3806 0\n",
      "3806\n",
      "3807 0\n",
      "3807\n",
      "3808 0\n",
      "3808\n",
      "3809 0\n",
      "3809\n",
      "3810 0\n",
      "3810\n",
      "3811 0\n",
      "3811\n",
      "3812 0\n",
      "3812\n",
      "3813 0\n",
      "3813\n",
      "3814 0\n",
      "3814\n",
      "3815 0\n",
      "3815\n",
      "3816 0\n",
      "3816\n",
      "3817 0\n",
      "3817\n",
      "3818 0\n",
      "3818\n",
      "3819 0\n",
      "3819\n",
      "3820 0\n",
      "3820\n",
      "3821 0\n",
      "3821\n",
      "3822 0\n",
      "3822\n",
      "3823 0\n",
      "3823\n",
      "3824 0\n",
      "3824\n",
      "3825 0\n",
      "3825\n",
      "3826 0\n",
      "3826\n",
      "3827 0\n",
      "3827\n",
      "3828 0\n",
      "3828\n",
      "3829 0\n",
      "3829\n",
      "3830 0\n",
      "3830\n",
      "3831 0\n",
      "3831\n",
      "3832 0\n",
      "3832\n",
      "3833 0\n",
      "3833\n",
      "3834 0\n",
      "3834\n",
      "3835 0\n",
      "3835\n",
      "3836 0\n",
      "3836\n",
      "3837 0\n",
      "3837\n",
      "3838 0\n",
      "3838\n",
      "3839 0\n",
      "3839\n",
      "3840 0\n",
      "3840\n",
      "3841 0\n",
      "3841\n",
      "3842 0\n",
      "3842\n",
      "3843 0\n",
      "3843\n",
      "3844 0\n",
      "3844\n",
      "3845 0\n",
      "3845\n",
      "3846 0\n",
      "3846\n",
      "3847 0\n",
      "3847\n",
      "3848 0\n",
      "3848\n",
      "3849 0\n",
      "3849\n",
      "3850 0\n",
      "3850\n",
      "3851 0\n",
      "3851\n",
      "3852 0\n",
      "3852\n",
      "3853 0\n",
      "3853\n",
      "3854 0\n",
      "3854\n",
      "3855 0\n",
      "3855\n",
      "3856 0\n",
      "3856\n",
      "3857 0\n",
      "3857\n",
      "3858 0\n",
      "3858\n",
      "3859 0\n",
      "3859\n",
      "3860 0\n",
      "3860\n",
      "3861 0\n",
      "3861\n",
      "3862 0\n",
      "3862\n",
      "3863 0\n",
      "3863\n",
      "3864 0\n",
      "3864\n",
      "3865 0\n",
      "3865\n",
      "3866 0\n",
      "3866\n",
      "3867 0\n",
      "3867\n",
      "3868 0\n",
      "3868\n",
      "3869 0\n",
      "3869\n",
      "3870 0\n",
      "3870\n",
      "3871 0\n",
      "3871\n",
      "3872 0\n",
      "3872\n",
      "3873 0\n",
      "3873\n",
      "3874 0\n",
      "3874\n",
      "3875 0\n",
      "3875\n",
      "3876 0\n",
      "3876\n",
      "3877 0\n",
      "3877\n",
      "3878 0\n",
      "3878\n",
      "3879 0\n",
      "3879\n",
      "3880 0\n",
      "3880\n",
      "3881 0\n",
      "3881\n",
      "3882 0\n",
      "3882\n",
      "3883 0\n",
      "3883\n",
      "3884 0\n",
      "3884\n",
      "3885 0\n",
      "3885\n",
      "3886 0\n",
      "3886\n",
      "3887 0\n",
      "3887\n",
      "3888 0\n",
      "3888\n",
      "3889 0\n",
      "3889\n",
      "3890 0\n",
      "3890\n",
      "3891 0\n",
      "3891\n",
      "3892 0\n",
      "3892\n",
      "3893 0\n",
      "3893\n",
      "3894 0\n",
      "3894\n",
      "3895 0\n",
      "3895\n",
      "3896 0\n",
      "3896\n",
      "3897 0\n",
      "3897\n",
      "3898 0\n",
      "3898\n",
      "3899 0\n",
      "3899\n",
      "3900 0\n",
      "3900\n",
      "3901 0\n",
      "3901\n",
      "3902 0\n",
      "3902\n",
      "3903 0\n",
      "3903\n",
      "3904 0\n",
      "3904\n",
      "3905 0\n",
      "3905\n",
      "3906 0\n",
      "3906\n",
      "3907 0\n",
      "3907\n",
      "3908 0\n",
      "3908\n",
      "3909 0\n",
      "3909\n",
      "3910 0\n",
      "3910\n",
      "3911 0\n",
      "3911\n",
      "3912 0\n",
      "3912\n",
      "3913 0\n",
      "3913\n",
      "3914 0\n",
      "3914\n",
      "3915 0\n",
      "3915\n",
      "3916 0\n",
      "3916\n",
      "3917 0\n",
      "3917\n",
      "3918 0\n",
      "3918\n",
      "3919 0\n",
      "3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3920 0\n",
      "3920\n",
      "3921 0\n",
      "3921\n",
      "3922 0\n",
      "3922\n",
      "3923 0\n",
      "3923\n",
      "3924 0\n",
      "3924\n",
      "3925 0\n",
      "3925\n",
      "3926 0\n",
      "3926\n",
      "3927 0\n",
      "3927\n",
      "3928 0\n",
      "3928\n",
      "3929 0\n",
      "3929\n",
      "3930 0\n",
      "3930\n",
      "3931 0\n",
      "3931\n",
      "3932 0\n",
      "3932\n",
      "3933 0\n",
      "3933\n",
      "3934 0\n",
      "3934\n",
      "3935 0\n",
      "3935\n",
      "3936 0\n",
      "3936\n",
      "3937 0\n",
      "3937\n",
      "3938 0\n",
      "3938\n",
      "3939 0\n",
      "3939\n",
      "3940 0\n",
      "3940\n",
      "3941 0\n",
      "3941\n",
      "3942 0\n",
      "3942\n",
      "3943 0\n",
      "3943\n",
      "3944 0\n",
      "3944\n",
      "3945 0\n",
      "3945\n",
      "3946 0\n",
      "3946\n",
      "3947 0\n",
      "3947\n",
      "3948 0\n",
      "3948\n",
      "3949 0\n",
      "3949\n",
      "3950 0\n",
      "3950\n",
      "3951 0\n",
      "3951\n",
      "3952 0\n",
      "3952\n",
      "3953 0\n",
      "3953\n",
      "3954 0\n",
      "3954\n",
      "3955 0\n",
      "3955\n",
      "3956 0\n",
      "3956\n",
      "3957 0\n",
      "3957\n",
      "3958 0\n",
      "3958\n",
      "3959 0\n",
      "3959\n",
      "3960 0\n",
      "3960\n",
      "3961 0\n",
      "3961\n",
      "3962 0\n",
      "3962\n",
      "3963 0\n",
      "3963\n",
      "3964 0\n",
      "3964\n",
      "3965 0\n",
      "3965\n",
      "3966 0\n",
      "3966\n",
      "3967 0\n",
      "3967\n",
      "3968 0\n",
      "3968\n",
      "3969 0\n",
      "3969\n",
      "3970 0\n",
      "3970\n",
      "3971 0\n",
      "3971\n",
      "3972 0\n",
      "3972\n",
      "3973 0\n",
      "3973\n",
      "3974 0\n",
      "3974\n",
      "3975 0\n",
      "3975\n",
      "3976 0\n",
      "3976\n",
      "3977 0\n",
      "3977\n",
      "3978 0\n",
      "3978\n",
      "3979 0\n",
      "3979\n",
      "3980 0\n",
      "3980\n",
      "3981 0\n",
      "3981\n",
      "3982 0\n",
      "3982\n",
      "3983 0\n",
      "3983\n",
      "3984 0\n",
      "3984\n",
      "3985 0\n",
      "3985\n",
      "3986 0\n",
      "3986\n",
      "3987 0\n",
      "3987\n",
      "3988 0\n",
      "3988\n",
      "3989 0\n",
      "3989\n",
      "3990 0\n",
      "3990\n",
      "3991 0\n",
      "3991\n",
      "3992 0\n",
      "3992\n",
      "3993 0\n",
      "3993\n",
      "3994 0\n",
      "3994\n",
      "3995 0\n",
      "3995\n",
      "3996 0\n",
      "3996\n",
      "3997 0\n",
      "3997\n",
      "3998 0\n",
      "3998\n",
      "3999 0\n",
      "3999\n",
      "4000 0\n",
      "4000\n",
      "4001 0\n",
      "4001\n",
      "4002 0\n",
      "4002\n",
      "4003 0\n",
      "4003\n",
      "4004 0\n",
      "4004\n",
      "4005 0\n",
      "4005\n",
      "4006 0\n",
      "4006\n",
      "4007 0\n",
      "4007\n",
      "4008 0\n",
      "4008\n",
      "4009 0\n",
      "4009\n",
      "4010 0\n",
      "4010\n",
      "4011 0\n",
      "4011\n",
      "4012 0\n",
      "4012\n",
      "4013 0\n",
      "4013\n",
      "4014 0\n",
      "4014\n",
      "4015 0\n",
      "4015\n",
      "4016 0\n",
      "4016\n",
      "4017 0\n",
      "4017\n",
      "4018 0\n",
      "4018\n",
      "4019 0\n",
      "4019\n",
      "4020 0\n",
      "4020\n",
      "4021 0\n",
      "4021\n",
      "4022 0\n",
      "4022\n",
      "4023 0\n",
      "4023\n",
      "4024 0\n",
      "4024\n",
      "4025 0\n",
      "4025\n",
      "4026 0\n",
      "4026\n",
      "4027 0\n",
      "4027\n",
      "4028 0\n",
      "4028\n",
      "4029 0\n",
      "4029\n",
      "4030 0\n",
      "4030\n",
      "4031 0\n",
      "4031\n",
      "4032 0\n",
      "4032\n",
      "4033 0\n",
      "4033\n",
      "4034 0\n",
      "4034\n",
      "4035 0\n",
      "4035\n",
      "4036 0\n",
      "4036\n",
      "4037 0\n",
      "4037\n",
      "4038 0\n",
      "4038\n",
      "4039 0\n",
      "4039\n",
      "4040 0\n",
      "4040\n",
      "4041 0\n",
      "4041\n",
      "4042 0\n",
      "4042\n",
      "4043 0\n",
      "4043\n",
      "4044 0\n",
      "4044\n",
      "4045 0\n",
      "4045\n",
      "4046 0\n",
      "4046\n",
      "4047 0\n",
      "4047\n",
      "4048 0\n",
      "4048\n",
      "4049 0\n",
      "4049\n",
      "4050 0\n",
      "4050\n",
      "4051 0\n",
      "4051\n",
      "4052 0\n",
      "4052\n",
      "4053 0\n",
      "4053\n",
      "4054 0\n",
      "4054\n",
      "4055 0\n",
      "4055\n",
      "4056 0\n",
      "4056\n",
      "4057 0\n",
      "4057\n",
      "4058 0\n",
      "4058\n",
      "4059 0\n",
      "4059\n",
      "4060 0\n",
      "4060\n",
      "4061 0\n",
      "4061\n",
      "4062 0\n",
      "4062\n",
      "4063 0\n",
      "4063\n",
      "4064 0\n",
      "4064\n",
      "4065 0\n",
      "4065\n",
      "4066 0\n",
      "4066\n",
      "4067 0\n",
      "4067\n",
      "4068 0\n",
      "4068\n",
      "4069 0\n",
      "4069\n",
      "4070 0\n",
      "4070\n",
      "4071 0\n",
      "4071\n",
      "4072 0\n",
      "4072\n",
      "4073 0\n",
      "4073\n",
      "4074 0\n",
      "4074\n",
      "4075 0\n",
      "4075\n",
      "4076 0\n",
      "4076\n",
      "4077 0\n",
      "4077\n",
      "4078 0\n",
      "4078\n",
      "4079 0\n",
      "4079\n",
      "4080 0\n",
      "4080\n",
      "4081 0\n",
      "4081\n",
      "4082 0\n",
      "4082\n",
      "4083 0\n",
      "4083\n",
      "4084 0\n",
      "4084\n",
      "4085 0\n",
      "4085\n",
      "4086 0\n",
      "4086\n",
      "4087 0\n",
      "4087\n",
      "4088 0\n",
      "4088\n",
      "4089 0\n",
      "4089\n",
      "4090 0\n",
      "4090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4091 0\n",
      "4091\n",
      "4092 0\n",
      "4092\n",
      "4093 0\n",
      "4093\n",
      "4094 0\n",
      "4094\n",
      "4095 0\n",
      "4095\n",
      "4096 0\n",
      "4096\n",
      "4097 0\n",
      "4097\n",
      "4098 0\n",
      "4098\n",
      "4099 0\n",
      "4099\n",
      "4100 0\n",
      "4100\n",
      "4101 0\n",
      "4101\n",
      "4102 0\n",
      "4102\n",
      "4103 0\n",
      "4103\n",
      "4104 0\n",
      "4104\n",
      "4105 0\n",
      "4105\n",
      "4106 0\n",
      "4106\n",
      "4107 0\n",
      "4107\n",
      "4108 0\n",
      "4108\n",
      "4109 0\n",
      "4109\n",
      "4110 0\n",
      "4110\n",
      "4111 0\n",
      "4111\n",
      "4112 0\n",
      "4112\n",
      "4113 0\n",
      "4113\n",
      "4114 0\n",
      "4114\n",
      "4115 0\n",
      "4115\n",
      "4116 0\n",
      "4116\n",
      "4117 0\n",
      "4117\n",
      "4118 0\n",
      "4118\n",
      "4119 0\n",
      "4119\n",
      "4120 0\n",
      "4120\n",
      "4121 0\n",
      "4121\n",
      "4122 0\n",
      "4122\n",
      "4123 0\n",
      "4123\n",
      "4124 0\n",
      "4124\n",
      "4125 0\n",
      "4125\n",
      "4126 0\n",
      "4126\n",
      "4127 0\n",
      "4127\n",
      "4128 0\n",
      "4128\n",
      "4129 0\n",
      "4129\n",
      "4130 0\n",
      "4130\n",
      "4131 0\n",
      "4131\n",
      "4132 0\n",
      "4132\n",
      "4133 0\n",
      "4133\n",
      "4134 0\n",
      "4134\n",
      "4135 0\n",
      "4135\n",
      "4136 0\n",
      "4136\n",
      "4137 0\n",
      "4137\n",
      "4138 0\n",
      "4138\n",
      "4139 0\n",
      "4139\n",
      "4140 0\n",
      "4140\n",
      "4141 0\n",
      "4141\n",
      "4142 0\n",
      "4142\n",
      "4143 0\n",
      "4143\n",
      "4144 0\n",
      "4144\n",
      "4145 0\n",
      "4145\n",
      "4146 0\n",
      "4146\n",
      "4147 0\n",
      "4147\n",
      "4148 0\n",
      "4148\n",
      "4149 0\n",
      "4149\n",
      "4150 0\n",
      "4150\n",
      "4151 0\n",
      "4151\n",
      "4152 0\n",
      "4152\n",
      "4153 0\n",
      "4153\n",
      "4154 0\n",
      "4154\n",
      "4155 0\n",
      "4155\n",
      "4156 0\n",
      "4156\n",
      "4157 0\n",
      "4157\n",
      "4158 0\n",
      "4158\n",
      "4159 0\n",
      "4159\n",
      "4160 0\n",
      "4160\n",
      "4161 0\n",
      "4161\n",
      "4162 0\n",
      "4162\n",
      "4163 0\n",
      "4163\n",
      "4164 0\n",
      "4164\n",
      "4165 0\n",
      "4165\n",
      "4166 0\n",
      "4166\n",
      "4167 0\n",
      "4167\n",
      "4168 0\n",
      "4168\n",
      "4169 0\n",
      "4169\n",
      "4170 0\n",
      "4170\n",
      "4171 0\n",
      "4171\n",
      "4172 0\n",
      "4172\n",
      "4173 0\n",
      "4173\n",
      "4174 0\n",
      "4174\n",
      "4175 0\n",
      "4175\n",
      "4176 0\n",
      "4176\n",
      "4177 0\n",
      "4177\n",
      "4178 0\n",
      "4178\n",
      "4179 0\n",
      "4179\n",
      "4180 0\n",
      "4180\n",
      "4181 0\n",
      "4181\n",
      "4182 0\n",
      "4182\n",
      "4183 0\n",
      "4183\n",
      "4184 0\n",
      "4184\n",
      "4185 0\n",
      "4185\n",
      "4186 0\n",
      "4186\n",
      "4187 0\n",
      "4187\n",
      "4188 0\n",
      "4188\n",
      "4189 0\n",
      "4189\n",
      "4190 0\n",
      "4190\n",
      "4191 0\n",
      "4191\n",
      "4192 0\n",
      "4192\n",
      "4193 0\n",
      "4193\n",
      "4194 0\n",
      "4194\n",
      "4195 0\n",
      "4195\n",
      "4196 0\n",
      "4196\n",
      "4197 0\n",
      "4197\n",
      "4198 0\n",
      "4198\n",
      "4199 0\n",
      "4199\n",
      "4200 0\n",
      "4200\n",
      "4201 0\n",
      "4201\n",
      "4202 0\n",
      "4202\n",
      "4203 0\n",
      "4203\n",
      "4204 0\n",
      "4204\n",
      "4205 0\n",
      "4205\n",
      "4206 0\n",
      "4206\n",
      "4207 0\n",
      "4207\n",
      "4208 0\n",
      "4208\n",
      "4209 0\n",
      "4209\n",
      "4210 0\n",
      "4210\n",
      "4211 0\n",
      "4211\n",
      "4212 0\n",
      "4212\n",
      "4213 0\n",
      "4213\n",
      "4214 0\n",
      "4214\n",
      "4215 0\n",
      "4215\n",
      "4216 0\n",
      "4216\n",
      "4217 0\n",
      "4217\n",
      "4218 0\n",
      "4218\n",
      "4219 0\n",
      "4219\n",
      "4220 0\n",
      "4220\n",
      "4221 0\n",
      "4221\n",
      "4222 0\n",
      "4222\n",
      "4223 0\n",
      "4223\n",
      "4224 0\n",
      "4224\n",
      "4225 0\n",
      "4225\n",
      "4226 0\n",
      "4226\n",
      "4227 0\n",
      "4227\n",
      "4228 0\n",
      "4228\n",
      "4229 0\n",
      "4229\n",
      "4230 0\n",
      "4230\n",
      "4231 0\n",
      "4231\n",
      "4232 0\n",
      "4232\n",
      "4233 0\n",
      "4233\n",
      "4234 0\n",
      "4234\n",
      "4235 0\n",
      "4235\n",
      "4236 0\n",
      "4236\n",
      "4237 0\n",
      "4237\n",
      "4238 0\n",
      "4238\n",
      "4239 0\n",
      "4239\n",
      "4240 0\n",
      "4240\n",
      "4241 0\n",
      "4241\n",
      "4242 0\n",
      "4242\n",
      "4243 0\n",
      "4243\n",
      "4244 0\n",
      "4244\n",
      "4245 0\n",
      "4245\n",
      "4246 0\n",
      "4246\n",
      "4247 0\n",
      "4247\n",
      "4248 0\n",
      "4248\n",
      "4249 0\n",
      "4249\n",
      "4250 0\n",
      "4250\n",
      "4251 0\n",
      "4251\n",
      "4252 0\n",
      "4252\n",
      "4253 0\n",
      "4253\n",
      "4254 0\n",
      "4254\n",
      "4255 0\n",
      "4255\n",
      "4256 0\n",
      "4256\n",
      "4257 0\n",
      "4257\n",
      "4258 0\n",
      "4258\n",
      "4259 0\n",
      "4259\n",
      "4260 0\n",
      "4260\n",
      "4261 0\n",
      "4261\n",
      "4262 0\n",
      "4262\n",
      "4263 0\n",
      "4263\n",
      "4264 0\n",
      "4264\n",
      "4265 0\n",
      "4265\n",
      "4266 0\n",
      "4266\n",
      "4267 0\n",
      "4267\n",
      "4268 0\n",
      "4268\n",
      "4269 0\n",
      "4269\n",
      "4270 0\n",
      "4270\n",
      "4271 0\n",
      "4271\n",
      "4272 0\n",
      "4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273 0\n",
      "4273\n",
      "4274 0\n",
      "4274\n",
      "4275 0\n",
      "4275\n",
      "4276 0\n",
      "4276\n",
      "4277 0\n",
      "4277\n",
      "4278 0\n",
      "4278\n",
      "4279 0\n",
      "4279\n",
      "4280 0\n",
      "4280\n",
      "4281 0\n",
      "4281\n",
      "4282 0\n",
      "4282\n",
      "4283 0\n",
      "4283\n",
      "4284 0\n",
      "4284\n",
      "4285 0\n",
      "4285\n",
      "4286 0\n",
      "4286\n",
      "4287 0\n",
      "4287\n",
      "4288 0\n",
      "4288\n",
      "4289 0\n",
      "4289\n",
      "4290 0\n",
      "4290\n",
      "4291 0\n",
      "4291\n",
      "4292 0\n",
      "4292\n",
      "4293 0\n",
      "4293\n",
      "4294 0\n",
      "4294\n",
      "4295 0\n",
      "4295\n",
      "4296 0\n",
      "4296\n",
      "4297 0\n",
      "4297\n",
      "4298 0\n",
      "4298\n",
      "4299 0\n",
      "4299\n",
      "4300 0\n",
      "4300\n",
      "4301 0\n",
      "4301\n",
      "4302 0\n",
      "4302\n",
      "4303 0\n",
      "4303\n",
      "4304 0\n",
      "4304\n",
      "4305 0\n",
      "4305\n",
      "4306 0\n",
      "4306\n",
      "4307 0\n",
      "4307\n",
      "4308 0\n",
      "4308\n",
      "4309 0\n",
      "4309\n",
      "4310 0\n",
      "4310\n",
      "4311 0\n",
      "4311\n",
      "4312 0\n",
      "4312\n",
      "4313 0\n",
      "4313\n",
      "4314 0\n",
      "4314\n",
      "4315 0\n",
      "4315\n",
      "4316 0\n",
      "4316\n",
      "4317 0\n",
      "4317\n",
      "4318 0\n",
      "4318\n",
      "4319 0\n",
      "4319\n",
      "4320 0\n",
      "4320\n",
      "4321 0\n",
      "4321\n",
      "4322 0\n",
      "4322\n",
      "4323 0\n",
      "4323\n",
      "4324 0\n",
      "4324\n",
      "4325 0\n",
      "4325\n",
      "4326 0\n",
      "4326\n",
      "4327 0\n",
      "4327\n",
      "4328 0\n",
      "4328\n",
      "4329 0\n",
      "4329\n",
      "4330 0\n",
      "4330\n",
      "4331 0\n",
      "4331\n",
      "4332 0\n",
      "4332\n",
      "4333 0\n",
      "4333\n",
      "4334 0\n",
      "4334\n",
      "4335 0\n",
      "4335\n",
      "4336 0\n",
      "4336\n",
      "4337 0\n",
      "4337\n",
      "4338 0\n",
      "4338\n",
      "4339 0\n",
      "4339\n",
      "4340 0\n",
      "4340\n",
      "4341 0\n",
      "4341\n",
      "4342 0\n",
      "4342\n",
      "4343 0\n",
      "4343\n",
      "4344 0\n",
      "4344\n",
      "4345 0\n",
      "4345\n",
      "4346 0\n",
      "4346\n",
      "4347 0\n",
      "4347\n",
      "4348 0\n",
      "4348\n",
      "4349 0\n",
      "4349\n",
      "4350 0\n",
      "4350\n",
      "4351 0\n",
      "4351\n",
      "4352 0\n",
      "4352\n",
      "4353 0\n",
      "4353\n",
      "4354 0\n",
      "4354\n",
      "4355 0\n",
      "4355\n",
      "4356 0\n",
      "4356\n",
      "4357 0\n",
      "4357\n",
      "4358 0\n",
      "4358\n",
      "4359 0\n",
      "4359\n",
      "4360 0\n",
      "4360\n",
      "4361 0\n",
      "4361\n",
      "4362 0\n",
      "4362\n",
      "4363 0\n",
      "4363\n",
      "4364 0\n",
      "4364\n",
      "4365 0\n",
      "4365\n",
      "4366 0\n",
      "4366\n",
      "4367 0\n",
      "4367\n",
      "4368 0\n",
      "4368\n",
      "4369 0\n",
      "4369\n",
      "4370 0\n",
      "4370\n",
      "4371 0\n",
      "4371\n",
      "4372 0\n",
      "4372\n",
      "4373 0\n",
      "4373\n",
      "4374 0\n",
      "4374\n",
      "4375 0\n",
      "4375\n",
      "4376 0\n",
      "4376\n",
      "4377 0\n",
      "4377\n",
      "4378 0\n",
      "4378\n",
      "4379 0\n",
      "4379\n",
      "4380 0\n",
      "4380\n",
      "4381 0\n",
      "4381\n",
      "4382 0\n",
      "4382\n",
      "4383 0\n",
      "4383\n",
      "4384 0\n",
      "4384\n",
      "4385 0\n",
      "4385\n",
      "4386 0\n",
      "4386\n",
      "4387 0\n",
      "4387\n",
      "4388 0\n",
      "4388\n",
      "4389 0\n",
      "4389\n",
      "4390 0\n",
      "4390\n",
      "4391 0\n",
      "4391\n",
      "4392 0\n",
      "4392\n",
      "4393 0\n",
      "4393\n",
      "4394 0\n",
      "4394\n",
      "4395 0\n",
      "4395\n",
      "4396 0\n",
      "4396\n",
      "4397 0\n",
      "4397\n",
      "4398 0\n",
      "4398\n",
      "4399 0\n",
      "4399\n",
      "4400 0\n",
      "4400\n",
      "4401 0\n",
      "4401\n",
      "4402 0\n",
      "4402\n",
      "4403 0\n",
      "4403\n",
      "4404 0\n",
      "4404\n",
      "4405 0\n",
      "4405\n",
      "4406 0\n",
      "4406\n",
      "4407 0\n",
      "4407\n",
      "4408 0\n",
      "4408\n",
      "4409 0\n",
      "4409\n",
      "4410 0\n",
      "4410\n",
      "4411 0\n",
      "4411\n",
      "4412 0\n",
      "4412\n",
      "4413 0\n",
      "4413\n",
      "4414 0\n",
      "4414\n",
      "4415 0\n",
      "4415\n",
      "4416 0\n",
      "4416\n",
      "4417 0\n",
      "4417\n",
      "4418 0\n",
      "4418\n",
      "4419 0\n",
      "4419\n",
      "4420 0\n",
      "4420\n",
      "4421 0\n",
      "4421\n",
      "4422 0\n",
      "4422\n",
      "4423 0\n",
      "4423\n",
      "4424 0\n",
      "4424\n",
      "4425 0\n",
      "4425\n",
      "4426 0\n",
      "4426\n",
      "4427 0\n",
      "4427\n",
      "4428 0\n",
      "4428\n",
      "4429 0\n",
      "4429\n",
      "4430 0\n",
      "4430\n",
      "4431 0\n",
      "4431\n",
      "4432 0\n",
      "4432\n",
      "4433 0\n",
      "4433\n",
      "4434 0\n",
      "4434\n",
      "4435 0\n",
      "4435\n",
      "4436 0\n",
      "4436\n",
      "4437 0\n",
      "4437\n",
      "4438 0\n",
      "4438\n",
      "4439 0\n",
      "4439\n",
      "4440 0\n",
      "4440\n",
      "4441 0\n",
      "4441\n",
      "4442 0\n",
      "4442\n",
      "4443 0\n",
      "4443\n",
      "4444 0\n",
      "4444\n",
      "4445 0\n",
      "4445\n",
      "4446 0\n",
      "4446\n",
      "4447 0\n",
      "4447\n",
      "4448 0\n",
      "4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4449 0\n",
      "4449\n",
      "4450 0\n",
      "4450\n",
      "4451 0\n",
      "4451\n",
      "4452 0\n",
      "4452\n",
      "4453 0\n",
      "4453\n",
      "4454 0\n",
      "4454\n",
      "4455 0\n",
      "4455\n",
      "4456 0\n",
      "4456\n",
      "4457 0\n",
      "4457\n",
      "4458 0\n",
      "4458\n",
      "4459 0\n",
      "4459\n",
      "4460 0\n",
      "4460\n",
      "4461 0\n",
      "4461\n",
      "4462 0\n",
      "4462\n",
      "4463 0\n",
      "4463\n",
      "4464 0\n",
      "4464\n",
      "4465 0\n",
      "4465\n",
      "4466 0\n",
      "4466\n",
      "4467 0\n",
      "4467\n",
      "4468 0\n",
      "4468\n",
      "4469 0\n",
      "4469\n",
      "4470 0\n",
      "4470\n",
      "4471 0\n",
      "4471\n",
      "4472 0\n",
      "4472\n",
      "4473 0\n",
      "4473\n",
      "4474 0\n",
      "4474\n",
      "4475 0\n",
      "4475\n",
      "4476 0\n",
      "4476\n",
      "4477 0\n",
      "4477\n",
      "4478 0\n",
      "4478\n",
      "4479 0\n",
      "4479\n",
      "4480 0\n",
      "4480\n",
      "4481 0\n",
      "4481\n",
      "4482 0\n",
      "4482\n",
      "4483 0\n",
      "4483\n",
      "4484 0\n",
      "4484\n",
      "4485 0\n",
      "4485\n",
      "4486 0\n",
      "4486\n",
      "4487 0\n",
      "4487\n",
      "4488 0\n",
      "4488\n",
      "4489 0\n",
      "4489\n",
      "4490 0\n",
      "4490\n",
      "4491 0\n",
      "4491\n",
      "4492 0\n",
      "4492\n",
      "4493 0\n",
      "4493\n",
      "4494 0\n",
      "4494\n",
      "4495 0\n",
      "4495\n",
      "4496 0\n",
      "4496\n",
      "4497 0\n",
      "4497\n",
      "4498 0\n",
      "4498\n",
      "4499 0\n",
      "4499\n",
      "4500 0\n",
      "4500\n",
      "4501 0\n",
      "4501\n",
      "4502 0\n",
      "4502\n",
      "4503 0\n",
      "4503\n",
      "4504 0\n",
      "4504\n",
      "4505 0\n",
      "4505\n",
      "4506 0\n",
      "4506\n",
      "4507 0\n",
      "4507\n",
      "4508 0\n",
      "4508\n",
      "4509 0\n",
      "4509\n",
      "4510 0\n",
      "4510\n",
      "4511 0\n",
      "4511\n",
      "4512 0\n",
      "4512\n",
      "4513 0\n",
      "4513\n",
      "4514 0\n",
      "4514\n",
      "4515 0\n",
      "4515\n",
      "4516 0\n",
      "4516\n",
      "4517 0\n",
      "4517\n",
      "4518 0\n",
      "4518\n",
      "4519 0\n",
      "4519\n",
      "4520 0\n",
      "4520\n",
      "4521 0\n",
      "4521\n",
      "4522 0\n",
      "4522\n",
      "4523 0\n",
      "4523\n",
      "4524 0\n",
      "4524\n",
      "4525 0\n",
      "4525\n",
      "4526 0\n",
      "4526\n",
      "4527 0\n",
      "4527\n",
      "4528 0\n",
      "4528\n",
      "4529 0\n",
      "4529\n",
      "4530 0\n",
      "4530\n",
      "4531 0\n",
      "4531\n",
      "4532 0\n",
      "4532\n",
      "4533 0\n",
      "4533\n",
      "4534 0\n",
      "4534\n",
      "4535 0\n",
      "4535\n",
      "4536 0\n",
      "4536\n",
      "4537 0\n",
      "4537\n",
      "4538 0\n",
      "4538\n",
      "4539 0\n",
      "4539\n",
      "4540 0\n",
      "4540\n",
      "4541 0\n",
      "4541\n",
      "4542 0\n",
      "4542\n",
      "4543 0\n",
      "4543\n",
      "4544 0\n",
      "4544\n",
      "4545 0\n",
      "4545\n",
      "4546 0\n",
      "4546\n",
      "4547 0\n",
      "4547\n",
      "4548 0\n",
      "4548\n",
      "4549 0\n",
      "4549\n",
      "4550 0\n",
      "4550\n",
      "4551 0\n",
      "4551\n",
      "4552 0\n",
      "4552\n",
      "4553 0\n",
      "4553\n",
      "4554 0\n",
      "4554\n",
      "4555 0\n",
      "4555\n",
      "4556 0\n",
      "4556\n",
      "4557 0\n",
      "4557\n",
      "4558 0\n",
      "4558\n",
      "4559 0\n",
      "4559\n",
      "4560 0\n",
      "4560\n",
      "4561 0\n",
      "4561\n",
      "4562 0\n",
      "4562\n",
      "4563 0\n",
      "4563\n",
      "4564 0\n",
      "4564\n",
      "4565 0\n",
      "4565\n",
      "4566 0\n",
      "4566\n",
      "4567 0\n",
      "4567\n",
      "4568 0\n",
      "4568\n",
      "4569 0\n",
      "4569\n",
      "4570 0\n",
      "4570\n",
      "4571 0\n",
      "4571\n",
      "4572 0\n",
      "4572\n",
      "4573 0\n",
      "4573\n",
      "4574 0\n",
      "4574\n",
      "4575 0\n",
      "4575\n",
      "4576 0\n",
      "4576\n",
      "4577 0\n",
      "4577\n",
      "4578 0\n",
      "4578\n",
      "4579 0\n",
      "4579\n",
      "4580 0\n",
      "4580\n",
      "4581 0\n",
      "4581\n",
      "4582 0\n",
      "4582\n",
      "4583 0\n",
      "4583\n",
      "4584 0\n",
      "4584\n",
      "4585 0\n",
      "4585\n",
      "4586 0\n",
      "4586\n",
      "4587 0\n",
      "4587\n",
      "4588 0\n",
      "4588\n",
      "4589 0\n",
      "4589\n",
      "4590 0\n",
      "4590\n",
      "4591 0\n",
      "4591\n",
      "4592 0\n",
      "4592\n",
      "4593 0\n",
      "4593\n",
      "4594 0\n",
      "4594\n",
      "4595 0\n",
      "4595\n",
      "4596 0\n",
      "4596\n",
      "4597 0\n",
      "4597\n",
      "4598 0\n",
      "4598\n",
      "4599 0\n",
      "4599\n",
      "4600 0\n",
      "4600\n",
      "4601 0\n",
      "4601\n",
      "4602 0\n",
      "4602\n",
      "4603 0\n",
      "4603\n",
      "4604 0\n",
      "4604\n",
      "4605 0\n",
      "4605\n",
      "4606 0\n",
      "4606\n",
      "4607 0\n",
      "4607\n",
      "4608 0\n",
      "4608\n",
      "4609 0\n",
      "4609\n",
      "4610 0\n",
      "4610\n",
      "4611 0\n",
      "4611\n",
      "4612 0\n",
      "4612\n",
      "4613 0\n",
      "4613\n",
      "4614 0\n",
      "4614\n",
      "4615 0\n",
      "4615\n",
      "4616 0\n",
      "4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4617 0\n",
      "4617\n",
      "4618 0\n",
      "4618\n",
      "4619 0\n",
      "4619\n",
      "4620 0\n",
      "4620\n",
      "4621 0\n",
      "4621\n",
      "4622 0\n",
      "4622\n",
      "4623 0\n",
      "4623\n",
      "4624 0\n",
      "4624\n",
      "4625 0\n",
      "4625\n",
      "4626 0\n",
      "4626\n",
      "4627 0\n",
      "4627\n",
      "4628 0\n",
      "4628\n",
      "4629 0\n",
      "4629\n",
      "4630 0\n",
      "4630\n",
      "4631 0\n",
      "4631\n",
      "4632 0\n",
      "4632\n",
      "4633 0\n",
      "4633\n",
      "4634 0\n",
      "4634\n",
      "4635 0\n",
      "4635\n",
      "4636 0\n",
      "4636\n",
      "4637 0\n",
      "4637\n",
      "4638 0\n",
      "4638\n",
      "4639 0\n",
      "4639\n",
      "4640 0\n",
      "4640\n",
      "4641 0\n",
      "4641\n",
      "4642 0\n",
      "4642\n",
      "4643 0\n",
      "4643\n",
      "4644 0\n",
      "4644\n",
      "4645 0\n",
      "4645\n",
      "4646 0\n",
      "4646\n",
      "4647 0\n",
      "4647\n",
      "4648 0\n",
      "4648\n",
      "4649 0\n",
      "4649\n",
      "4650 0\n",
      "4650\n",
      "4651 0\n",
      "4651\n",
      "4652 0\n",
      "4652\n",
      "4653 0\n",
      "4653\n",
      "4654 0\n",
      "4654\n",
      "4655 0\n",
      "4655\n",
      "4656 0\n",
      "4656\n",
      "4657 0\n",
      "4657\n",
      "4658 0\n",
      "4658\n",
      "4659 0\n",
      "4659\n",
      "4660 0\n",
      "4660\n",
      "4661 0\n",
      "4661\n",
      "4662 0\n",
      "4662\n",
      "4663 0\n",
      "4663\n",
      "4664 0\n",
      "4664\n",
      "4665 0\n",
      "4665\n",
      "4666 0\n",
      "4666\n",
      "4667 0\n",
      "4667\n",
      "4668 0\n",
      "4668\n",
      "4669 0\n",
      "4669\n",
      "4670 0\n",
      "4670\n",
      "4671 0\n",
      "4671\n",
      "4672 0\n",
      "4672\n",
      "4673 0\n",
      "4673\n",
      "4674 0\n",
      "4674\n",
      "4675 0\n",
      "4675\n",
      "4676 0\n",
      "4676\n",
      "4677 0\n",
      "4677\n",
      "4678 0\n",
      "4678\n",
      "4679 0\n",
      "4679\n",
      "4680 0\n",
      "4680\n",
      "4681 0\n",
      "4681\n",
      "4682 0\n",
      "4682\n",
      "4683 0\n",
      "4683\n",
      "4684 0\n",
      "4684\n",
      "4685 0\n",
      "4685\n",
      "4686 0\n",
      "4686\n",
      "4687 0\n",
      "4687\n",
      "4688 0\n",
      "4688\n",
      "4689 0\n",
      "4689\n",
      "4690 0\n",
      "4690\n",
      "4691 0\n",
      "4691\n",
      "4692 0\n",
      "4692\n",
      "4693 0\n",
      "4693\n",
      "4694 0\n",
      "4694\n",
      "4695 0\n",
      "4695\n",
      "4696 0\n",
      "4696\n",
      "4697 0\n",
      "4697\n",
      "4698 0\n",
      "4698\n",
      "4699 0\n",
      "4699\n",
      "4700 0\n",
      "4700\n",
      "4701 0\n",
      "4701\n",
      "4702 0\n",
      "4702\n",
      "4703 0\n",
      "4703\n",
      "4704 0\n",
      "4704\n",
      "4705 0\n",
      "4705\n",
      "4706 0\n",
      "4706\n",
      "4707 0\n",
      "4707\n",
      "4708 0\n",
      "4708\n",
      "4709 0\n",
      "4709\n",
      "4710 0\n",
      "4710\n",
      "4711 0\n",
      "4711\n",
      "4712 0\n",
      "4712\n",
      "4713 0\n",
      "4713\n",
      "4714 0\n",
      "4714\n",
      "4715 0\n",
      "4715\n",
      "4716 0\n",
      "4716\n",
      "4717 0\n",
      "4717\n",
      "4718 0\n",
      "4718\n",
      "4719 0\n",
      "4719\n",
      "4720 0\n",
      "4720\n",
      "4721 0\n",
      "4721\n",
      "4722 0\n",
      "4722\n",
      "4723 0\n",
      "4723\n",
      "4724 0\n",
      "4724\n",
      "4725 0\n",
      "4725\n",
      "4726 0\n",
      "4726\n",
      "4727 0\n",
      "4727\n",
      "4728 0\n",
      "4728\n",
      "4729 0\n",
      "4729\n",
      "4730 0\n",
      "4730\n",
      "4731 0\n",
      "4731\n",
      "4732 0\n",
      "4732\n",
      "4733 0\n",
      "4733\n",
      "4734 0\n",
      "4734\n",
      "4735 0\n",
      "4735\n",
      "4736 0\n",
      "4736\n",
      "4737 0\n",
      "4737\n",
      "4738 0\n",
      "4738\n",
      "4739 0\n",
      "4739\n",
      "4740 0\n",
      "4740\n",
      "4741 0\n",
      "4741\n",
      "4742 0\n",
      "4742\n",
      "4743 0\n",
      "4743\n",
      "4744 0\n",
      "4744\n",
      "4745 0\n",
      "4745\n",
      "4746 0\n",
      "4746\n",
      "4747 0\n",
      "4747\n",
      "4748 0\n",
      "4748\n",
      "4749 0\n",
      "4749\n",
      "4750 0\n",
      "4750\n",
      "4751 0\n",
      "4751\n",
      "4752 0\n",
      "4752\n",
      "4753 0\n",
      "4753\n",
      "4754 0\n",
      "4754\n",
      "4755 0\n",
      "4755\n",
      "4756 0\n",
      "4756\n",
      "4757 0\n",
      "4757\n",
      "4758 0\n",
      "4758\n",
      "4759 0\n",
      "4759\n",
      "4760 0\n",
      "4760\n",
      "4761 0\n",
      "4761\n",
      "4762 0\n",
      "4762\n",
      "4763 0\n",
      "4763\n",
      "4764 0\n",
      "4764\n",
      "4765 0\n",
      "4765\n",
      "4766 0\n",
      "4766\n",
      "4767 0\n",
      "4767\n",
      "4768 0\n",
      "4768\n",
      "4769 0\n",
      "4769\n",
      "4770 0\n",
      "4770\n",
      "4771 0\n",
      "4771\n",
      "4772 0\n",
      "4772\n",
      "4773 0\n",
      "4773\n",
      "4774 0\n",
      "4774\n",
      "4775 0\n",
      "4775\n",
      "4776 0\n",
      "4776\n",
      "4777 0\n",
      "4777\n",
      "4778 0\n",
      "4778\n",
      "4779 0\n",
      "4779\n",
      "4780 0\n",
      "4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4781 0\n",
      "4781\n",
      "4782 0\n",
      "4782\n",
      "4783 0\n",
      "4783\n",
      "4784 0\n",
      "4784\n",
      "4785 0\n",
      "4785\n",
      "4786 0\n",
      "4786\n",
      "4787 0\n",
      "4787\n",
      "4788 0\n",
      "4788\n",
      "4789 0\n",
      "4789\n",
      "4790 0\n",
      "4790\n",
      "4791 0\n",
      "4791\n",
      "4792 0\n",
      "4792\n",
      "4793 0\n",
      "4793\n",
      "4794 0\n",
      "4794\n",
      "4795 0\n",
      "4795\n",
      "4796 0\n",
      "4796\n",
      "4797 0\n",
      "4797\n",
      "4798 0\n",
      "4798\n",
      "4799 0\n",
      "4799\n",
      "4800 0\n",
      "4800\n",
      "4801 0\n",
      "4801\n",
      "4802 0\n",
      "4802\n",
      "4803 0\n",
      "4803\n",
      "4804 0\n",
      "4804\n",
      "4805 0\n",
      "4805\n",
      "4806 0\n",
      "4806\n",
      "4807 0\n",
      "4807\n",
      "4808 0\n",
      "4808\n",
      "4809 0\n",
      "4809\n",
      "4810 0\n",
      "4810\n",
      "4811 0\n",
      "4811\n",
      "4812 0\n",
      "4812\n",
      "4813 0\n",
      "4813\n",
      "4814 0\n",
      "4814\n",
      "4815 0\n",
      "4815\n",
      "4816 0\n",
      "4816\n",
      "4817 0\n",
      "4817\n",
      "4818 0\n",
      "4818\n",
      "4819 0\n",
      "4819\n",
      "4820 0\n",
      "4820\n",
      "4821 0\n",
      "4821\n",
      "4822 0\n",
      "4822\n",
      "4823 0\n",
      "4823\n",
      "4824 0\n",
      "4824\n",
      "4825 0\n",
      "4825\n",
      "4826 0\n",
      "4826\n",
      "4827 0\n",
      "4827\n",
      "4828 0\n",
      "4828\n",
      "4829 0\n",
      "4829\n",
      "4830 0\n",
      "4830\n",
      "4831 0\n",
      "4831\n",
      "4832 0\n",
      "4832\n",
      "4833 0\n",
      "4833\n",
      "4834 0\n",
      "4834\n",
      "4835 0\n",
      "4835\n",
      "4836 0\n",
      "4836\n",
      "4837 0\n",
      "4837\n",
      "4838 0\n",
      "4838\n",
      "4839 0\n",
      "4839\n",
      "4840 0\n",
      "4840\n",
      "4841 0\n",
      "4841\n",
      "4842 0\n",
      "4842\n",
      "4843 0\n",
      "4843\n",
      "4844 0\n",
      "4844\n",
      "4845 0\n",
      "4845\n",
      "4846 0\n",
      "4846\n",
      "4847 0\n",
      "4847\n",
      "4848 0\n",
      "4848\n",
      "4849 0\n",
      "4849\n",
      "4850 0\n",
      "4850\n",
      "4851 0\n",
      "4851\n",
      "4852 0\n",
      "4852\n",
      "4853 0\n",
      "4853\n",
      "4854 0\n",
      "4854\n",
      "4855 0\n",
      "4855\n",
      "4856 0\n",
      "4856\n",
      "4857 0\n",
      "4857\n",
      "4858 0\n",
      "4858\n",
      "4859 0\n",
      "4859\n",
      "4860 0\n",
      "4860\n",
      "4861 0\n",
      "4861\n",
      "4862 0\n",
      "4862\n",
      "4863 0\n",
      "4863\n",
      "4864 0\n",
      "4864\n",
      "4865 0\n",
      "4865\n",
      "4866 0\n",
      "4866\n",
      "4867 0\n",
      "4867\n",
      "4868 0\n",
      "4868\n",
      "4869 0\n",
      "4869\n",
      "4870 0\n",
      "4870\n",
      "4871 0\n",
      "4871\n",
      "4872 0\n",
      "4872\n",
      "4873 0\n",
      "4873\n",
      "4874 0\n",
      "4874\n",
      "4875 0\n",
      "4875\n",
      "4876 0\n",
      "4876\n",
      "4877 0\n",
      "4877\n",
      "4878 0\n",
      "4878\n",
      "4879 0\n",
      "4879\n",
      "4880 0\n",
      "4880\n",
      "4881 0\n",
      "4881\n",
      "4882 0\n",
      "4882\n",
      "4883 0\n",
      "4883\n",
      "4884 0\n",
      "4884\n",
      "4885 0\n",
      "4885\n",
      "4886 0\n",
      "4886\n",
      "4887 0\n",
      "4887\n",
      "4888 0\n",
      "4888\n",
      "4889 0\n",
      "4889\n",
      "4890 0\n",
      "4890\n",
      "4891 0\n",
      "4891\n",
      "4892 0\n",
      "4892\n",
      "4893 0\n",
      "4893\n",
      "4894 0\n",
      "4894\n",
      "4895 0\n",
      "4895\n",
      "4896 0\n",
      "4896\n",
      "4897 0\n",
      "4897\n",
      "4898 0\n",
      "4898\n",
      "4899 0\n",
      "4899\n",
      "4900 0\n",
      "4900\n",
      "4901 0\n",
      "4901\n",
      "4902 0\n",
      "4902\n",
      "4903 0\n",
      "4903\n",
      "4904 0\n",
      "4904\n",
      "4905 0\n",
      "4905\n",
      "4906 0\n",
      "4906\n",
      "4907 0\n",
      "4907\n",
      "4908 0\n",
      "4908\n",
      "4909 0\n",
      "4909\n",
      "4910 0\n",
      "4910\n",
      "4911 0\n",
      "4911\n",
      "4912 0\n",
      "4912\n",
      "4913 0\n",
      "4913\n",
      "4914 0\n",
      "4914\n",
      "4915 0\n",
      "4915\n",
      "4916 0\n",
      "4916\n",
      "4917 0\n",
      "4917\n",
      "4918 0\n",
      "4918\n",
      "4919 0\n",
      "4919\n",
      "4920 0\n",
      "4920\n",
      "4921 0\n",
      "4921\n",
      "4922 0\n",
      "4922\n",
      "4923 0\n",
      "4923\n",
      "4924 0\n",
      "4924\n",
      "4925 0\n",
      "4925\n",
      "4926 0\n",
      "4926\n",
      "4927 0\n",
      "4927\n",
      "4928 0\n",
      "4928\n",
      "4929 0\n",
      "4929\n",
      "4930 0\n",
      "4930\n",
      "4931 0\n",
      "4931\n",
      "4932 0\n",
      "4932\n",
      "4933 0\n",
      "4933\n",
      "4934 0\n",
      "4934\n",
      "4935 0\n",
      "4935\n",
      "4936 0\n",
      "4936\n",
      "4937 0\n",
      "4937\n",
      "4938 0\n",
      "4938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4939 0\n",
      "4939\n",
      "4940 0\n",
      "4940\n",
      "4941 0\n",
      "4941\n",
      "4942 0\n",
      "4942\n",
      "4943 0\n",
      "4943\n",
      "4944 0\n",
      "4944\n",
      "4945 0\n",
      "4945\n",
      "4946 0\n",
      "4946\n",
      "4947 0\n",
      "4947\n",
      "4948 0\n",
      "4948\n",
      "4949 0\n",
      "4949\n",
      "4950 0\n",
      "4950\n",
      "4951 0\n",
      "4951\n",
      "4952 0\n",
      "4952\n",
      "4953 0\n",
      "4953\n",
      "4954 0\n",
      "4954\n",
      "4955 0\n",
      "4955\n",
      "4956 0\n",
      "4956\n",
      "4957 0\n",
      "4957\n",
      "4958 0\n",
      "4958\n",
      "4959 0\n",
      "4959\n",
      "4960 0\n",
      "4960\n",
      "4961 0\n",
      "4961\n",
      "4962 0\n",
      "4962\n",
      "4963 0\n",
      "4963\n",
      "4964 0\n",
      "4964\n",
      "4965 0\n",
      "4965\n",
      "4966 0\n",
      "4966\n",
      "4967 0\n",
      "4967\n",
      "4968 0\n",
      "4968\n",
      "4969 0\n",
      "4969\n",
      "4970 0\n",
      "4970\n",
      "4971 0\n",
      "4971\n",
      "4972 0\n",
      "4972\n",
      "4973 0\n",
      "4973\n",
      "4974 0\n",
      "4974\n",
      "4975 0\n",
      "4975\n",
      "4976 0\n",
      "4976\n",
      "4977 0\n",
      "4977\n",
      "4978 0\n",
      "4978\n",
      "4979 0\n",
      "4979\n",
      "4980 0\n",
      "4980\n",
      "4981 0\n",
      "4981\n",
      "4982 0\n",
      "4982\n",
      "4983 0\n",
      "4983\n",
      "4984 0\n",
      "4984\n",
      "4985 0\n",
      "4985\n",
      "4986 0\n",
      "4986\n",
      "4987 0\n",
      "4987\n",
      "4988 0\n",
      "4988\n",
      "4989 0\n",
      "4989\n",
      "4990 0\n",
      "4990\n",
      "4991 0\n",
      "4991\n",
      "4992 0\n",
      "4992\n",
      "4993 0\n",
      "4993\n",
      "4994 0\n",
      "4994\n",
      "4995 0\n",
      "4995\n",
      "4996 0\n",
      "4996\n",
      "4997 0\n",
      "4997\n",
      "4998 0\n",
      "4998\n",
      "4999 0\n",
      "4999\n",
      "5000 0\n",
      "5000\n",
      "5001 0\n",
      "5001\n",
      "5002 0\n",
      "5002\n",
      "5003 0\n",
      "5003\n",
      "5004 0\n",
      "5004\n",
      "5005 0\n",
      "5005\n",
      "5006 0\n",
      "5006\n",
      "5007 0\n",
      "5007\n",
      "5008 0\n",
      "5008\n",
      "5009 0\n",
      "5009\n",
      "5010 0\n",
      "5010\n",
      "5011 0\n",
      "5011\n",
      "5012 0\n",
      "5012\n",
      "5013 0\n",
      "5013\n",
      "5014 0\n",
      "5014\n",
      "5015 0\n",
      "5015\n",
      "5016 0\n",
      "5016\n",
      "5017 0\n",
      "5017\n",
      "5018 0\n",
      "5018\n",
      "5019 0\n",
      "5019\n",
      "5020 0\n",
      "5020\n",
      "5021 0\n",
      "5021\n",
      "5022 0\n",
      "5022\n",
      "5023 0\n",
      "5023\n",
      "5024 0\n",
      "5024\n",
      "5025 0\n",
      "5025\n",
      "5026 0\n",
      "5026\n",
      "5027 0\n",
      "5027\n",
      "5028 0\n",
      "5028\n",
      "5029 0\n",
      "5029\n",
      "5030 0\n",
      "5030\n",
      "5031 0\n",
      "5031\n",
      "5032 0\n",
      "5032\n",
      "5033 0\n",
      "5033\n",
      "5034 0\n",
      "5034\n",
      "5035 0\n",
      "5035\n",
      "5036 0\n",
      "5036\n",
      "5037 0\n",
      "5037\n",
      "5038 0\n",
      "5038\n",
      "5039 0\n",
      "5039\n",
      "5040 0\n",
      "5040\n",
      "5041 0\n",
      "5041\n",
      "5042 0\n",
      "5042\n",
      "5043 0\n",
      "5043\n",
      "5044 0\n",
      "5044\n",
      "5045 0\n",
      "5045\n",
      "5046 0\n",
      "5046\n",
      "5047 0\n",
      "5047\n",
      "5048 0\n",
      "5048\n",
      "5049 0\n",
      "5049\n",
      "5050 0\n",
      "5050\n",
      "5051 0\n",
      "5051\n",
      "5052 0\n",
      "5052\n",
      "5053 0\n",
      "5053\n",
      "5054 0\n",
      "5054\n",
      "5055 0\n",
      "5055\n",
      "5056 0\n",
      "5056\n",
      "5057 0\n",
      "5057\n",
      "5058 0\n",
      "5058\n",
      "5059 0\n",
      "5059\n",
      "5060 0\n",
      "5060\n",
      "5061 0\n",
      "5061\n",
      "5062 0\n",
      "5062\n",
      "5063 0\n",
      "5063\n",
      "5064 0\n",
      "5064\n",
      "5065 0\n",
      "5065\n",
      "5066 0\n",
      "5066\n",
      "5067 0\n",
      "5067\n",
      "5068 0\n",
      "5068\n",
      "5069 0\n",
      "5069\n",
      "5070 0\n",
      "5070\n",
      "5071 0\n",
      "5071\n",
      "5072 0\n",
      "5072\n",
      "5073 0\n",
      "5073\n",
      "5074 0\n",
      "5074\n",
      "5075 0\n",
      "5075\n",
      "5076 0\n",
      "5076\n",
      "5077 0\n",
      "5077\n",
      "5078 0\n",
      "5078\n",
      "5079 0\n",
      "5079\n",
      "5080 0\n",
      "5080\n",
      "5081 0\n",
      "5081\n",
      "5082 0\n",
      "5082\n",
      "5083 0\n",
      "5083\n",
      "5084 0\n",
      "5084\n",
      "5085 0\n",
      "5085\n",
      "5086 0\n",
      "5086\n",
      "5087 0\n",
      "5087\n",
      "5088 0\n",
      "5088\n",
      "5089 0\n",
      "5089\n",
      "5090 0\n",
      "5090\n",
      "5091 0\n",
      "5091\n",
      "5092 0\n",
      "5092\n",
      "5093 0\n",
      "5093\n",
      "5094 0\n",
      "5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n",
      "/tmp/ipykernel_1553722/2428675678.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_temp[i, j] = 0 ## 현재 자리 0으로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095 0\n",
      "5095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: (5096, 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/frame.py:4490\u001b[39m, in \u001b[36mDataFrame._set_item_mgr\u001b[39m\u001b[34m(self, key, value, refs)\u001b[39m\n\u001b[32m   4489\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4490\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4491\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   4492\u001b[39m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: (5096, 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m) :\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dup.iloc[i, j] != \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[43mdf_temp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m = \u001b[32m0\u001b[39m \u001b[38;5;66;03m## 현재 자리 0으로\u001b[39;00m\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[32m      6\u001b[39m             df_temp.iloc[i, dup.iloc[i, j][\u001b[32m0\u001b[39m]] = dup.iloc[i, j][\u001b[32m1\u001b[39m] \u001b[38;5;66;03m## 원래 자리 확률값으로\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/frame.py:4316\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4313\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4315\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4316\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/frame.py:4543\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4540\u001b[39m             value = np.tile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece.columns), \u001b[32m1\u001b[39m)).T\n\u001b[32m   4541\u001b[39m             refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4543\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/frame.py:4493\u001b[39m, in \u001b[36mDataFrame._set_item_mgr\u001b[39m\u001b[34m(self, key, value, refs)\u001b[39m\n\u001b[32m   4490\u001b[39m     loc = \u001b[38;5;28mself\u001b[39m._info_axis.get_loc(key)\n\u001b[32m   4491\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   4492\u001b[39m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4493\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4494\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4495\u001b[39m     \u001b[38;5;28mself\u001b[39m._iset_item_mgr(loc, value, refs=refs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/internals/managers.py:1392\u001b[39m, in \u001b[36mBlockManager.insert\u001b[39m\u001b[34m(self, loc, item, value, refs)\u001b[39m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks += (block,)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_extension\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m100\u001b[39m:\n\u001b[32m   1393\u001b[39m     warnings.warn(\n\u001b[32m   1394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDataFrame is highly fragmented.  This is usually the result \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof calling `frame.insert` many times, which has poor performance.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1400\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/NLP/lib/python3.12/site-packages/pandas/core/internals/managers.py:1392\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks += (block,)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m block.is_extension \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks) > \u001b[32m100\u001b[39m:\n\u001b[32m   1393\u001b[39m     warnings.warn(\n\u001b[32m   1394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDataFrame is highly fragmented.  This is usually the result \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof calling `frame.insert` many times, which has poor performance.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1400\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(df_temp)) :\n",
    "    for j in range(5) :\n",
    "        if dup.iloc[i, j] != None :\n",
    "            df_temp[i, j] = 0 ## 현재 자리 0으로\n",
    "            try :\n",
    "                df_temp.iloc[i, dup.iloc[i, j][0]] = dup.iloc[i, j][1] ## 원래 자리 확률값으로\n",
    "            except :\n",
    "                print(i, j)\n",
    "                print(dup.iloc[i, j])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aab7481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.8691078), (1, 0.12751621)],\n",
       " [(0, 0.98204076), (3, 0.016536381)],\n",
       " [(0, 0.89222616), (1, 0.060337104), (2, 0.014186196), (3, 0.032855492)],\n",
       " [(0, 0.9511336), (1, 0.044832963)],\n",
       " [(0, 0.9299507), (1, 0.06666628)],\n",
       " [(0, 0.9735764), (3, 0.02522781)],\n",
       " [(0, 0.9095308), (1, 0.058316592), (2, 0.022097778)],\n",
       " [(0, 0.9501818), (1, 0.045875773)],\n",
       " [(0, 0.89087325), (1, 0.054906517), (3, 0.05346034)],\n",
       " [(0, 0.95735043), (1, 0.03843825)],\n",
       " [(0, 0.955734), (3, 0.033920832)],\n",
       " [(0, 0.97862506), (3, 0.020014388)],\n",
       " [(0, 0.8954746), (1, 0.067486495), (3, 0.036160335)],\n",
       " [(0, 0.970366), (3, 0.028437342)],\n",
       " [(0, 0.9957849)],\n",
       " [(0, 0.9512731), (1, 0.04475538)],\n",
       " [(0, 0.9540714), (1, 0.042237263)],\n",
       " [(0, 0.9671737), (3, 0.03141432)],\n",
       " [(0, 0.94888276), (1, 0.036503393), (2, 0.011969951)],\n",
       " [(0, 0.9955138)],\n",
       " [(0, 0.9560205), (1, 0.033269655)],\n",
       " [(0, 0.89161396), (1, 0.07836879), (3, 0.02921693)],\n",
       " [(0, 0.99585915)],\n",
       " [(0, 0.96598995), (1, 0.030503968)],\n",
       " [(0, 0.97807455), (3, 0.020660466)],\n",
       " [(0, 0.91215783), (1, 0.073115796), (3, 0.013912773)],\n",
       " [(0, 0.95382154), (4, 0.043143496)],\n",
       " [(0, 0.92849886), (1, 0.06831812)],\n",
       " [(0, 0.83963937), (1, 0.15727067)],\n",
       " [(0, 0.91332096), (1, 0.08342491)],\n",
       " [(0, 0.96653986), (3, 0.031749688)],\n",
       " [(0, 0.8881312), (4, 0.10695995)],\n",
       " [(0, 0.51865876), (3, 0.049243346), (4, 0.43116906)],\n",
       " [(0, 0.95352083), (4, 0.041762255)],\n",
       " [(0, 0.81592655), (4, 0.18044963)],\n",
       " [(0, 0.90030366), (3, 0.042286772), (4, 0.056262117)],\n",
       " [(0, 0.78347754), (3, 0.03430798), (4, 0.18093757)],\n",
       " [(0, 0.74376506), (4, 0.2505256)],\n",
       " [(0, 0.7622966), (4, 0.23290765)],\n",
       " [(0, 0.71092916), (4, 0.282314)],\n",
       " [(0, 0.99564534)],\n",
       " [(0, 0.6788824), (3, 0.19298673), (4, 0.12673044)],\n",
       " [(0, 0.9529095), (4, 0.041887343)],\n",
       " [(0, 0.85207206), (3, 0.021101344), (4, 0.1258795)],\n",
       " [(0, 0.972792), (3, 0.025634263)],\n",
       " [(0, 0.6395084), (4, 0.35616383)],\n",
       " [(0, 0.862905), (4, 0.132511)],\n",
       " [(0, 0.7011239), (4, 0.2955697)],\n",
       " [(0, 0.8510833), (4, 0.14297043)],\n",
       " [(0, 0.7099841), (3, 0.02780211), (4, 0.261064)],\n",
       " [(0, 0.8407051), (4, 0.15422593)],\n",
       " [(0, 0.81102014), (3, 0.08337309), (4, 0.10424635)],\n",
       " [(0, 0.9447199), (4, 0.05056425)],\n",
       " [(0, 0.7144164), (1, 0.012538576), (3, 0.01840211), (4, 0.25404096)],\n",
       " [(0, 0.85545355), (3, 0.026738564), (4, 0.11659763)],\n",
       " [(0, 0.6904903), (4, 0.3057057)],\n",
       " [(0, 0.8322947), (4, 0.16319329)],\n",
       " [(0, 0.64260733), (4, 0.35393733)],\n",
       " [(0, 0.85998935), (3, 0.04856468), (4, 0.0904147)],\n",
       " [(0, 0.7960794), (3, 0.04899242), (4, 0.15399328)],\n",
       " [(0, 0.18037795), (4, 0.81493133)],\n",
       " [(0, 0.24224724), (4, 0.7529803)],\n",
       " [(0, 0.20581213), (4, 0.787542)],\n",
       " [(0, 0.21556304), (2, 0.031380184), (4, 0.74941653)],\n",
       " [(0, 0.15139744), (3, 0.02405354), (4, 0.82343763)],\n",
       " [(0, 0.22148532), (4, 0.77463955)],\n",
       " [(0, 0.21450329), (4, 0.7791777)],\n",
       " [(0, 0.13096446), (4, 0.8638516)],\n",
       " [(0, 0.2718831), (4, 0.7239072)],\n",
       " [(0, 0.3603118), (4, 0.63493603)],\n",
       " [(0, 0.23986462), (3, 0.01692034), (4, 0.7420945)],\n",
       " [(0, 0.09370828), (4, 0.9024069)],\n",
       " [(0, 0.16342168), (4, 0.83234644)],\n",
       " [(0, 0.048932906), (1, 0.012641372), (4, 0.9337178)],\n",
       " [(0, 0.043784574), (3, 0.029680153), (4, 0.9254244)],\n",
       " [(0, 0.31134275), (4, 0.6850355)],\n",
       " [(0, 0.24561694), (3, 0.12647514), (4, 0.62691444)],\n",
       " [(0, 0.22872874), (1, 0.012455671), (4, 0.7547022)],\n",
       " [(0, 0.2738692), (4, 0.7218408)],\n",
       " [(0, 0.24940135), (3, 0.051956598), (4, 0.69751227)],\n",
       " [(0, 0.28931478), (4, 0.7057065)],\n",
       " [(0, 0.19721852), (2, 0.010317428), (3, 0.01610509), (4, 0.77584267)],\n",
       " [(0, 0.17715691), (4, 0.81929)],\n",
       " [(0, 0.32199514), (2, 0.010789229), (4, 0.6632628)],\n",
       " [(0, 0.23397838), (3, 0.07964679), (4, 0.6851603)],\n",
       " [(0, 0.4104617), (3, 0.018083548), (4, 0.57061815)],\n",
       " [(0, 0.18311319), (2, 0.013541209), (4, 0.7996059)],\n",
       " [(0, 0.1721801), (4, 0.82320815)],\n",
       " [(0, 0.36078927), (4, 0.63476926)],\n",
       " [(0, 0.2932428), (4, 0.7021356)],\n",
       " [(0, 0.82136905), (3, 0.13186827), (4, 0.045850053)],\n",
       " [(0, 0.9679639), (3, 0.030725474)],\n",
       " [(0, 0.9605053), (3, 0.038191747)],\n",
       " [(0, 0.9962165)],\n",
       " [(0, 0.9616618), (4, 0.034757353)],\n",
       " [(0, 0.9591184), (3, 0.039338246)],\n",
       " [(0, 0.96100634), (1, 0.035946757)],\n",
       " [(0, 0.93225485), (3, 0.06652731)],\n",
       " [(0, 0.9960696)],\n",
       " [(0, 0.99647635)],\n",
       " [(0, 0.9876875), (3, 0.010871849)],\n",
       " [(0, 0.92165214), (3, 0.0770579)],\n",
       " [(0, 0.94779664), (3, 0.033752717), (4, 0.01755158)],\n",
       " [(0, 0.9842438), (3, 0.014542798)],\n",
       " [(0, 0.9966306)],\n",
       " [(0, 0.8809167), (1, 0.012227307), (3, 0.028201872), (4, 0.07823027)],\n",
       " [(0, 0.9955251)],\n",
       " [(0, 0.98855084)],\n",
       " [(0, 0.87108815), (4, 0.12438337)],\n",
       " [(0, 0.9236524), (3, 0.024000822), (4, 0.051461972)],\n",
       " [(0, 0.98552155), (3, 0.013213608)],\n",
       " [(0, 0.9890035)],\n",
       " [(0, 0.9965375)],\n",
       " [(0, 0.97451776), (3, 0.024079476)],\n",
       " [(0, 0.99607086)],\n",
       " [(0, 0.9959707)],\n",
       " [(0, 0.96442765), (3, 0.034085467)],\n",
       " [(0, 0.9045701), (3, 0.094017275)],\n",
       " [(0, 0.9485929), (3, 0.050140034)],\n",
       " [(0, 0.93568915), (4, 0.0593378)],\n",
       " [(0, 0.36519903), (4, 0.63046485)],\n",
       " [(0, 0.35235295), (1, 0.012406806), (4, 0.632489)],\n",
       " [(0, 0.301363), (4, 0.6957117)],\n",
       " [(0, 0.21318603), (4, 0.7828885)],\n",
       " [(0, 0.17323677), (4, 0.823448)],\n",
       " [(0, 0.2880963), (3, 0.019556114), (4, 0.6910914)],\n",
       " [(0, 0.34003344), (1, 0.024826998), (4, 0.632222)],\n",
       " [(0, 0.29221296), (1, 0.017216897), (4, 0.6879484)],\n",
       " [(0, 0.26425332), (3, 0.011698073), (4, 0.72321135)],\n",
       " [(0, 0.32268283), (1, 0.0134562), (4, 0.66153866)],\n",
       " [(0, 0.27537218), (4, 0.7211418)],\n",
       " [(0, 0.27980843), (3, 0.041423757), (4, 0.67758405)],\n",
       " [(0, 0.21561797), (1, 0.038547523), (4, 0.74248654)],\n",
       " [(0, 0.2732985), (4, 0.72377604)],\n",
       " [(0, 0.17476033), (1, 0.04602528), (4, 0.77623713)],\n",
       " [(0, 0.24266927), (4, 0.75413036)],\n",
       " [(0, 0.37149405), (4, 0.625247)],\n",
       " [(0, 0.32098398), (1, 0.050457533), (4, 0.62506914)],\n",
       " [(0, 0.26346883), (1, 0.07833294), (4, 0.6557459)],\n",
       " [(0, 0.32280838), (2, 0.012999858), (4, 0.6614364)],\n",
       " [(0, 0.17095448), (1, 0.034463007), (4, 0.791474)],\n",
       " [(0, 0.25215045), (1, 0.037249036), (4, 0.7074139)],\n",
       " [(0, 0.28473058), (4, 0.7111385)],\n",
       " [(0, 0.14291655), (1, 0.05955473), (4, 0.7947122)],\n",
       " [(0, 0.29803675), (3, 0.019048719), (4, 0.6819871)],\n",
       " [(0, 0.22670817), (4, 0.7692287)],\n",
       " [(0, 0.3636669), (4, 0.63212115)],\n",
       " [(0, 0.22922042), (2, 0.04846779), (4, 0.71921533)],\n",
       " [(0, 0.2644973), (4, 0.73257166)],\n",
       " [(0, 0.34164348), (4, 0.6549547)],\n",
       " [(0, 0.8436069), (2, 0.010927904), (3, 0.14459772)],\n",
       " [(0, 0.92040455), (2, 0.01976196), (3, 0.05896543)],\n",
       " [(0, 0.95247096), (2, 0.012094032), (3, 0.034629375)],\n",
       " [(0, 0.81403023), (3, 0.18469936)],\n",
       " [(0, 0.95378935), (3, 0.044806175)],\n",
       " [(0, 0.9288927), (3, 0.06960686)],\n",
       " [(0, 0.92386216), (3, 0.07486134)],\n",
       " [(0, 0.9730432), (4, 0.023081811)],\n",
       " [(0, 0.98176384), (4, 0.014838465)],\n",
       " [(0, 0.8877927), (3, 0.110846065)],\n",
       " [(0, 0.8935724), (1, 0.019161452), (3, 0.077618666)],\n",
       " [(0, 0.9575573), (1, 0.016184842), (2, 0.018567529)],\n",
       " [(0, 0.97691625), (3, 0.0217811)],\n",
       " [(0, 0.9548986), (3, 0.016529255), (4, 0.018519972)],\n",
       " [(0, 0.9225909), (3, 0.0763155)],\n",
       " [(0, 0.7832107), (3, 0.21533704)],\n",
       " [(0, 0.9481674), (3, 0.05074927)],\n",
       " [(0, 0.9778018), (3, 0.020812863)],\n",
       " [(0, 0.9723985), (3, 0.026257956)],\n",
       " [(0, 0.9806625), (3, 0.018171659)],\n",
       " [(0, 0.932073), (2, 0.030015491), (3, 0.03686657)],\n",
       " [(0, 0.87364763), (3, 0.11022913), (4, 0.015116939)],\n",
       " [(0, 0.98159957), (3, 0.017104054)],\n",
       " [(0, 0.9064046), (3, 0.09241623)],\n",
       " [(0, 0.894129), (1, 0.022153266), (3, 0.08284227)],\n",
       " [(0, 0.9628351), (3, 0.029165851)],\n",
       " [(0, 0.88224065), (3, 0.11642134)],\n",
       " [(0, 0.8913578), (2, 0.015636817), (3, 0.09218023)],\n",
       " [(0, 0.97150886), (3, 0.027000844)],\n",
       " [(0, 0.9689261), (3, 0.02949688)],\n",
       " [(0, 0.9967866)],\n",
       " [(0, 0.9387324), (3, 0.05992436)],\n",
       " [(0, 0.9886192)],\n",
       " [(0, 0.930632), (3, 0.06829302)],\n",
       " [(0, 0.9965103)],\n",
       " [(0, 0.9202056), (3, 0.07856521)],\n",
       " [(0, 0.9861593), (3, 0.012768253)],\n",
       " [(0, 0.9826727), (3, 0.015977275)],\n",
       " [(0, 0.9875391), (3, 0.011173842)],\n",
       " [(0, 0.8508294), (3, 0.14810792)],\n",
       " [(0, 0.9749238), (1, 0.015289012)],\n",
       " [(0, 0.9911286)],\n",
       " [(0, 0.98034245), (3, 0.018563312)],\n",
       " [(0, 0.93257535), (1, 0.019337904), (3, 0.047257274)],\n",
       " [(0, 0.99377555)],\n",
       " [(0, 0.99646395)],\n",
       " [(0, 0.9276564), (3, 0.07117834)],\n",
       " [(0, 0.9340188), (3, 0.055820484)],\n",
       " [(0, 0.99008006)],\n",
       " [(0, 0.99616957)],\n",
       " [(0, 0.95614994), (3, 0.042547524)],\n",
       " [(0, 0.920354), (3, 0.078657486)],\n",
       " [(0, 0.996809)],\n",
       " [(0, 0.99627364)],\n",
       " [(0, 0.9873101)],\n",
       " [(0, 0.9967742)],\n",
       " [(0, 0.96400726), (3, 0.034902576)],\n",
       " [(0, 0.976895), (3, 0.021701312)],\n",
       " [(0, 0.9857546), (3, 0.013093591)],\n",
       " [(0, 0.97602344), (3, 0.022962725)],\n",
       " [(0, 0.5145584), (1, 0.46062154), (3, 0.024090143)],\n",
       " [(0, 0.6641661), (1, 0.23778148), (3, 0.0972385)],\n",
       " [(0, 0.66708255), (1, 0.3166843), (3, 0.01522268)],\n",
       " [(0, 0.62666607), (1, 0.37019658)],\n",
       " [(0, 0.72123206), (1, 0.2394679), (3, 0.038544733)],\n",
       " [(0, 0.6232815), (1, 0.3124021), (2, 0.04141168), (3, 0.022376012)],\n",
       " [(0, 0.550267), (1, 0.32090455), (3, 0.12813331)],\n",
       " [(0, 0.5844172), (1, 0.3601105), (3, 0.054511532)],\n",
       " [(0, 0.5961304), (1, 0.36861223), (2, 0.032024108)],\n",
       " [(0, 0.6216808), (1, 0.3628605), (3, 0.014616751)],\n",
       " [(0, 0.69278246), (1, 0.28382185), (3, 0.022572547)],\n",
       " [(0, 0.5806928), (1, 0.41530666)],\n",
       " [(0, 0.5983496), (1, 0.33621615), (3, 0.028718648), (4, 0.036333587)],\n",
       " [(0, 0.63882834), (1, 0.35679233)],\n",
       " [(0, 0.49322423), (1, 0.4712133), (4, 0.032777183)],\n",
       " [(0, 0.5766876), (1, 0.24373499), (3, 0.17868344)],\n",
       " [(0, 0.5429305), (1, 0.4126793), (4, 0.03980869)],\n",
       " [(0, 0.6311357), (1, 0.28771567), (3, 0.08026363)],\n",
       " [(0, 0.6831469), (1, 0.31352752)],\n",
       " [(0, 0.60695887), (1, 0.35344547), (3, 0.038670264)],\n",
       " [(0, 0.48612824), (1, 0.43884403), (3, 0.07433506)],\n",
       " [(0, 0.5105904), (1, 0.32879022), (3, 0.15972297)],\n",
       " [(0, 0.54043293), (1, 0.4023096), (3, 0.056521367)],\n",
       " [(0, 0.7161805), (1, 0.2417815), (3, 0.03281851)],\n",
       " [(0, 0.65404046), (1, 0.27517402), (3, 0.06987243)],\n",
       " [(0, 0.6333465), (1, 0.2878151), (3, 0.060691144), (4, 0.017689992)],\n",
       " [(0, 0.4627029), (1, 0.3896309), (3, 0.14691167)],\n",
       " [(0, 0.6835535), (1, 0.28618893), (4, 0.027304536)],\n",
       " [(0, 0.5788854), (1, 0.35431615), (3, 0.06606629)],\n",
       " [(0, 0.7103618), (1, 0.20263954), (3, 0.086178176)],\n",
       " [(0, 0.40440837), (1, 0.5921795)],\n",
       " [(0, 0.54277074), (1, 0.41812354), (3, 0.038242828)],\n",
       " [(0, 0.52453905), (1, 0.4048064), (3, 0.06984414)],\n",
       " [(0, 0.42968494), (1, 0.4827225), (3, 0.08659879)],\n",
       " [(0, 0.238969), (1, 0.71848226), (3, 0.041803952)],\n",
       " [(0, 0.657642), (1, 0.3391607)],\n",
       " [(0, 0.5436876), (1, 0.3581568), (3, 0.09750196)],\n",
       " [(0, 0.58049786), (1, 0.41636086)],\n",
       " [(0, 0.4151097), (1, 0.56690633), (3, 0.01706036)],\n",
       " [(0, 0.42767254), (1, 0.5547617), (3, 0.016686782)],\n",
       " [(0, 0.59946126), (1, 0.33932856), (3, 0.06046116)],\n",
       " [(0, 0.48918122), (1, 0.47664452), (3, 0.033342123)],\n",
       " [(0, 0.52084935), (1, 0.47568142)],\n",
       " [(0, 0.4809132), (1, 0.4731732), (3, 0.044852983)],\n",
       " [(0, 0.58832145), (1, 0.38193595), (3, 0.028664103)],\n",
       " [(0, 0.32496315), (1, 0.6459203), (3, 0.028243678)],\n",
       " [(0, 0.43125153), (1, 0.5201173), (3, 0.047682606)],\n",
       " [(0, 0.5600918), (1, 0.43592134)],\n",
       " [(0, 0.4189215), (1, 0.57616585)],\n",
       " [(0, 0.4316776), (1, 0.5300609), (3, 0.037319753)],\n",
       " [(0, 0.3098807), (1, 0.5366002), (3, 0.15251651)],\n",
       " [(0, 0.41298187), (1, 0.5841938)],\n",
       " [(0, 0.47745764), (1, 0.49786404), (3, 0.023670077)],\n",
       " [(0, 0.5237693), (1, 0.38854507), (3, 0.08651105)],\n",
       " [(0, 0.4506988), (1, 0.5457131)],\n",
       " [(0, 0.36548942), (1, 0.52687716), (3, 0.10671536)],\n",
       " [(0, 0.50664216), (1, 0.49002877)],\n",
       " [(0, 0.55672497), (1, 0.4391628)],\n",
       " [(0, 0.391048), (1, 0.5076991), (3, 0.100266)],\n",
       " [(0, 0.34499535), (1, 0.64088523), (2, 0.010415505)],\n",
       " [(0, 0.07407198), (2, 0.06613542), (4, 0.85604846)],\n",
       " [(0, 0.12064906), (3, 0.016039133), (4, 0.86240005)],\n",
       " [(0, 0.07529962), (4, 0.9207486)],\n",
       " [(0, 0.059532188), (1, 0.021166433), (2, 0.014538217), (4, 0.90207803)],\n",
       " [(0, 0.11276495), (1, 0.024361882), (4, 0.85964185)],\n",
       " [(0, 0.03953224), (2, 0.032030087), (4, 0.925413)],\n",
       " [(3, 0.097900346), (4, 0.89740795)],\n",
       " [(0, 0.15421668), (4, 0.8421946)],\n",
       " [(4, 0.99399585)],\n",
       " [(0, 0.032330647), (3, 0.05753727), (4, 0.9093088)],\n",
       " [(0, 0.05908308), (3, 0.026395492), (4, 0.9135182)],\n",
       " [(3, 0.0644161), (4, 0.9229232)],\n",
       " [(0, 0.13423689), (4, 0.86169136)],\n",
       " [(0, 0.15569961), (3, 0.018703002), (4, 0.8246584)],\n",
       " [(0, 0.09026028), (2, 0.029989915), (4, 0.8765814)],\n",
       " [(0, 0.09182993), (2, 0.03291626), (4, 0.87260497)],\n",
       " [(0, 0.060897358), (3, 0.015017778), (4, 0.9137543)],\n",
       " [(0, 0.083206356), (2, 0.03728159), (4, 0.8730518)],\n",
       " [(0, 0.057913825), (3, 0.014422868), (4, 0.92675275)],\n",
       " [(0, 0.13283436), (4, 0.86075354)],\n",
       " [(0, 0.12212607), (4, 0.8738893)],\n",
       " [(0, 0.027937455), (3, 0.014284519), (4, 0.9568628)],\n",
       " [(0, 0.048109055), (2, 0.022260971), (4, 0.92588097)],\n",
       " [(0, 0.10840893), (3, 0.020155711), (4, 0.870451)],\n",
       " [(0, 0.046302903), (3, 0.01723182), (4, 0.9355987)],\n",
       " [(2, 0.02384456), (4, 0.9712979)],\n",
       " [(2, 0.011194693), (4, 0.9835466)],\n",
       " [(0, 0.02296616), (2, 0.035635523), (3, 0.017600797), (4, 0.9232633)],\n",
       " [(0, 0.033343606), (4, 0.9557707)],\n",
       " [(0, 0.08609461), (4, 0.9102763)],\n",
       " [(0, 0.8519046), (3, 0.112823516), (4, 0.03435414)],\n",
       " [(0, 0.7328872), (3, 0.26576892)],\n",
       " [(0, 0.7759783), (3, 0.22286935)],\n",
       " [(0, 0.7994759), (3, 0.19919674)],\n",
       " [(0, 0.89182466), (3, 0.107058115)],\n",
       " [(0, 0.93212754), (3, 0.05168834), (4, 0.015079873)],\n",
       " [(0, 0.95350003), (3, 0.044901326)],\n",
       " [(0, 0.8698383), (3, 0.12871182)],\n",
       " [(0, 0.7427172), (1, 0.02513587), (3, 0.23119414)],\n",
       " [(0, 0.91419655), (1, 0.043605737), (3, 0.041096605)],\n",
       " [(0, 0.848374), (3, 0.10344384), (4, 0.04739933)],\n",
       " [(0, 0.77642643), (3, 0.22212325)],\n",
       " [(0, 0.9278982), (3, 0.07044382)],\n",
       " [(0, 0.83651054), (3, 0.1621918)],\n",
       " [(0, 0.67398953), (3, 0.32435957)],\n",
       " [(0, 0.8407161), (3, 0.15769698)],\n",
       " [(0, 0.8408935), (3, 0.1576412)],\n",
       " [(0, 0.90641445), (3, 0.092590794)],\n",
       " [(0, 0.8546571), (3, 0.14327246)],\n",
       " [(0, 0.6136435), (3, 0.384845)],\n",
       " [(0, 0.83584285), (3, 0.16242577)],\n",
       " [(0, 0.88950527), (3, 0.109062366)],\n",
       " [(0, 0.99492395)],\n",
       " [(0, 0.9198281), (3, 0.07888416)],\n",
       " [(0, 0.924168), (3, 0.07424176)],\n",
       " [(0, 0.8944204), (3, 0.104369834)],\n",
       " [(0, 0.85119456), (3, 0.14755632)],\n",
       " [(0, 0.7903048), (3, 0.2082886)],\n",
       " [(0, 0.7093512), (3, 0.2895211)],\n",
       " [(0, 0.8106638), (3, 0.1881059)],\n",
       " [(0, 0.18365723), (2, 0.10385436), (4, 0.70973533)],\n",
       " [(0, 0.25617284), (2, 0.16571869), (4, 0.57530123)],\n",
       " [(0, 0.18342507), (2, 0.07665023), (4, 0.7370001)],\n",
       " [(0, 0.13687664), (2, 0.15621907), (4, 0.69933915)],\n",
       " [(0, 0.33457017), (2, 0.05018095), (3, 0.023829076), (4, 0.59085804)],\n",
       " [(0, 0.104036555), (2, 0.10868037), (3, 0.027790459), (4, 0.7590543)],\n",
       " [(0, 0.18921842), (2, 0.049879532), (3, 0.018222878), (4, 0.7422389)],\n",
       " [(0, 0.13130356), (2, 0.053956684), (4, 0.8112853)],\n",
       " [(0, 0.2076924), (2, 0.057429414), (3, 0.017867882), (4, 0.71658194)],\n",
       " [(0, 0.18614548), (2, 0.020765923), (4, 0.788974)],\n",
       " [(0, 0.16511299), (2, 0.08506028), (4, 0.74585944)],\n",
       " [(0, 0.14827375), (1, 0.024054436), (3, 0.025855578), (4, 0.80138016)],\n",
       " [(0, 0.1587995), (4, 0.8369458)],\n",
       " [(0, 0.114116274), (2, 0.10236748), (3, 0.026379658), (4, 0.75661355)],\n",
       " [(0, 0.21213003), (2, 0.044650022), (4, 0.7401254)],\n",
       " [(0, 0.06599147), (2, 0.18500803), (4, 0.7459404)],\n",
       " [(0, 0.18383813), (2, 0.062894896), (4, 0.7500202)],\n",
       " [(0, 0.11897924), (2, 0.030773954), (3, 0.027756572), (4, 0.8219057)],\n",
       " [(0, 0.112475775), (2, 0.054074597), (4, 0.83059835)],\n",
       " [(0, 0.18398406), (2, 0.10194341), (4, 0.7114819)],\n",
       " [(0, 0.07423572), (2, 0.044859968), (4, 0.87786096)],\n",
       " [(0, 0.33458394), (2, 0.08181517), (3, 0.044752967), (4, 0.53833795)],\n",
       " [(0, 0.20649679), (2, 0.061066773), (3, 0.04381757), (4, 0.6881825)],\n",
       " [(0, 0.15872008), (2, 0.10361987), (3, 0.018381145), (4, 0.7188861)],\n",
       " [(0, 0.10621907), (2, 0.120649435), (4, 0.76611066)],\n",
       " [(0, 0.14602955), (2, 0.09218156), (3, 0.047558967), (4, 0.7137501)],\n",
       " [(0, 0.1160363), (2, 0.1145856), (3, 0.055636346), (4, 0.71326494)],\n",
       " [(0, 0.17894237), (2, 0.08100218), (4, 0.7375516)],\n",
       " [(0, 0.14503565), (2, 0.111903794), (4, 0.7400592)],\n",
       " [(0, 0.04018538), (2, 0.1332031), (4, 0.8231129)],\n",
       " [(0, 0.224759), (2, 0.37384084), (4, 0.3986969)],\n",
       " [(0, 0.13333586), (2, 0.30405024), (3, 0.011405974), (4, 0.5507304)],\n",
       " [(0, 0.2892652), (2, 0.33573467), (4, 0.3719897)],\n",
       " [(0, 0.1699786), (2, 0.22970496), (3, 0.05287263), (4, 0.54696333)],\n",
       " [(0, 0.33234194), (2, 0.37323073), (4, 0.29127207)],\n",
       " [(0, 0.060135152), (2, 0.42582747), (4, 0.5110139)],\n",
       " [(0, 0.19636486), (2, 0.3235236), (4, 0.47717246)],\n",
       " [(0, 0.3766391), (2, 0.25161442), (3, 0.017347317), (4, 0.35397536)],\n",
       " [(0, 0.20712256), (2, 0.35922423), (4, 0.43080372)],\n",
       " [(0, 0.27654284), (2, 0.3043371), (4, 0.4162096)],\n",
       " [(0, 0.176687), (2, 0.35524365), (4, 0.4656849)],\n",
       " [(0, 0.34572557), (2, 0.35803246), (4, 0.29350406)],\n",
       " [(0, 0.3374168), (2, 0.30884716), (4, 0.35095447)],\n",
       " [(0, 0.21571063), (2, 0.20898357), (4, 0.5726364)],\n",
       " [(0, 0.24523728), (2, 0.31790498), (4, 0.4342825)],\n",
       " [(0, 0.20137693), (2, 0.35513344), (4, 0.44016674)],\n",
       " [(0, 0.15304579), (2, 0.38881463), (4, 0.45532006)],\n",
       " [(0, 0.21343139), (2, 0.47192365), (4, 0.3120533)],\n",
       " [(0, 0.15681261), (2, 0.39084417), (3, 0.06534842), (4, 0.38656357)],\n",
       " [(0, 0.17365925), (2, 0.2883262), (3, 0.017340442), (4, 0.52026)],\n",
       " [(0, 0.1647478), (2, 0.33691236), (4, 0.49536696)],\n",
       " [(0, 0.16920255), (2, 0.40757525), (4, 0.4205778)],\n",
       " [(0, 0.10928061), (2, 0.32381362), (4, 0.56422263)],\n",
       " [(0, 0.2703655), (2, 0.3784996), (4, 0.34759608)],\n",
       " [(0, 0.11734768), (2, 0.34393266), (3, 0.023086976), (4, 0.5150986)],\n",
       " [(0, 0.21964769), (2, 0.28438488), (4, 0.49111682)],\n",
       " [(0, 0.19854823), (2, 0.48379982), (4, 0.31426683)],\n",
       " [(0, 0.22394307), (2, 0.32242647), (4, 0.4512552)],\n",
       " [(0, 0.15216045), (2, 0.3575838), (4, 0.48682374)],\n",
       " [(0, 0.20233603), (2, 0.43300515), (4, 0.36209968)],\n",
       " [(0, 0.15974326), (2, 0.21117973), (4, 0.62642545)],\n",
       " [(0, 0.3378895), (2, 0.20959258), (4, 0.44905728)],\n",
       " [(0, 0.13050075), (1, 0.03993387), (2, 0.21196887), (4, 0.6138892)],\n",
       " [(0, 0.15010697), (2, 0.20693168), (4, 0.6396728)],\n",
       " [(0, 0.46155268), (2, 0.16803485), (3, 0.05216009), (4, 0.31781673)],\n",
       " [(0, 0.24602394), (2, 0.15672901), (4, 0.59411776)],\n",
       " [(0, 0.32580253), (2, 0.33410043), (4, 0.3370765)],\n",
       " [(0, 0.16103837), (2, 0.05238077), (4, 0.7831707)],\n",
       " [(0, 0.16456364), (2, 0.09220117), (4, 0.7385151)],\n",
       " [(0, 0.24208012),\n",
       "  (1, 0.02835784),\n",
       "  (2, 0.16303386),\n",
       "  (3, 0.015321622),\n",
       "  (4, 0.5512065)],\n",
       " [(0, 0.2436665), (2, 0.24634692), (4, 0.50326383)],\n",
       " [(0, 0.30820104), (2, 0.07573161), (3, 0.06656287), (4, 0.5488956)],\n",
       " [(0, 0.26440772), (2, 0.1178361), (4, 0.614738)],\n",
       " [(0, 0.21018523), (2, 0.2230924), (4, 0.5639255)],\n",
       " [(0, 0.2565772), (2, 0.09880564), (3, 0.020416088), (4, 0.62365836)],\n",
       " [(0, 0.12297428),\n",
       "  (1, 0.074826136),\n",
       "  (2, 0.36440507),\n",
       "  (3, 0.014149165),\n",
       "  (4, 0.42364538)],\n",
       " [(0, 0.33162665), (2, 0.23960145), (3, 0.16019298), (4, 0.26797476)],\n",
       " [(0, 0.2772924), (1, 0.0873479), (2, 0.1968683), (4, 0.43545756)],\n",
       " [(1, 0.029302154), (3, 0.017085277), (4, 0.94817)],\n",
       " [(0, 0.08288466), (2, 0.09918344), (4, 0.8143637)],\n",
       " [(0, 0.30257288), (1, 0.11983728), (2, 0.43637982), (4, 0.13831715)],\n",
       " [(0, 0.21517324), (2, 0.35498852), (3, 0.10781276), (4, 0.32163)],\n",
       " [(0, 0.17666553), (1, 0.050616894), (2, 0.32814187), (4, 0.4417464)],\n",
       " [(0, 0.11291599), (2, 0.20424601), (4, 0.67908305)],\n",
       " [(0, 0.25644374),\n",
       "  (1, 0.061752107),\n",
       "  (2, 0.23555732),\n",
       "  (3, 0.02250916),\n",
       "  (4, 0.4237377)],\n",
       " [(0, 0.12637722), (1, 0.20028947), (4, 0.66984653)],\n",
       " [(0, 0.04398547), (2, 0.1353065), (4, 0.8172996)],\n",
       " [(0, 0.11630696),\n",
       "  (1, 0.04491356),\n",
       "  (2, 0.21427716),\n",
       "  (3, 0.01594389),\n",
       "  (4, 0.6085584)],\n",
       " [(0, 0.34633842), (1, 0.10476715), (2, 0.23733266), (4, 0.30874017)],\n",
       " [(0, 0.2566174), (2, 0.24204114), (4, 0.4978456)],\n",
       " [(0, 0.32822782), (3, 0.15580629), (4, 0.5145368)],\n",
       " [(0, 0.19972815), (3, 0.2747161), (4, 0.52381665)],\n",
       " [(0, 0.3506771), (3, 0.10455479), (4, 0.5431984)],\n",
       " [(0, 0.36331394), (2, 0.060439326), (4, 0.56966686)],\n",
       " [(0, 0.2666133), (1, 0.04297885), (3, 0.21045473), (4, 0.47902986)],\n",
       " [(0, 0.27740487), (3, 0.21835522), (4, 0.5025383)],\n",
       " [(0, 0.24818322), (4, 0.74431443)],\n",
       " [(0, 0.22141072), (2, 0.014265985), (4, 0.7594444)],\n",
       " [(0, 0.24961567), (1, 0.015405723), (3, 0.072618075), (4, 0.66151357)],\n",
       " [(0, 0.33782473), (3, 0.14553566), (4, 0.5154331)],\n",
       " [(0, 0.29728606), (2, 0.031318862), (4, 0.6665143)],\n",
       " [(0, 0.25896254), (3, 0.020247765), (4, 0.7194253)],\n",
       " [(0, 0.43741083), (3, 0.28163013), (4, 0.2793201)],\n",
       " [(0, 0.20804493), (2, 0.06263617), (3, 0.32700837), (4, 0.4015979)],\n",
       " [(0, 0.23261474), (2, 0.05976624), (3, 0.029083103), (4, 0.6777688)],\n",
       " [(0, 0.33103922), (3, 0.06161895), (4, 0.6058024)],\n",
       " [(0, 0.23546475), (3, 0.4034894), (4, 0.35925838)],\n",
       " [(0, 0.40414307), (4, 0.58995855)],\n",
       " [(0, 0.2412299), (3, 0.2002352), (4, 0.5569988)],\n",
       " [(0, 0.1803586), (1, 0.025424719), (3, 0.058827937), (4, 0.7347802)],\n",
       " [(0, 0.2810364), (3, 0.11823529), (4, 0.598855)],\n",
       " [(0, 0.3963174), (4, 0.5966818)],\n",
       " [(0, 0.2867894), (1, 0.057952307), (3, 0.07139814), (4, 0.5831598)],\n",
       " [(0, 0.2405002), (3, 0.07235138), (4, 0.68530184)],\n",
       " [(0, 0.3602099), (3, 0.10986368), (4, 0.5286903)],\n",
       " [(0, 0.16653502), (2, 0.045700915), (3, 0.026036251), (4, 0.7609603)],\n",
       " [(0, 0.17095318), (4, 0.81937397)],\n",
       " [(0, 0.38961574), (3, 0.053529244), (4, 0.5552646)],\n",
       " [(0, 0.19754295), (3, 0.34895447), (4, 0.45105943)],\n",
       " [(0, 0.34586412), (2, 0.017764494), (3, 0.081226364), (4, 0.5543274)],\n",
       " [(0, 0.16620561), (2, 0.035691854), (3, 0.6815488), (4, 0.11609759)],\n",
       " [(0, 0.20816702), (2, 0.024253493), (3, 0.7208438), (4, 0.046196625)],\n",
       " [(0, 0.069399774), (3, 0.6497692), (4, 0.2796987)],\n",
       " [(0, 0.25387818), (3, 0.48070315), (4, 0.26442736)],\n",
       " [(0, 0.18310973), (3, 0.7706573), (4, 0.04488541)],\n",
       " [(0, 0.49357092), (3, 0.43978006), (4, 0.06554272)],\n",
       " [(0, 0.12716687), (1, 0.021422653), (3, 0.8504953)],\n",
       " [(0, 0.079024784), (2, 0.030635761), (3, 0.8101242), (4, 0.07967873)],\n",
       " [(0, 0.19933113), (3, 0.65344584), (4, 0.1463703)],\n",
       " [(0, 0.21094985), (3, 0.66279864), (4, 0.12514545)],\n",
       " [(0, 0.22026587), (3, 0.7778131)],\n",
       " [(0, 0.19811493), (1, 0.016596494), (3, 0.6457337), (4, 0.13911894)],\n",
       " [(0, 0.19690242), (3, 0.5588566), (4, 0.24317065)],\n",
       " [(0, 0.14162317), (1, 0.015051055), (3, 0.50557804), (4, 0.337284)],\n",
       " [(0, 0.1376835), (3, 0.67129266), (4, 0.18968451)],\n",
       " [(0, 0.26505738), (3, 0.66013646), (4, 0.073978074)],\n",
       " [(0, 0.052772805), (3, 0.8819419), (4, 0.06420969)],\n",
       " [(0, 0.06808086), (2, 0.054451276), (3, 0.7631935), (4, 0.11371353)],\n",
       " [(0, 0.1802953), (3, 0.75977194), (4, 0.05903163)],\n",
       " [(0, 0.24830733), (3, 0.75013965)],\n",
       " [(0, 0.04245468), (1, 0.017432302), (3, 0.70364106), (4, 0.23599859)],\n",
       " [(0, 0.12747815), (1, 0.030170754), (3, 0.53220063), (4, 0.30968192)],\n",
       " [(0, 0.25923896), (3, 0.61508924), (4, 0.12459346)],\n",
       " [(0, 0.15250984), (3, 0.5896795), (4, 0.2567604)],\n",
       " [(0, 0.17280258), (3, 0.7641503), (4, 0.062262967)],\n",
       " [(0, 0.20208028), (1, 0.023250354), (3, 0.70250696), (4, 0.07161035)],\n",
       " [(0, 0.36183202), (3, 0.63649666)],\n",
       " [(0, 0.13798223), (3, 0.8077224), (4, 0.053230185)],\n",
       " [(0, 0.05817656), (3, 0.9167793), (4, 0.024031611)],\n",
       " [(0, 0.14718105), (3, 0.85123044)],\n",
       " [(0, 0.106986925), (3, 0.12068861), (4, 0.7712677)],\n",
       " [(0, 0.24984744), (1, 0.017494777), (3, 0.09305107), (4, 0.63907367)],\n",
       " [(0, 0.07722509), (3, 0.1703282), (4, 0.75110817)],\n",
       " [(3, 0.18300688), (4, 0.81236356)],\n",
       " [(3, 0.19312914), (4, 0.802951)],\n",
       " [(0, 0.16359536), (3, 0.08702986), (4, 0.7478689)],\n",
       " [(3, 0.22408257), (4, 0.7701422)],\n",
       " [(0, 0.12396304), (3, 0.24078168), (4, 0.63401616)],\n",
       " [(0, 0.062118124), (3, 0.16644505), (4, 0.77037215)],\n",
       " [(0, 0.12101051), (3, 0.08956292), (4, 0.7881887)],\n",
       " [(0, 0.08409817), (3, 0.105679214), (4, 0.8088851)],\n",
       " [(2, 0.017720455), (3, 0.09280729), (4, 0.8848997)],\n",
       " [(0, 0.13333589), (3, 0.23386037), (4, 0.63169026)],\n",
       " [(3, 0.14082812), (4, 0.8540954)],\n",
       " [(0, 0.055226844), (2, 0.03142486), (3, 0.11826279), (4, 0.7945095)],\n",
       " [(0, 0.09086431), (2, 0.048455264), (3, 0.1843034), (4, 0.6758729)],\n",
       " [(0, 0.029561557), (3, 0.2707842), (4, 0.69841766)],\n",
       " [(0, 0.090376906), (3, 0.12061053), (4, 0.78794736)],\n",
       " [(0, 0.06310369), (2, 0.012924597), (4, 0.91890264)],\n",
       " [(3, 0.22601579), (4, 0.7689436)],\n",
       " [(1, 0.017247565), (2, 0.09264699), (3, 0.25553522), (4, 0.63065684)],\n",
       " [(0, 0.10244395), (1, 0.010014078), (3, 0.22084244), (4, 0.66620696)],\n",
       " [(0, 0.06952232), (3, 0.22851752), (4, 0.7006662)],\n",
       " [(3, 0.36976764), (4, 0.62598205)],\n",
       " [(0, 0.14807718), (3, 0.233184), (4, 0.61763823)],\n",
       " [(3, 0.25908157), (4, 0.7355373)],\n",
       " [(0, 0.119799376), (3, 0.14655223), (4, 0.7326691)],\n",
       " [(0, 0.17574467), (3, 0.14029677), (4, 0.6828843)],\n",
       " [(2, 0.06284261), (3, 0.23590854), (4, 0.69457287)],\n",
       " [(0, 0.09072652), (3, 0.21762167), (4, 0.69043547)],\n",
       " [(0, 0.60983956), (3, 0.3279421), (4, 0.060983345)],\n",
       " [(0, 0.7951408), (3, 0.1433215), (4, 0.06045804)],\n",
       " [(0, 0.9131765), (3, 0.08491913)],\n",
       " [(0, 0.941541), (3, 0.056449376)],\n",
       " [(0, 0.9153506), (3, 0.045851614), (4, 0.03736852)],\n",
       " [(0, 0.94500446), (3, 0.05305948)],\n",
       " [(0, 0.76472765), (3, 0.15651645), (4, 0.07759867)],\n",
       " [(0, 0.9059614), (3, 0.09188384)],\n",
       " [(0, 0.95753545), (2, 0.038382646)],\n",
       " [(0, 0.9018038), (2, 0.034804936), (3, 0.06230974)],\n",
       " [(0, 0.8922693), (3, 0.105979644)],\n",
       " [(0, 0.89181674), (2, 0.02263554), (3, 0.08426312)],\n",
       " [(0, 0.93604165), (3, 0.062434185)],\n",
       " [(0, 0.87690085), (3, 0.12128722)],\n",
       " [(0, 0.926799), (2, 0.02092871), (3, 0.05113415)],\n",
       " [(0, 0.9178697), (3, 0.0807645)],\n",
       " [(0, 0.7838994), (3, 0.16031638), (4, 0.0545668)],\n",
       " [(0, 0.92569226), (3, 0.07237079)],\n",
       " [(0, 0.87257344), (3, 0.1259173)],\n",
       " [(0, 0.80998605), (3, 0.18858224)],\n",
       " [(0, 0.8176555), (3, 0.1804244)],\n",
       " [(0, 0.83448815), (3, 0.16378513)],\n",
       " [(0, 0.98273146), (3, 0.015221185)],\n",
       " [(0, 0.93047893), (3, 0.06784714)],\n",
       " [(0, 0.9603928), (3, 0.037842702)],\n",
       " [(0, 0.80941516), (3, 0.18832591)],\n",
       " [(0, 0.9114411), (3, 0.08665598)],\n",
       " [(0, 0.75934696), (3, 0.23912728)],\n",
       " [(0, 0.9545362), (4, 0.04001479)],\n",
       " [(0, 0.84373397), (3, 0.15411903)],\n",
       " [(0, 0.60905117), (1, 0.18050396), (3, 0.022413313), (4, 0.18762794)],\n",
       " [(0, 0.5342925), (1, 0.22033663), (3, 0.2076322), (4, 0.037415173)],\n",
       " [(0, 0.6277429), (1, 0.14937381), (3, 0.07085953), (4, 0.15156974)],\n",
       " [(0, 0.664139),\n",
       "  (1, 0.11819267),\n",
       "  (2, 0.025214672),\n",
       "  (3, 0.11138747),\n",
       "  (4, 0.08106626)],\n",
       " [(0, 0.7387085), (1, 0.24450836), (3, 0.016111454)],\n",
       " [(0, 0.6450529), (1, 0.12412771), (3, 0.12770458), (4, 0.102766186)],\n",
       " [(0, 0.64734566), (1, 0.2435522), (3, 0.040717836), (4, 0.06796094)],\n",
       " [(0, 0.6419427), (1, 0.13159727), (3, 0.15470633), (4, 0.071281455)],\n",
       " [(0, 0.6967274),\n",
       "  (1, 0.1538681),\n",
       "  (2, 0.024239844),\n",
       "  (3, 0.04584682),\n",
       "  (4, 0.079317816)],\n",
       " [(0, 0.69814384), (1, 0.19199239), (3, 0.108902715)],\n",
       " [(0, 0.7004754), (1, 0.18246712), (2, 0.062854506), (3, 0.053780995)],\n",
       " [(0, 0.64711183), (1, 0.13981868), (3, 0.090528764), (4, 0.12221978)],\n",
       " [(0, 0.6323868), (1, 0.19996212), (3, 0.12154906), (4, 0.04567843)],\n",
       " [(0, 0.66818774), (1, 0.2565483), (4, 0.07155111)],\n",
       " [(0, 0.7377672), (1, 0.18688881), (3, 0.02171004), (4, 0.05317945)],\n",
       " [(0, 0.7566531), (1, 0.17968924), (2, 0.040204242), (4, 0.021237312)],\n",
       " [(0, 0.7472378), (1, 0.07499475), (3, 0.11687065), (4, 0.06056284)],\n",
       " [(0, 0.60701394), (1, 0.32196802), (3, 0.043251965), (4, 0.027331848)],\n",
       " [(0, 0.6616876),\n",
       "  (1, 0.17724259),\n",
       "  (2, 0.05297247),\n",
       "  (3, 0.060431045),\n",
       "  (4, 0.047666233)],\n",
       " [(0, 0.6815071), (1, 0.21899897), (3, 0.06927706), (4, 0.029815754)],\n",
       " [(0, 0.5758392), (1, 0.30527347), (2, 0.023186935), (4, 0.092889056)],\n",
       " [(0, 0.65073484), (1, 0.25512937), (3, 0.055310953), (4, 0.038427502)],\n",
       " [(0, 0.5301031), (1, 0.301855), (3, 0.09371767), (4, 0.0739507)],\n",
       " [(0, 0.6805821),\n",
       "  (1, 0.1756691),\n",
       "  (2, 0.058412135),\n",
       "  (3, 0.04463677),\n",
       "  (4, 0.040699866)],\n",
       " [(0, 0.6832393), (1, 0.15279247), (3, 0.04638151), (4, 0.11719968)],\n",
       " [(0, 0.6357626), (1, 0.23807894), (3, 0.12538469)],\n",
       " [(0, 0.6761759), (1, 0.15651597), (3, 0.08462128), (4, 0.0823358)],\n",
       " [(0, 0.705196), (1, 0.19862747), (3, 0.095412426)],\n",
       " [(0, 0.7947691), (1, 0.091776155), (2, 0.083464645), (3, 0.029446557)],\n",
       " [(0, 0.7247195), (1, 0.18719265), (3, 0.08727898)],\n",
       " [(0, 0.27048042), (2, 0.5976751), (3, 0.019404758), (4, 0.11190199)],\n",
       " [(0, 0.12532017), (2, 0.7607995), (4, 0.10979794)],\n",
       " [(0, 0.106296964), (2, 0.5653338), (4, 0.32417884)],\n",
       " [(0, 0.058894556), (2, 0.6680408), (3, 0.05863973), (4, 0.21377501)],\n",
       " [(0, 0.33049843), (2, 0.6151529), (3, 0.053016644)],\n",
       " [(0, 0.24551244), (2, 0.50562024), (3, 0.027068758), (4, 0.22111453)],\n",
       " [(0, 0.18021525), (2, 0.34371012), (4, 0.46938142)],\n",
       " [(0, 0.29227585), (2, 0.5790989), (4, 0.12368727)],\n",
       " [(0, 0.2923419), (2, 0.5248835), (4, 0.17858484)],\n",
       " [(0, 0.29887488), (2, 0.6250278), (4, 0.07228952)],\n",
       " [(0, 0.13140258), (2, 0.7761774), (4, 0.08838699)],\n",
       " [(0, 0.26165518), (2, 0.5692804), (4, 0.16348489)],\n",
       " [(0, 0.4759811), (2, 0.518533)],\n",
       " [(0, 0.2700725), (2, 0.5525834), (4, 0.17181207)],\n",
       " [(0, 0.32901546), (2, 0.51599497), (4, 0.15007766)],\n",
       " [(0, 0.28896138), (2, 0.54996985), (4, 0.15559284)],\n",
       " [(0, 0.21618915), (2, 0.46165642), (4, 0.31712064)],\n",
       " [(0, 0.36537418), (2, 0.5055512), (3, 0.047376875), (4, 0.08099872)],\n",
       " [(0, 0.138825), (2, 0.59199834), (4, 0.26462436)],\n",
       " [(0, 0.30472812), (2, 0.523724), (4, 0.166164)],\n",
       " [(0, 0.33120298), (2, 0.49865893), (4, 0.16573896)],\n",
       " [(0, 0.44976267), (2, 0.5438465)],\n",
       " [(0, 0.15842074),\n",
       "  (1, 0.02192766),\n",
       "  (2, 0.5810765),\n",
       "  (3, 0.019964946),\n",
       "  (4, 0.2186102)],\n",
       " [(0, 0.27329987), (1, 0.016991502), (2, 0.5632426), (4, 0.14295772)],\n",
       " [(0, 0.18169998), (2, 0.46529058), (4, 0.34847513)],\n",
       " [(0, 0.16832195), (2, 0.58362675), (3, 0.050640933), (4, 0.19672596)],\n",
       " [(0, 0.27715656), (2, 0.7174759)],\n",
       " [(0, 0.24645433), (2, 0.5432728), (4, 0.20558695)],\n",
       " [(0, 0.23967858), (2, 0.55099815), (3, 0.016284594), (4, 0.19241072)],\n",
       " [(0, 0.2418194), (2, 0.6125115), (4, 0.14170049)],\n",
       " [(0, 0.6624214), (1, 0.33349687)],\n",
       " [(0, 0.87841946), (2, 0.11656218)],\n",
       " [(0, 0.44751805), (1, 0.21046947), (2, 0.30652314), (3, 0.03484181)],\n",
       " [(0, 0.47710449),\n",
       "  (1, 0.38310307),\n",
       "  (2, 0.028252201),\n",
       "  (3, 0.030163813),\n",
       "  (4, 0.0813764)],\n",
       " [(0, 0.5475737), (1, 0.40062684), (2, 0.03485776), (3, 0.01643006)],\n",
       " [(0, 0.5412613), (1, 0.32873145), (2, 0.09707869), (3, 0.032321416)],\n",
       " [(0, 0.5673767), (1, 0.36266223), (2, 0.0667857)],\n",
       " [(0, 0.4950715), (1, 0.47482076), (4, 0.026726943)],\n",
       " [(0, 0.7167385), (1, 0.2544124), (2, 0.02510713)],\n",
       " [(0, 0.6033426), (1, 0.35737222), (4, 0.035899866)],\n",
       " [(0, 0.60703164), (1, 0.27789918), (2, 0.092635386), (4, 0.019713955)],\n",
       " [(0, 0.53135735), (1, 0.3782311), (2, 0.08678917)],\n",
       " [(0, 0.81650966), (1, 0.10602765), (2, 0.07349545)],\n",
       " [(0, 0.48254552),\n",
       "  (1, 0.14133719),\n",
       "  (2, 0.24362777),\n",
       "  (3, 0.033983108),\n",
       "  (4, 0.09850638)],\n",
       " [(0, 0.584306), (1, 0.3071159), (2, 0.092019856), (3, 0.015975526)],\n",
       " [(0, 0.43770432), (1, 0.25992382), (2, 0.29934454)],\n",
       " [(0, 0.45279017), (1, 0.18526705), (2, 0.2105403), (4, 0.14837089)],\n",
       " [(0, 0.5584292), (1, 0.08855117), (2, 0.33076015), (3, 0.021755923)],\n",
       " [(0, 0.6259277), (1, 0.33359838), (2, 0.036382828)],\n",
       " [(0, 0.5468402), (1, 0.34944227), (3, 0.060038034), (4, 0.04314474)],\n",
       " [(0, 0.68787014), (1, 0.11014105), (2, 0.16218676), (4, 0.037298564)],\n",
       " [(0, 0.53260255), (1, 0.39646766), (2, 0.037958812), (3, 0.032435764)],\n",
       " [(0, 0.5143784), (1, 0.39190078), (2, 0.0905255)],\n",
       " [(0, 0.50871676), (1, 0.39985022), (2, 0.087991886)],\n",
       " [(0, 0.6252509),\n",
       "  (1, 0.18786715),\n",
       "  (2, 0.15355772),\n",
       "  (3, 0.015350345),\n",
       "  (4, 0.017973877)],\n",
       " [(0, 0.7086688), (1, 0.28610468)],\n",
       " [(0, 0.5285733), (1, 0.3451625), (2, 0.12359184)],\n",
       " [(0, 0.36189428), (1, 0.23979943), (2, 0.069455944), (4, 0.32548788)],\n",
       " [(0, 0.47680163), (1, 0.39974895), (2, 0.11965617)],\n",
       " [(0, 0.70650834), (1, 0.2892087)],\n",
       " [(0, 0.5781319), (2, 0.024837911), (3, 0.03579924), (4, 0.3606667)],\n",
       " [(0, 0.15495685), (1, 0.13720623), (3, 0.05218783), (4, 0.6550011)],\n",
       " [(0, 0.23194157), (1, 0.038517404), (2, 0.046136446), (4, 0.67993397)],\n",
       " [(0, 0.3177251), (1, 0.09798483), (4, 0.57968843)],\n",
       " [(0, 0.19478229), (3, 0.07203529), (4, 0.73217493)],\n",
       " [(0, 0.20144984), (1, 0.09044846), (3, 0.01790748), (4, 0.6897399)],\n",
       " [(0, 0.22702211), (3, 0.08706658), (4, 0.68468547)],\n",
       " [(0, 0.09912884), (3, 0.30699778), (4, 0.5927831)],\n",
       " [(0, 0.26293114), (2, 0.03595074), (3, 0.045930486), (4, 0.65442556)],\n",
       " [(0, 0.24480987), (3, 0.088342324), (4, 0.6660704)],\n",
       " [(0, 0.28002295), (3, 0.062025063), (4, 0.65687877)],\n",
       " [(0, 0.30332685), (3, 0.03267294), (4, 0.6632349)],\n",
       " [(0, 0.1751525), (1, 0.08932504), (3, 0.082533285), (4, 0.65248233)],\n",
       " [(0, 0.37188715), (1, 0.044285532), (2, 0.05450592), (4, 0.526362)],\n",
       " [(0, 0.36845604), (3, 0.056989577), (4, 0.57350546)],\n",
       " [(0, 0.35883576), (1, 0.0251133), (3, 0.04665464), (4, 0.56897736)],\n",
       " [(0, 0.21276283), (2, 0.05120646), (3, 0.0848781), (4, 0.65065634)],\n",
       " [(0, 0.28518787), (1, 0.04615811), (2, 0.025246108), (4, 0.63970256)],\n",
       " [(0, 0.33243027), (2, 0.04883202), (4, 0.61443555)],\n",
       " [(0, 0.09905672), (2, 0.012892904), (3, 0.11338113), (4, 0.774141)],\n",
       " [(0, 0.21363929), (2, 0.013449375), (3, 0.115178525), (4, 0.65717417)],\n",
       " [(0, 0.29386327), (4, 0.7010393)],\n",
       " [(0, 0.22665848), (1, 0.07622584), (4, 0.6923867)],\n",
       " [(0, 0.3735898), (1, 0.03993399), (4, 0.5822186)],\n",
       " [(0, 0.30844933), (1, 0.14598744), (4, 0.5410927)],\n",
       " [(0, 0.37264526), (3, 0.10143731), (4, 0.5248541)],\n",
       " [(0, 0.289698), (1, 0.057332147), (2, 0.06122662), (4, 0.5883752)],\n",
       " [(0, 0.23741445), (1, 0.07404095), (3, 0.030729124), (4, 0.6573089)],\n",
       " [(0, 0.27346298), (1, 0.046659414), (3, 0.1103321), (4, 0.569046)],\n",
       " [(0, 0.45566815), (1, 0.04410165), (4, 0.49654973)],\n",
       " [(0, 0.97453386), (4, 0.021781487)],\n",
       " [(0, 0.99643403)],\n",
       " [(0, 0.9340423), (3, 0.06468505)],\n",
       " [(0, 0.9953852)],\n",
       " [(0, 0.9951727)],\n",
       " [(0, 0.91415906), (3, 0.016215876), (4, 0.06865338)],\n",
       " [(0, 0.91690683), (4, 0.07948644)],\n",
       " [(0, 0.9964069)],\n",
       " [(0, 0.995349)],\n",
       " [(0, 0.9606273), (4, 0.034629192)],\n",
       " [(0, 0.99626267)],\n",
       " [(0, 0.99636126)],\n",
       " [(0, 0.99583095)],\n",
       " [(0, 0.97729546), (4, 0.019242333)],\n",
       " [(0, 0.9745447), (4, 0.022358613)],\n",
       " [(0, 0.94800764), (3, 0.050211366)],\n",
       " [(0, 0.9958471)],\n",
       " [(0, 0.99573106)],\n",
       " [(0, 0.99549466)],\n",
       " [(0, 0.947734), (3, 0.05072501)],\n",
       " [(0, 0.9957476)],\n",
       " [(0, 0.99559164)],\n",
       " [(0, 0.93182), (3, 0.06652717)],\n",
       " [(0, 0.99579436)],\n",
       " [(0, 0.8645475), (3, 0.081307754), (4, 0.053047486)],\n",
       " [(0, 0.99481475)],\n",
       " [(0, 0.99613386)],\n",
       " [(0, 0.99503684)],\n",
       " [(0, 0.99492776)],\n",
       " [(0, 0.9809487), (4, 0.015818566)],\n",
       " [(0, 0.87568593), (3, 0.070566066), (4, 0.052599523)],\n",
       " [(0, 0.8668932), (1, 0.017849253), (3, 0.11443908)],\n",
       " [(0, 0.9744854), (4, 0.021334635)],\n",
       " [(0, 0.9596512), (4, 0.036513396)],\n",
       " [(0, 0.90840816), (3, 0.09023883)],\n",
       " [(0, 0.9204465), (3, 0.0777736)],\n",
       " [(0, 0.9243736), (3, 0.07409557)],\n",
       " [(0, 0.9109961), (3, 0.053918578), (4, 0.034022298)],\n",
       " [(0, 0.99448407)],\n",
       " [(0, 0.89328873), (3, 0.10482395)],\n",
       " [(0, 0.88236237), (3, 0.09812079), (4, 0.018420376)],\n",
       " [(0, 0.9603572), (2, 0.03544767)],\n",
       " [(0, 0.9954123)],\n",
       " [(0, 0.95779485), (3, 0.04066063)],\n",
       " [(0, 0.92239517), (3, 0.075959)],\n",
       " [(0, 0.9475444), (3, 0.05074477)],\n",
       " [(0, 0.97576505), (3, 0.022694018)],\n",
       " [(0, 0.8929732), (3, 0.04518694), (4, 0.06081337)],\n",
       " [(0, 0.8894086), (3, 0.07692269), (4, 0.032710273)],\n",
       " [(0, 0.9629716), (2, 0.014962454), (4, 0.018277047)],\n",
       " [(0, 0.9032364), (3, 0.0950616)],\n",
       " [(0, 0.9950831)],\n",
       " [(0, 0.9100595), (2, 0.022943119), (3, 0.06594747)],\n",
       " [(0, 0.8963869), (2, 0.033854425), (3, 0.06882687)],\n",
       " [(0, 0.8725099), (2, 0.013143463), (3, 0.049747955), (4, 0.06412163)],\n",
       " [(0, 0.94990313), (2, 0.0332521), (3, 0.015784515)],\n",
       " [(0, 0.9050208), (3, 0.032671846), (4, 0.061226778)],\n",
       " [(0, 0.92183673), (3, 0.07678187)],\n",
       " [(0, 0.85831136), (3, 0.062055632), (4, 0.078823395)],\n",
       " [(0, 0.9578802), (3, 0.040474735)],\n",
       " [(0, 0.88679534), (3, 0.11196782)],\n",
       " [(0, 0.7439494), (3, 0.25458112)],\n",
       " [(0, 0.87531936), (3, 0.12345634)],\n",
       " [(0, 0.8027054), (3, 0.19600667)],\n",
       " [(0, 0.7618808), (3, 0.23701812)],\n",
       " [(0, 0.8427939), (1, 0.048258957), (3, 0.107999146)],\n",
       " [(0, 0.6776889), (1, 0.071209624), (3, 0.25026366)],\n",
       " [(0, 0.8157224), (1, 0.034468085), (3, 0.14887144)],\n",
       " [(0, 0.8005869), (1, 0.08077445), (3, 0.117838465)],\n",
       " [(0, 0.8292498), (3, 0.16960464)],\n",
       " [(0, 0.6955138), (3, 0.30336156)],\n",
       " [(0, 0.7259867), (3, 0.27258104)],\n",
       " [(0, 0.90031105), (3, 0.09849177)],\n",
       " [(0, 0.8353738), (3, 0.16342919)],\n",
       " [(0, 0.72741246), (3, 0.27140194)],\n",
       " [(0, 0.7335736), (1, 0.043631226), (3, 0.22201589)],\n",
       " [(0, 0.6796934), (3, 0.31906328)],\n",
       " [(0, 0.7965879), (3, 0.2022085)],\n",
       " [(0, 0.81820786), (3, 0.18060078)],\n",
       " [(0, 0.6467029), (1, 0.09947538), (3, 0.2528924)],\n",
       " [(0, 0.724554), (1, 0.01487832), (3, 0.2595802)],\n",
       " [(0, 0.8152869), (1, 0.017617518), (3, 0.16615707)],\n",
       " [(0, 0.8997708), (3, 0.09895415)],\n",
       " [(0, 0.69808966), (3, 0.3004798)],\n",
       " [(0, 0.81524146), (3, 0.18300724)],\n",
       " [(0, 0.7598045), (3, 0.23890693)],\n",
       " [(0, 0.7995435), (1, 0.0530614), (3, 0.14650564)],\n",
       " [(0, 0.86858356), (1, 0.0107667735), (3, 0.119935095)],\n",
       " [(0, 0.75822514), (3, 0.24062262)],\n",
       " [(0, 0.7671547), (1, 0.021981644), (2, 0.016440202), (3, 0.19400427)],\n",
       " [(0, 0.8952127), (2, 0.04662981), (3, 0.05738362)],\n",
       " [(0, 0.5880681), (2, 0.01320084), (3, 0.33815792), (4, 0.060023773)],\n",
       " [(0, 0.7527364), (1, 0.03136665), (3, 0.21491244)],\n",
       " [(0, 0.79930466), (1, 0.092894964), (3, 0.10690231)],\n",
       " [(0, 0.6704691), (2, 0.02921148), (3, 0.21283978), (4, 0.08697361)],\n",
       " [(0, 0.6956636), (3, 0.18461272), (4, 0.11857071)],\n",
       " [(0, 0.92999053), (1, 0.06399428)],\n",
       " [(0, 0.61343634), (1, 0.052464623), (3, 0.22813205), (4, 0.1054289)],\n",
       " [(0, 0.7878142), (3, 0.18045804), (4, 0.030868825)],\n",
       " [(0, 0.6217696), (2, 0.029457603), (3, 0.13669975), (4, 0.21165687)],\n",
       " [(0, 0.88513494), (1, 0.058015272), (3, 0.05551952)],\n",
       " [(0, 0.9704651), (3, 0.027897885)],\n",
       " [(0, 0.8854883), (1, 0.05996841), (2, 0.018303743), (4, 0.033868946)],\n",
       " [(0, 0.67814296), (3, 0.021370368), (4, 0.29948452)],\n",
       " [(0, 0.86406374), (3, 0.09098143), (4, 0.04384238)],\n",
       " [(0, 0.706165), (1, 0.016477268), (2, 0.010395158), (3, 0.2664227)],\n",
       " [(0, 0.8162438), (1, 0.10545988), (3, 0.045384545), (4, 0.03251583)],\n",
       " [(0, 0.65359896), (3, 0.15799269), (4, 0.18739262)],\n",
       " [(0, 0.8700589), (3, 0.12866938)],\n",
       " [(0, 0.8287211), (1, 0.0283264), (3, 0.032554545), (4, 0.10982343)],\n",
       " [(0, 0.777965), (3, 0.22012049)],\n",
       " [(0, 0.79894465),\n",
       "  (1, 0.038440216),\n",
       "  (2, 0.023618268),\n",
       "  (3, 0.039787866),\n",
       "  (4, 0.09920902)],\n",
       " [(0, 0.75135446), (3, 0.24721828)],\n",
       " [(0, 0.5463223), (1, 0.15819868), (3, 0.13878264), (4, 0.15625536)],\n",
       " [(0, 0.7151609), (2, 0.048392266), (4, 0.23245421)],\n",
       " [(0, 0.6097732), (3, 0.26554927), (4, 0.123327546)],\n",
       " [(0, 0.5022321), (1, 0.29981714), (3, 0.19702269)],\n",
       " [(0, 0.6851693), (1, 0.07607784), (3, 0.09885922), (4, 0.13930401)],\n",
       " [(0, 0.7524489), (1, 0.076314524), (3, 0.17009853)],\n",
       " [(0, 0.7589511), (3, 0.18784425), (4, 0.051954787)],\n",
       " [(0, 0.117131), (3, 0.094898224), (4, 0.78655523)],\n",
       " [(0, 0.11478678), (4, 0.8799815)],\n",
       " [(4, 0.98902994)],\n",
       " [(0, 0.10203662), (3, 0.022592638), (4, 0.8739708)],\n",
       " [(0, 0.16412053), (4, 0.8310315)],\n",
       " [(0, 0.38582975), (3, 0.044364057), (4, 0.56844383)],\n",
       " [(0, 0.20712355), (4, 0.7878738)],\n",
       " [(0, 0.1450102), (4, 0.8492997)],\n",
       " [(0, 0.2325732), (4, 0.7627356)],\n",
       " [(0, 0.09130113), (3, 0.04481442), (4, 0.8625948)],\n",
       " [(0, 0.20302175), (4, 0.7924907)],\n",
       " [(0, 0.028217122), (4, 0.9670122)],\n",
       " [(0, 0.062478326), (2, 0.023592304), (4, 0.9103226)],\n",
       " [(0, 0.052122623), (3, 0.019102134), (4, 0.9275392)],\n",
       " [(0, 0.33359066), (4, 0.6617136)],\n",
       " [(0, 0.15425931), (4, 0.84001863)],\n",
       " [(0, 0.064226866), (3, 0.07050038), (4, 0.864079)],\n",
       " [(0, 0.05588237), (4, 0.93942046)],\n",
       " [(0, 0.22175013), (4, 0.77263945)],\n",
       " [(0, 0.24989013), (4, 0.74606925)],\n",
       " [(0, 0.045161534), (1, 0.011117923), (4, 0.9396539)],\n",
       " [(0, 0.1563768), (4, 0.8383219)],\n",
       " [(0, 0.31087798), (3, 0.026310641), (4, 0.66169184)],\n",
       " [(0, 0.10416704), (4, 0.89168787)],\n",
       " [(0, 0.17814927), (2, 0.041525282), (3, 0.048474055), (4, 0.73120344)],\n",
       " [(0, 0.11843866), (4, 0.87496185)],\n",
       " [(0, 0.080317624), (4, 0.9152384)],\n",
       " [(0, 0.12236376), (2, 0.031201271), (4, 0.8423293)],\n",
       " [(0, 0.23362046), (4, 0.7615306)],\n",
       " [(0, 0.1748443), (4, 0.8185774)],\n",
       " [(0, 0.19971667), (4, 0.79568905)],\n",
       " [(0, 0.08759651), (4, 0.9078374)],\n",
       " [(0, 0.14205515), (4, 0.8543037)],\n",
       " [(0, 0.23174234), (4, 0.764535)],\n",
       " [(0, 0.0922449), (4, 0.9019362)],\n",
       " [(0, 0.15925781), (4, 0.8365378)],\n",
       " [(0, 0.14313792), (3, 0.013943001), (4, 0.84182423)],\n",
       " [(0, 0.1244968), (4, 0.87061393)],\n",
       " [(0, 0.24735849), (4, 0.7489987)],\n",
       " [(0, 0.095403), (4, 0.89957297)],\n",
       " [(0, 0.30625266), (4, 0.6900078)],\n",
       " [(0, 0.35378948), (4, 0.6423712)],\n",
       " [(0, 0.25717175), (4, 0.738475)],\n",
       " [(0, 0.19783953), (4, 0.79814)],\n",
       " [(0, 0.30029032), (4, 0.6950897)],\n",
       " [(0, 0.22169098), (2, 0.018179696), (4, 0.7571106)],\n",
       " [(0, 0.21072513), (4, 0.78482217)],\n",
       " [(0, 0.20796627), (1, 0.029617757), (4, 0.75815356)],\n",
       " [(0, 0.2399538), (3, 0.02738), (4, 0.73125315)],\n",
       " [(0, 0.4023863), (2, 0.085474715), (4, 0.5085135)],\n",
       " [(0, 0.25629455), (3, 0.027686987), (4, 0.7149836)],\n",
       " [(0, 0.19511549), (4, 0.79968816)],\n",
       " [(0, 0.19745551), (4, 0.798911)],\n",
       " [(0, 0.14707156), (4, 0.8488664)],\n",
       " [(0, 0.27120176), (4, 0.7248193)],\n",
       " [(0, 0.19817571), (4, 0.7977023)],\n",
       " [(0, 0.18884079), (4, 0.80681854)],\n",
       " [(0, 0.13152237), (4, 0.86439127)],\n",
       " [(0, 0.24145146), (4, 0.754644)],\n",
       " [(0, 0.16344054), (4, 0.83204895)],\n",
       " [(0, 0.5203142), (2, 0.42964405), (3, 0.019383231), (4, 0.030355908)],\n",
       " [(0, 0.6376385), (2, 0.3598242)],\n",
       " [(0, 0.5764013), (2, 0.3956474), (3, 0.02713501)],\n",
       " [(0, 0.5562953), (2, 0.4409525)],\n",
       " [(0, 0.49827597), (1, 0.024017686), (2, 0.47564462)],\n",
       " [(0, 0.58502376), (2, 0.38798273), (3, 0.026362777)],\n",
       " [(0, 0.5401252), (2, 0.45752427)],\n",
       " [(0, 0.5521301), (2, 0.44575956)],\n",
       " [(0, 0.6391426), (2, 0.30178472), (3, 0.05843654)],\n",
       " [(0, 0.5973964), (2, 0.33959156), (3, 0.046749976), (4, 0.01586271)],\n",
       " [(0, 0.5347162), (1, 0.04064287), (2, 0.3598471), (3, 0.064441)],\n",
       " [(0, 0.5593661), (2, 0.4374849)],\n",
       " [(0, 0.43449786), (2, 0.4938075), (3, 0.06200618)],\n",
       " [(0, 0.41224492), (2, 0.52561635), (4, 0.059421826)],\n",
       " [(0, 0.55054444), (1, 0.011786306), (2, 0.41650414), (4, 0.019045332)],\n",
       " [(0, 0.5244879), (1, 0.072076395), (2, 0.40135202)],\n",
       " [(0, 0.57396), (2, 0.4231767)],\n",
       " [(0, 0.5019261), (1, 0.029304286), (2, 0.44936928), (4, 0.017329996)],\n",
       " [(0, 0.5415448), (2, 0.44035143), (3, 0.017301718)],\n",
       " [(0, 0.44123155), (1, 0.04355072), (2, 0.4843857), (3, 0.030447707)],\n",
       " [(0, 0.53888285), (1, 0.042942792), (2, 0.41588238)],\n",
       " [(0, 0.5454509), (1, 0.012943181), (2, 0.43854097)],\n",
       " [(0, 0.4490522), (2, 0.48211303), (3, 0.06805555)],\n",
       " [(0, 0.60364467), (2, 0.3938217)],\n",
       " [(0, 0.5868945), (2, 0.4101174)],\n",
       " [(0, 0.623752), (1, 0.049204122), (2, 0.32514516)],\n",
       " [(0, 0.54174757), (2, 0.41440418), (4, 0.041589253)],\n",
       " [(0, 0.6027125), (2, 0.3490118), (4, 0.046115752)],\n",
       " [(0, 0.583263), (2, 0.3736127), (3, 0.028852712), (4, 0.013876362)],\n",
       " [(0, 0.5622769), (2, 0.4341127)],\n",
       " [(0, 0.57380414), (3, 0.42420626)],\n",
       " [(0, 0.42173195), (3, 0.5763487)],\n",
       " [(0, 0.55895483), (3, 0.4394565)],\n",
       " [(0, 0.62438023), (3, 0.37338394)],\n",
       " [(0, 0.5710429), (3, 0.4269096)],\n",
       " [(0, 0.6097014), (3, 0.38827047)],\n",
       " [(0, 0.31471044), (3, 0.6837064)],\n",
       " [(0, 0.47076428), (3, 0.5269063)],\n",
       " [(0, 0.5797846), (3, 0.41842315)],\n",
       " [(0, 0.42388698), (3, 0.5739657)],\n",
       " [(0, 0.4157884), (3, 0.5820862)],\n",
       " [(0, 0.4878378), (3, 0.5101895)],\n",
       " [(0, 0.41638914), (3, 0.5817551)],\n",
       " [(0, 0.5984077), (3, 0.39904755)],\n",
       " [(0, 0.6924431), (3, 0.3056697)],\n",
       " [(0, 0.8027426), (3, 0.19568205)],\n",
       " [(0, 0.63177633), (3, 0.36647823)],\n",
       " [(0, 0.5916549), (3, 0.40664512)],\n",
       " [(0, 0.5440841), (3, 0.45388776)],\n",
       " [(0, 0.43446624), (3, 0.5635611)],\n",
       " [(0, 0.5083219), (1, 0.029099096), (3, 0.4605833)],\n",
       " [(0, 0.6830496), (3, 0.31500646)],\n",
       " [(0, 0.43433014), (3, 0.5634997)],\n",
       " [(0, 0.5997112), (3, 0.3983672)],\n",
       " [(0, 0.8741304), (3, 0.123842016)],\n",
       " [(0, 0.56138766), (1, 0.10032211), (3, 0.33673328)],\n",
       " [(0, 0.6127361), (3, 0.32202518), (4, 0.06354485)],\n",
       " [(0, 0.52985865), (3, 0.4684855)],\n",
       " [(0, 0.2486677), (3, 0.7497663)],\n",
       " [(0, 0.55845064), (3, 0.43964475)],\n",
       " [(0, 0.8473871), (3, 0.15086098)],\n",
       " [(0, 0.83221984), (3, 0.16608243)],\n",
       " [(0, 0.7669641), (2, 0.03163204), (3, 0.20016098)],\n",
       " [(0, 0.7042376), (3, 0.29350415)],\n",
       " [(0, 0.93430495), (3, 0.06377531)],\n",
       " [(0, 0.90060127), (3, 0.097118855)],\n",
       " [(0, 0.90822816), (3, 0.08976421)],\n",
       " [(0, 0.71818495), (3, 0.2802948)],\n",
       " [(0, 0.9313448), (3, 0.06683136)],\n",
       " [(0, 0.89199406), (3, 0.10606672)],\n",
       " [(0, 0.9233541), (3, 0.074673265)],\n",
       " [(0, 0.91764015), (3, 0.08067522)],\n",
       " [(0, 0.91210157), (3, 0.08568165)],\n",
       " [(0, 0.72704583), (3, 0.2709956)],\n",
       " [(0, 0.99478453)],\n",
       " [(0, 0.85768455), (3, 0.140059)],\n",
       " [(0, 0.91708976), (3, 0.08088242)],\n",
       " [(0, 0.89370084), (3, 0.10408848)],\n",
       " [(0, 0.99472314)],\n",
       " [(0, 0.8518846), (3, 0.14603043)],\n",
       " [(0, 0.7727432), (3, 0.22543347)],\n",
       " [(0, 0.8318705), (3, 0.16622657)],\n",
       " [(0, 0.805433), (3, 0.19296885)],\n",
       " [(0, 0.8517982), (3, 0.14640738)],\n",
       " [(0, 0.8331429), (3, 0.16528127)],\n",
       " [(0, 0.89848405), (3, 0.099490725)],\n",
       " [(0, 0.7322591), (2, 0.011955432), (3, 0.2545307)],\n",
       " [(0, 0.9258749), (3, 0.07216736)],\n",
       " [(0, 0.88256073), (3, 0.11589866)],\n",
       " [(0, 0.87118506), (3, 0.12648252)],\n",
       " [(0, 0.7382309), (1, 0.17049722), (2, 0.08738258)],\n",
       " [(0, 0.73123455), (1, 0.13249563), (2, 0.03266552), (3, 0.10295171)],\n",
       " [(0, 0.8365956), (1, 0.11857435), (3, 0.043426104)],\n",
       " [(0, 0.69592386), (1, 0.2282997), (3, 0.07468369)],\n",
       " [(0, 0.66010743), (1, 0.19148019), (2, 0.04479963), (3, 0.102976374)],\n",
       " [(0, 0.8257468), (3, 0.17253774)],\n",
       " [(0, 0.8170452), (1, 0.08211851), (3, 0.09977266)],\n",
       " [(0, 0.8113029), (1, 0.18315728)],\n",
       " [(0, 0.6731699), (1, 0.2119624), (3, 0.1138865)],\n",
       " [(0, 0.75485647), (1, 0.123060256), (3, 0.12101517)],\n",
       " [(0, 0.6156346), (1, 0.1872381), (3, 0.19577721)],\n",
       " [(0, 0.7490143), (1, 0.094076455), (3, 0.15589343)],\n",
       " [(0, 0.8856075), (1, 0.10919998)],\n",
       " [(0, 0.7605844), (1, 0.07150324), (3, 0.16687004)],\n",
       " [(0, 0.8204239), (1, 0.15213951), (3, 0.026136372)],\n",
       " [(0, 0.8086824), (1, 0.18674825)],\n",
       " [(0, 0.8061945), (3, 0.19169274)],\n",
       " [(0, 0.7496334), (1, 0.14014372), (3, 0.10863122)],\n",
       " [(0, 0.7700513), (1, 0.16949983), (3, 0.059171073)],\n",
       " [(0, 0.7179766), (3, 0.27928552)],\n",
       " [(0, 0.6837271), (1, 0.21452379), (3, 0.066992715), (4, 0.034138836)],\n",
       " [(0, 0.78202987), (1, 0.13726974), (3, 0.07963171)],\n",
       " [(0, 0.8718541), (1, 0.030258927), (3, 0.09650882)],\n",
       " [(0, 0.8615264), (1, 0.12178791), (2, 0.012843351)],\n",
       " [(0, 0.82915306), (1, 0.1251752), (3, 0.044645827)],\n",
       " [(0, 0.72853386), (1, 0.19384257), (3, 0.07617827)],\n",
       " [(0, 0.51557386), (1, 0.20723668), (2, 0.026503025), (3, 0.25019667)],\n",
       " [(0, 0.764849), (1, 0.084912725), (3, 0.14911875)],\n",
       " [(0, 0.7479485), (1, 0.12757662), (3, 0.12360141)],\n",
       " [(0, 0.72730964), (1, 0.12399725), (2, 0.021392534), (3, 0.12667799)],\n",
       " [(0, 0.12547189), (1, 0.021227647), (2, 0.15784688), (3, 0.69500613)],\n",
       " [(0, 0.186457), (2, 0.25837988), (3, 0.5540646)],\n",
       " [(0, 0.1955907), (2, 0.21313027), (3, 0.590273)],\n",
       " [(0, 0.17275408), (1, 0.0770975), (2, 0.17208533), (3, 0.5775942)],\n",
       " [(0, 0.17113923), (2, 0.3218876), (3, 0.505985)],\n",
       " [(2, 0.30706444), (3, 0.6890745)],\n",
       " [(0, 0.18735358), (2, 0.20218202), (3, 0.60947484)],\n",
       " [(0, 0.12693663), (2, 0.35077146), (3, 0.52128816)],\n",
       " [(0, 0.3344351), (1, 0.025516434), (2, 0.2211817), (3, 0.41839555)],\n",
       " [(0, 0.12789275), (1, 0.04156018), (2, 0.0712286), (3, 0.75891685)],\n",
       " [(0, 0.24441935), (2, 0.3642135), (3, 0.39039767)],\n",
       " [(0, 0.1557561), (2, 0.17729391), (3, 0.66594607)],\n",
       " [(0, 0.06431677), (2, 0.38267168), (3, 0.5520288)],\n",
       " [(0, 0.12643473), (1, 0.07278142), (2, 0.35286418), (3, 0.4474952)],\n",
       " [(0, 0.09970854), (2, 0.33857372), (3, 0.54247797), (4, 0.018739186)],\n",
       " [(0, 0.25196895), (1, 0.08202514), (2, 0.41312954), (3, 0.25237295)],\n",
       " [(0, 0.17694923), (1, 0.08967101), (2, 0.11868484), (3, 0.6143086)],\n",
       " [(0, 0.25795138), (2, 0.2504792), (3, 0.49071226)],\n",
       " [(0, 0.22944579), (2, 0.3878672), (3, 0.38171676)],\n",
       " [(0, 0.31197056), (2, 0.345952), (3, 0.34119877)],\n",
       " [(0, 0.21003687), (2, 0.2014105), (3, 0.58756155)],\n",
       " [(0, 0.25400963), (1, 0.067121364), (2, 0.1480393), (3, 0.5303966)],\n",
       " [(0, 0.24149947), (2, 0.29896662), (3, 0.45874706)],\n",
       " [(0, 0.16877957), (2, 0.2735172), (3, 0.46462753), (4, 0.09259782)],\n",
       " [(0, 0.25006822), (1, 0.041760433), (2, 0.28414264), (3, 0.42352262)],\n",
       " [(0, 0.2141673), (2, 0.093259394), (3, 0.69175905)],\n",
       " [(0, 0.26130238), (1, 0.07295669), (2, 0.13468118), (3, 0.53060377)],\n",
       " [(0, 0.19197637), (2, 0.17005885), (3, 0.63707596)],\n",
       " [(0, 0.120711625), (1, 0.02420015), (2, 0.19985591), (3, 0.6548377)],\n",
       " [(0, 0.24033646), (2, 0.39244235), (3, 0.3661629)],\n",
       " [(0, 0.99658185)],\n",
       " [(0, 0.99675673)],\n",
       " [(0, 0.99572134)],\n",
       " [(0, 0.996238)],\n",
       " [(0, 0.9963842)],\n",
       " [(0, 0.99651456)],\n",
       " [(0, 0.9766633), (3, 0.022057)],\n",
       " [(0, 0.92693615), (3, 0.071825914)],\n",
       " [(0, 0.99674624)],\n",
       " [(0, 0.99679285)],\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dist for dist in main_model[corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from kneed import KneeLocator\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def extract_english_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def select_optimal_topic(coherence_scores, topic_range):\n",
    "    knee = KneeLocator(topic_range, coherence_scores, curve=\"concave\", direction=\"decreasing\")\n",
    "    if knee.knee is not None:\n",
    "        print(f\"KneeLocator detected optimal topic number: {knee.knee}\")\n",
    "        return knee.knee\n",
    "    best_topic = topic_range[np.argmax(coherence_scores)]\n",
    "    print(f\"Using highest coherence: {best_topic}\")\n",
    "    return best_topic\n",
    "\n",
    "def main():\n",
    "    file_path = \"C:/Users/User/Desktop/soloplay/01. nlp/combined_data_NLP.xlsx\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.dropna(subset=['answer'])\n",
    "    df['text'] = df['answer']\n",
    "\n",
    "    # 불용어 및 토큰화 설정\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    global stop_words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['tokens'] = df['text'].apply(extract_english_tokens)\n",
    "    tokens_list = df['tokens'].tolist()\n",
    "\n",
    "    dictionary = Dictionary(tokens_list)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_list]\n",
    "\n",
    "    # 2. 최적 토픽 수 계산\n",
    "    topic_range = range(2, 6)\n",
    "    coherence_scores = []\n",
    "\n",
    "    print(\"토픽 갯수 산정을 위한 coherence score 계산중\")\n",
    "    for n in topic_range:\n",
    "        lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n, passes=10, random_state=42, alpha='auto')\n",
    "        cm = CoherenceModel(model=lda_model, texts=tokens_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_scores.append(cm.get_coherence())\n",
    "        print(f\"Topic {n} - Coherence: {coherence_scores[-1]:.4f}\")\n",
    "\n",
    "    optimal_topics = select_optimal_topic(coherence_scores, topic_range)\n",
    "    print(f\"최적 토픽 수: {optimal_topics}\")\n",
    "\n",
    "    # 3. 메인 토픽 모델 학습\n",
    "    main_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_topics,\n",
    "                          passes=50,\n",
    "                          random_state=42,\n",
    "                          update_every=1,\n",
    "                          eval_every=1,\n",
    "                          alpha='auto')\n",
    "\n",
    "    # 4. 서브토픽 분석\n",
    "    subtopic_records = []\n",
    "    subtopic_labels = []\n",
    "    subtopic_words = []\n",
    "\n",
    "    for topic_id in range(optimal_topics):\n",
    "        print(f\"\\n[Subtopic Analysis for Topic {topic_id}]\")\n",
    "        topic_docs = []\n",
    "        for i, dist in enumerate(main_model[corpus]):\n",
    "            for t_id, prob in dist:\n",
    "                if t_id == topic_id:\n",
    "                    topic_docs.append((i, prob))\n",
    "        topic_docs = sorted(topic_docs, key=lambda x: x[1], reverse=True)[:20]\n",
    "        doc_indices = [idx for idx, _ in topic_docs]\n",
    "        filtered_texts = [tokens_list[i] for i in doc_indices]\n",
    "\n",
    "        if len(filtered_texts) == 0:\n",
    "            print(f\"⚠️ Topic {topic_id} has no representative documents.\")\n",
    "            continue\n",
    "\n",
    "        sub_dict = Dictionary(filtered_texts)\n",
    "        sub_corpus = [sub_dict.doc2bow(text) for text in tokens_list]  # 전체 문서 대상\n",
    "\n",
    "        # 최적 서브토픽 수 선택\n",
    "        scores = []\n",
    "        for n in range(2, 10):\n",
    "            try:\n",
    "                temp_model = LdaModel(corpus=[sub_dict.doc2bow(t) for t in filtered_texts], id2word=sub_dict, num_topics=n, passes=15, random_state=42)\n",
    "                cm = CoherenceModel(model=temp_model, texts=filtered_texts, dictionary=sub_dict, coherence='c_v')\n",
    "                scores.append(cm.get_coherence())\n",
    "            except:\n",
    "                scores.append(0)\n",
    "        best_n = range(2, 10)[np.argmax(scores)]\n",
    "\n",
    "        # 서브모델 학습\n",
    "        sub_model = LdaModel(corpus=sub_corpus, id2word=sub_dict, num_topics=best_n,\n",
    "                             passes=50,\n",
    "                             random_state=42,\n",
    "                             update_every=1,\n",
    "                             eval_every=1,\n",
    "                             alpha='auto')\n",
    "\n",
    "        # 키워드 저장\n",
    "        for t_id in range(best_n):\n",
    "            terms = sub_model.show_topic(t_id, topn=20)\n",
    "            keywords = \", \".join([f\"{word} ({weight:.3f})\" for word, weight in terms])\n",
    "            subtopic_words.append({\n",
    "                \"Model\": f\"LDA {topic_id}\",\n",
    "                \"Subtopic\": f\"{topic_id}-{t_id+1}\",\n",
    "                \"Keywords\": keywords\n",
    "            })\n",
    "\n",
    "        # 전체 문서에 대한 분포 계산\n",
    "        for i, doc in enumerate(tokens_list):\n",
    "            bow = sub_dict.doc2bow(doc)\n",
    "            dist = sub_model.get_document_topics(bow, minimum_probability=0) #해당 문서가 서브토픽들에 대해 가지는 확률 분포\n",
    "            record = {'doc_id': i}\n",
    "            for t_id, prob in dist:\n",
    "                label = f\"{topic_id}-{t_id+1}\"\n",
    "                record[label] = prob\n",
    "                if label not in subtopic_labels:\n",
    "                    subtopic_labels.append(label)\n",
    "            subtopic_records.append(record)\n",
    "\n",
    "    # 5. 결과 저장\n",
    "    result_df = pd.DataFrame(subtopic_records)\n",
    "    result_df = result_df.groupby(\"doc_id\").mean().reset_index()\n",
    "    result_df[\"llm_name\"] = df[\"model\"]\n",
    "\n",
    "    # 누락된 서브토픽 컬럼 0으로 채우기\n",
    "    for col in subtopic_labels:\n",
    "        if col not in result_df.columns:\n",
    "            result_df[col] = 0.0\n",
    "\n",
    "    result_df.to_excel(\"C:/Users/User/Desktop/soloplay/01. nlp/stopic_final/llm_subtopic_distribution_v2.xlsx\", index=False)\n",
    "    print(\"저장 완료: llm_subtopic_distribution.xlsx\")\n",
    "\n",
    "    words_df = pd.DataFrame(subtopic_words)\n",
    "    words_df.to_excel(\"C:/Users/User/Desktop/soloplay/01. nlp/stopic_final/subtopic_keywords_v2.xlsx\", index=False)\n",
    "    print(\"저장 완료: subtopic_keywords.xlsx\")\n",
    "\n",
    "# 6. 실행 시작\n",
    "if __name__ == '__main__':\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
